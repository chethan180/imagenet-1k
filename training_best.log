2025-11-03 19:42:54 /home/ubuntu/imagenet-1k/src/training/trainer.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
2025-11-03 19:42:54   self.scaler = torch.cuda.amp.GradScaler() if config['misc']['amp'] else None
2025-11-03 19:42:54 2025-11-03 19:42:53,841 - INFO - Model info: {'total_parameters': 25557032, 'trainable_parameters': 25557032, 'model_size_mb': 97.49234008789062}
2025-11-03 19:42:54 Train Epoch 000:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-03 19:42:59   with torch.cuda.amp.autocast():
2025-11-03 19:43:04 Train Epoch 000:   0%|          | 0/2502 [00:10<?, ?it/s, Loss=7.1120, Top1=N/A, LR=0.100000]2025-11-03 19:43:04,212 - INFO - Step 0: {'train_loss_batch': 7.111979007720947, 'train_lr': 0.1, 'batch_time': 10.367703914642334, 'data_time': 5.111258029937744}
2025-11-03 19:43:04 Train Epoch 000:   0%|          | 6/2502 [00:18<1:31:09,  2.19s/it, Loss=7.1120, Top1=N/A, LR=0.100000]wandb: WARNING Tried to log to step 0 that is less than the current step 1. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-03 19:43:14 Train Epoch 000:   4%|▍         | 100/2502 [02:54<1:05:50,  1.64s/it, Loss=7.1914, Top1=N/A, LR=0.100000]2025-11-03 19:45:48,344 - INFO - Step 100: {'train_loss_batch': 6.972568988800049, 'train_lr': 0.1, 'batch_time': 1.7277279726349482, 'data_time': 0.05143762815116656}
2025-11-03 19:45:48 Train Epoch 000:   8%|▊         | 200/2502 [05:39<1:03:31,  1.66s/it, Loss=7.0598, Top1=0.13%, LR=0.100000]2025-11-03 19:48:33,484 - INFO - Step 200: {'train_loss_batch': 6.906253814697266, 'train_lr': 0.1, 'batch_time': 1.6897574408137384, 'data_time': 0.026275442607367215}
2025-11-03 19:48:33 Train Epoch 000:  12%|█▏        | 300/2502 [08:25<1:00:24,  1.65s/it, Loss=7.0111, Top1=0.12%, LR=0.100000]2025-11-03 19:51:18,954 - INFO - Step 300: {'train_loss_batch': 6.908353805541992, 'train_lr': 0.1, 'batch_time': 1.678107628394599, 'data_time': 0.017843972012846175}
2025-11-03 19:51:18 Train Epoch 000:  16%|█▌        | 400/2502 [11:10<58:01,  1.66s/it, Loss=6.9862, Top1=N/A, LR=0.100000]  2025-11-03 19:54:03,916 - INFO - Step 400: {'train_loss_batch': 6.914018630981445, 'train_lr': 0.1, 'batch_time': 1.6710034915039367, 'data_time': 0.013618123204333527}
2025-11-03 19:54:03 Train Epoch 000:  20%|█▉        | 500/2502 [13:55<54:45,  1.64s/it, Loss=6.9681, Top1=N/A, LR=0.100000]2025-11-03 19:56:48,878 - INFO - Step 500: {'train_loss_batch': 6.88545036315918, 'train_lr': 0.1, 'batch_time': 1.666735153236313, 'data_time': 0.01108610796595286}
2025-11-03 19:56:48 Train Epoch 000:  24%|██▍       | 600/2502 [16:39<52:09,  1.65s/it, Loss=6.9508, Top1=N/A, LR=0.100000]2025-11-03 19:59:33,699 - INFO - Step 600: {'train_loss_batch': 6.804302215576172, 'train_lr': 0.1, 'batch_time': 1.6636527516877593, 'data_time': 0.009391517290061404}
2025-11-03 19:59:33 Train Epoch 000:  28%|██▊       | 700/2502 [19:25<49:50,  1.66s/it, Loss=6.9317, Top1=N/A, LR=0.100000]2025-11-03 20:02:19,387 - INFO - Step 700: {'train_loss_batch': 6.834394454956055, 'train_lr': 0.1, 'batch_time': 1.6626867438519053, 'data_time': 0.008181025060199297}
2025-11-03 20:02:19 Train Epoch 000:  32%|███▏      | 800/2502 [22:11<47:07,  1.66s/it, Loss=6.9079, Top1=N/A, LR=0.100000]2025-11-03 20:05:05,141 - INFO - Step 800: {'train_loss_batch': 6.631833076477051, 'train_lr': 0.1, 'batch_time': 1.6620437325014454, 'data_time': 0.007280853952510229}
2025-11-03 20:05:05 Train Epoch 000:  36%|███▌      | 900/2502 [24:56<43:45,  1.64s/it, Loss=6.8820, Top1=N/A, LR=0.100000]2025-11-03 20:07:50,432 - INFO - Step 900: {'train_loss_batch': 6.8479533195495605, 'train_lr': 0.1, 'batch_time': 1.6610290559626844, 'data_time': 0.0065739334754224095}
2025-11-03 20:07:50 Train Epoch 000:  40%|███▉      | 1000/2502 [27:41<41:29,  1.66s/it, Loss=6.8536, Top1=N/A, LR=0.100000]2025-11-03 20:10:35,406 - INFO - Step 1000: {'train_loss_batch': 6.452040672302246, 'train_lr': 0.1, 'batch_time': 1.6599014129314746, 'data_time': 0.006009587279328338}
2025-11-03 20:10:35 Train Epoch 000:  44%|████▍     | 1100/2502 [30:27<38:27,  1.65s/it, Loss=6.8234, Top1=0.46%, LR=0.100000]2025-11-03 20:13:21,106 - INFO - Step 1100: {'train_loss_batch': 6.353535175323486, 'train_lr': 0.1, 'batch_time': 1.659637555980769, 'data_time': 0.00554190084784817}
2025-11-03 20:13:21 Train Epoch 000:  48%|████▊     | 1200/2502 [33:12<35:46,  1.65s/it, Loss=6.7926, Top1=N/A, LR=0.100000]  2025-11-03 20:16:06,645 - INFO - Step 1200: {'train_loss_batch': 6.304940223693848, 'train_lr': 0.1, 'batch_time': 1.6592836447500567, 'data_time': 0.005146335503342348}
2025-11-03 20:16:06 Train Epoch 000:  52%|█████▏    | 1300/2502 [35:58<33:15,  1.66s/it, Loss=6.7600, Top1=N/A, LR=0.100000]2025-11-03 20:18:52,571 - INFO - Step 1300: {'train_loss_batch': 6.232202053070068, 'train_lr': 0.1, 'batch_time': 1.6592813230861982, 'data_time': 0.004815943693766495}
2025-11-03 20:18:52 Train Epoch 000:  56%|█████▌    | 1400/2502 [38:44<30:28,  1.66s/it, Loss=6.7272, Top1=0.81%, LR=0.100000]2025-11-03 20:21:38,600 - INFO - Step 1400: {'train_loss_batch': 6.145053863525391, 'train_lr': 0.1, 'batch_time': 1.6593535783714606, 'data_time': 0.0045347256289474626}
2025-11-03 20:21:38 Train Epoch 000:  60%|█████▉    | 1500/2502 [41:31<27:46,  1.66s/it, Loss=6.6960, Top1=0.97%, LR=0.100000]2025-11-03 20:24:24,893 - INFO - Step 1500: {'train_loss_batch': 6.083698749542236, 'train_lr': 0.1, 'batch_time': 1.6595921232095168, 'data_time': 0.004292863595493629}
2025-11-03 20:24:24 Train Epoch 000:  64%|██████▍   | 1600/2502 [44:16<24:49,  1.65s/it, Loss=6.6642, Top1=N/A, LR=0.100000]  2025-11-03 20:27:10,457 - INFO - Step 1600: {'train_loss_batch': 5.935922622680664, 'train_lr': 0.1, 'batch_time': 1.659345250364991, 'data_time': 0.004086697123930798}
2025-11-03 20:27:10 Train Epoch 000:  68%|██████▊   | 1700/2502 [47:02<22:14,  1.66s/it, Loss=6.6319, Top1=N/A, LR=0.100000]2025-11-03 20:29:56,108 - INFO - Step 1700: {'train_loss_batch': 5.926428318023682, 'train_lr': 0.1, 'batch_time': 1.6591781304206377, 'data_time': 0.0039056622932967825}
2025-11-03 20:29:56 Train Epoch 000:  72%|███████▏  | 1800/2502 [49:47<19:20,  1.65s/it, Loss=6.6026, Top1=N/A, LR=0.100000]2025-11-03 20:32:41,525 - INFO - Step 1800: {'train_loss_batch': 6.01826810836792, 'train_lr': 0.1, 'batch_time': 1.658900017211465, 'data_time': 0.0037477258706609122}
2025-11-03 20:32:41 Train Epoch 000:  76%|███████▌  | 1900/2502 [52:32<16:26,  1.64s/it, Loss=6.5757, Top1=N/A, LR=0.100000]2025-11-03 20:35:26,083 - INFO - Step 1900: {'train_loss_batch': 5.73761510848999, 'train_lr': 0.1, 'batch_time': 1.658199191030736, 'data_time': 0.003601061927839557}
2025-11-03 20:35:26 Train Epoch 000:  80%|███████▉  | 2000/2502 [55:18<13:51,  1.66s/it, Loss=6.5453, Top1=1.84%, LR=0.100000]2025-11-03 20:38:11,930 - INFO - Step 2000: {'train_loss_batch': 5.709601402282715, 'train_lr': 0.1, 'batch_time': 1.6582123730434053, 'data_time': 0.0034667190940662483}
2025-11-03 20:38:11 Train Epoch 000:  84%|████████▍ | 2100/2502 [58:03<10:57,  1.64s/it, Loss=6.5183, Top1=N/A, LR=0.100000]  2025-11-03 20:40:57,233 - INFO - Step 2100: {'train_loss_batch': 6.4370269775390625, 'train_lr': 0.1, 'batch_time': 1.6579656600952148, 'data_time': 0.0033471828299099803}
2025-11-03 20:40:57 Train Epoch 000:  88%|████████▊ | 2200/2502 [1:00:48<08:23,  1.67s/it, Loss=6.4908, Top1=N/A, LR=0.100000]2025-11-03 20:43:41,844 - INFO - Step 2200: {'train_loss_batch': 5.8206787109375, 'train_lr': 0.1, 'batch_time': 1.657427125494895, 'data_time': 0.003239339397799584}
2025-11-03 20:43:41 Train Epoch 000:  92%|█████████▏| 2300/2502 [1:03:33<05:32,  1.65s/it, Loss=6.4629, Top1=N/A, LR=0.100000]2025-11-03 20:46:27,571 - INFO - Step 2300: {'train_loss_batch': 5.681471824645996, 'train_lr': 0.1, 'batch_time': 1.6574201732032043, 'data_time': 0.0031414931362372593}
2025-11-03 20:46:27 Train Epoch 000:  96%|█████████▌| 2400/2502 [1:06:19<02:49,  1.66s/it, Loss=6.4368, Top1=N/A, LR=0.100000]2025-11-03 20:49:13,360 - INFO - Step 2400: {'train_loss_batch': 5.475764751434326, 'train_lr': 0.1, 'batch_time': 1.6574394373236374, 'data_time': 0.0030516077706536767}
2025-11-03 20:49:13 Train Epoch 000: 100%|█████████▉| 2500/2502 [1:09:05<00:03,  1.66s/it, Loss=6.4121, Top1=2.77%, LR=0.100000]2025-11-03 20:51:59,454 - INFO - Step 2500: {'train_loss_batch': 5.576394081115723, 'train_lr': 0.1, 'batch_time': 1.6575796538379277, 'data_time': 0.003031406150918539}
2025-11-03 20:51:59 Train Epoch 000: 100%|██████████| 2502/2502 [1:09:07<00:00,  1.66s/it, Loss=6.4121, Top1=2.77%, LR=0.100000]
2025-11-03 20:52:01 Val Epoch 000:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-03 20:52:06   with torch.cuda.amp.autocast():
2025-11-03 20:52:06 Val Epoch 000: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=5.3634, Top1=9.16%, Top5=24.15%] 
2025-11-03 20:53:52 2025-11-03 20:53:52,214 - INFO - Step 0: {'epoch': 0, 'learning_rate': 1e-06, 'train_loss': 6.412045545143475, 'train_top1': 2.7748899217221137, 'train_top5': 8.652611301369863, 'train_precision': 2.904720896135699, 'train_recall': 2.735307207454258, 'train_f1': 2.263019735574086, 'val_loss': 5.36335177444458, 'val_top1': 9.160000001831055, 'val_top5': 24.1480000012207, 'val_precision': 8.309538520761885, 'val_recall': 9.158, 'val_f1': 6.189705425616483}
2025-11-03 20:53:52 2025-11-03 20:53:52,216 - INFO - Epoch 000 Summary - LR: 0.000001, Train Loss: 6.4120, Val Loss: 5.3634, Val F1: 6.19%, Val Precision: 8.31%, Val Recall: 9.16%
2025-11-03 20:53:54 wandb: WARNING Tried to log to step 0 that is less than the current step 2500. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-03 20:53:54 2025-11-03 20:53:54,372 - INFO - New best model saved with validation accuracy: 9.160%
2025-11-03 20:53:54 2025-11-03 20:53:54,373 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_001.pth
2025-11-03 20:53:54 Train Epoch 001:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-03 20:53:58   with torch.cuda.amp.autocast():
2025-11-03 20:54:00 Train Epoch 001:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=6.2920, Top1=N/A, LR=0.000001]2025-11-03 20:54:00,293 - INFO - Step 2502: {'train_loss_batch': 6.2919769287109375, 'train_lr': 1e-06, 'batch_time': 5.9176859855651855, 'data_time': 4.268007040023804}
2025-11-03 20:54:00 Train Epoch 001:   4%|▍         | 100/2502 [02:52<1:06:51,  1.67s/it, Loss=5.7962, Top1=N/A, LR=0.000001]2025-11-03 20:56:46,495 - INFO - Step 2602: {'train_loss_batch': 5.891415596008301, 'train_lr': 1e-06, 'batch_time': 1.7041592125845428, 'data_time': 0.0432730500060733}
2025-11-03 20:56:46 Train Epoch 001:   8%|▊         | 200/2502 [05:38<1:03:35,  1.66s/it, Loss=5.7869, Top1=8.12%, LR=0.000001]2025-11-03 20:59:32,467 - INFO - Step 2702: {'train_loss_batch': 5.491551876068115, 'train_lr': 1e-06, 'batch_time': 1.6820491795516133, 'data_time': 0.022218126562697376}
2025-11-03 20:59:32 Train Epoch 001:  12%|█▏        | 300/2502 [08:24<1:01:02,  1.66s/it, Loss=5.7691, Top1=N/A, LR=0.000001]  2025-11-03 21:02:18,853 - INFO - Step 2802: {'train_loss_batch': 5.4897780418396, 'train_lr': 1e-06, 'batch_time': 1.6760058300043656, 'data_time': 0.01516353885992817}
2025-11-03 21:02:18 Train Epoch 001:  16%|█▌        | 400/2502 [11:10<57:37,  1.64s/it, Loss=5.7824, Top1=N/A, LR=0.000001]2025-11-03 21:05:04,709 - INFO - Step 2902: {'train_loss_batch': 6.2716383934021, 'train_lr': 1e-06, 'batch_time': 1.6716549836489327, 'data_time': 0.011636418891963816}
2025-11-03 21:05:04 Train Epoch 001:  20%|█▉        | 500/2502 [13:55<54:33,  1.64s/it, Loss=5.7786, Top1=N/A, LR=0.000001]2025-11-03 21:07:49,634 - INFO - Step 3002: {'train_loss_batch': 6.3117804527282715, 'train_lr': 1e-06, 'batch_time': 1.6671824650374238, 'data_time': 0.009507835982088557}
2025-11-03 21:07:49 Train Epoch 001:  24%|██▍       | 600/2502 [16:39<52:30,  1.66s/it, Loss=5.7830, Top1=N/A, LR=0.000001]2025-11-03 21:10:34,225 - INFO - Step 3102: {'train_loss_batch': 5.476878643035889, 'train_lr': 1e-06, 'batch_time': 1.6636430249237975, 'data_time': 0.008087154236093734}
2025-11-03 21:10:34 Train Epoch 001:  28%|██▊       | 700/2502 [19:25<49:36,  1.65s/it, Loss=5.7797, Top1=N/A, LR=0.000001]2025-11-03 21:13:20,167 - INFO - Step 3202: {'train_loss_batch': 5.5586018562316895, 'train_lr': 1e-06, 'batch_time': 1.6630399584260034, 'data_time': 0.00708172800877635}
2025-11-03 21:13:20 Train Epoch 001:  32%|███▏      | 800/2502 [22:12<47:11,  1.66s/it, Loss=5.7778, Top1=N/A, LR=0.000001]2025-11-03 21:16:06,519 - INFO - Step 3302: {'train_loss_batch': 6.324275493621826, 'train_lr': 1e-06, 'batch_time': 1.663098827878783, 'data_time': 0.006322290120499857}
2025-11-03 21:16:06 Train Epoch 001:  36%|███▌      | 900/2502 [24:57<44:13,  1.66s/it, Loss=5.7754, Top1=N/A, LR=0.000001]2025-11-03 21:18:51,616 - INFO - Step 3402: {'train_loss_batch': 5.565582752227783, 'train_lr': 1e-06, 'batch_time': 1.6617530985227833, 'data_time': 0.005728207735322027}
2025-11-03 21:18:51 Train Epoch 001:  40%|███▉      | 1000/2502 [27:41<41:16,  1.65s/it, Loss=5.7750, Top1=N/A, LR=0.000001]2025-11-03 21:21:36,122 - INFO - Step 3502: {'train_loss_batch': 6.211291313171387, 'train_lr': 1e-06, 'batch_time': 1.6600845827089323, 'data_time': 0.005256374637325565}
2025-11-03 21:21:36 Train Epoch 001:  44%|████▍     | 1100/2502 [30:28<38:57,  1.67s/it, Loss=5.7803, Top1=N/A, LR=0.000001]2025-11-03 21:24:22,383 - INFO - Step 3602: {'train_loss_batch': 5.640265464782715, 'train_lr': 1e-06, 'batch_time': 1.660314388647608, 'data_time': 0.004866793846456058}
2025-11-03 21:24:22 Train Epoch 001:  48%|████▊     | 1200/2502 [33:13<35:58,  1.66s/it, Loss=5.7799, Top1=N/A, LR=0.000001]2025-11-03 21:27:08,366 - INFO - Step 3702: {'train_loss_batch': 6.081864356994629, 'train_lr': 1e-06, 'batch_time': 1.6602735831080428, 'data_time': 0.004543907735667359}
2025-11-03 21:27:08 Train Epoch 001:  52%|█████▏    | 1300/2502 [36:00<33:18,  1.66s/it, Loss=5.7811, Top1=N/A, LR=0.000001]2025-11-03 21:29:54,394 - INFO - Step 3802: {'train_loss_batch': 5.754330635070801, 'train_lr': 1e-06, 'batch_time': 1.6602738982984233, 'data_time': 0.0042707331816477555}
2025-11-03 21:29:54 Train Epoch 001:  56%|█████▌    | 1400/2502 [38:46<30:28,  1.66s/it, Loss=5.7816, Top1=N/A, LR=0.000001]2025-11-03 21:32:40,786 - INFO - Step 3902: {'train_loss_batch': 5.468851089477539, 'train_lr': 1e-06, 'batch_time': 1.6605336907759127, 'data_time': 0.004037206467350069}
2025-11-03 21:32:40 Train Epoch 001:  60%|█████▉    | 1500/2502 [41:32<27:44,  1.66s/it, Loss=5.7775, Top1=N/A, LR=0.000001]2025-11-03 21:35:26,984 - INFO - Step 4002: {'train_loss_batch': 5.857131004333496, 'train_lr': 1e-06, 'batch_time': 1.6606303910109934, 'data_time': 0.0038370285567881504}
2025-11-03 21:35:26 Train Epoch 001:  64%|██████▍   | 1600/2502 [44:18<24:44,  1.65s/it, Loss=5.7771, Top1=7.95%, LR=0.000001]2025-11-03 21:38:12,449 - INFO - Step 4102: {'train_loss_batch': 5.487142086029053, 'train_lr': 1e-06, 'batch_time': 1.660256782373289, 'data_time': 0.0036623629832699626}
2025-11-03 21:38:12 Train Epoch 001:  68%|██████▊   | 1700/2502 [47:03<22:05,  1.65s/it, Loss=5.7770, Top1=N/A, LR=0.000001]  2025-11-03 21:40:57,915 - INFO - Step 4202: {'train_loss_batch': 5.426335334777832, 'train_lr': 1e-06, 'batch_time': 1.6599276727399428, 'data_time': 0.0035041766191916492}
2025-11-03 21:40:57 Train Epoch 001:  72%|███████▏  | 1800/2502 [49:49<19:32,  1.67s/it, Loss=5.7777, Top1=N/A, LR=0.000001]2025-11-03 21:43:44,224 - INFO - Step 4302: {'train_loss_batch': 5.615945816040039, 'train_lr': 1e-06, 'batch_time': 1.6601030765673241, 'data_time': 0.003368622458424587}
2025-11-03 21:43:44 Train Epoch 001:  76%|███████▌  | 1900/2502 [52:35<16:43,  1.67s/it, Loss=5.7772, Top1=N/A, LR=0.000001]2025-11-03 21:46:29,914 - INFO - Step 4402: {'train_loss_batch': 5.755578517913818, 'train_lr': 1e-06, 'batch_time': 1.6599343889076918, 'data_time': 0.0032437345594057967}
2025-11-03 21:46:29 Train Epoch 001:  80%|███████▉  | 2000/2502 [55:21<13:49,  1.65s/it, Loss=5.7741, Top1=8.01%, LR=0.000001]2025-11-03 21:49:16,115 - INFO - Step 4502: {'train_loss_batch': 5.481349945068359, 'train_lr': 1e-06, 'batch_time': 1.6600380466914904, 'data_time': 0.00312762424863618}
2025-11-03 21:49:16 Train Epoch 001:  84%|████████▍ | 2100/2502 [58:07<11:05,  1.66s/it, Loss=5.7741, Top1=8.04%, LR=0.000001]2025-11-03 21:52:01,966 - INFO - Step 4602: {'train_loss_batch': 5.4586663246154785, 'train_lr': 1e-06, 'batch_time': 1.6599653638697192, 'data_time': 0.0030236500663339723}
2025-11-03 21:52:01 Train Epoch 001:  88%|████████▊ | 2200/2502 [1:00:53<08:21,  1.66s/it, Loss=5.7765, Top1=N/A, LR=0.000001]  2025-11-03 21:54:47,849 - INFO - Step 4702: {'train_loss_batch': 5.989096641540527, 'train_lr': 1e-06, 'batch_time': 1.6599135808325096, 'data_time': 0.0029313513388800547}
2025-11-03 21:54:47 Train Epoch 001:  92%|█████████▏| 2300/2502 [1:03:39<05:35,  1.66s/it, Loss=5.7775, Top1=N/A, LR=0.000001]2025-11-03 21:57:33,389 - INFO - Step 4802: {'train_loss_batch': 5.880761623382568, 'train_lr': 1e-06, 'batch_time': 1.6597172082480738, 'data_time': 0.002846460557926846}
2025-11-03 21:57:33 Train Epoch 001:  96%|█████████▌| 2400/2502 [1:06:24<02:49,  1.66s/it, Loss=5.7795, Top1=N/A, LR=0.000001]2025-11-03 22:00:19,270 - INFO - Step 4902: {'train_loss_batch': 5.598176956176758, 'train_lr': 1e-06, 'batch_time': 1.6596795306112408, 'data_time': 0.002769188898794356}
2025-11-03 22:00:19 Train Epoch 001: 100%|█████████▉| 2500/2502 [1:09:11<00:03,  1.66s/it, Loss=5.7771, Top1=N/A, LR=0.000001]2025-11-03 22:03:05,793 - INFO - Step 5002: {'train_loss_batch': 6.203849792480469, 'train_lr': 1e-06, 'batch_time': 1.6599013675741556, 'data_time': 0.0027331827355117524}
2025-11-03 22:03:05 Train Epoch 001: 100%|██████████| 2502/2502 [1:09:13<00:00,  1.66s/it, Loss=5.7771, Top1=N/A, LR=0.000001]
2025-11-03 22:03:08 Val Epoch 001:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-03 22:03:12   with torch.cuda.amp.autocast():
2025-11-03 22:03:12 Val Epoch 001: 100%|██████████| 98/98 [01:49<00:00,  1.11s/it, Loss=5.2749, Top1=10.00%, Top5=25.50%]
2025-11-03 22:04:57 2025-11-03 22:04:57,249 - INFO - Step 1: {'epoch': 1, 'learning_rate': 0.020000800000000003, 'train_loss': 5.776971587555395, 'train_top1': 8.054903656126482, 'train_top5': 21.945636734189723, 'train_precision': 8.836105602521931, 'train_recall': 8.021911166928238, 'train_f1': 5.6860948730885275, 'val_loss': 5.274937259979248, 'val_top1': 9.996000001220704, 'val_top5': 25.495999985351563, 'val_precision': 9.631915767046396, 'val_recall': 9.998, 'val_f1': 6.973711764983315}
2025-11-03 22:04:57 2025-11-03 22:04:57,251 - INFO - Epoch 001 Summary - LR: 0.020001, Train Loss: 5.7770, Val Loss: 5.2749, Val F1: 6.97%, Val Precision: 9.63%, Val Recall: 10.00%
2025-11-03 22:05:00 2025-11-03 22:05:00,916 - INFO - New best model saved with validation accuracy: 9.996%
2025-11-03 22:05:00 2025-11-03 22:05:00,916 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_002.pth
2025-11-03 22:05:00 Train Epoch 002:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 1 that is less than the current step 5002. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-03 22:05:04 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-03 22:05:04   with torch.cuda.amp.autocast():
2025-11-03 22:05:06 Train Epoch 002:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=5.6875, Top1=N/A, LR=0.020001]2025-11-03 22:05:06,575 - INFO - Step 5004: {'train_loss_batch': 5.687453269958496, 'train_lr': 0.020000800000000003, 'batch_time': 5.656140565872192, 'data_time': 4.007433652877808}
2025-11-03 22:05:06 Train Epoch 002:   4%|▍         | 100/2502 [02:51<1:06:02,  1.65s/it, Loss=5.7078, Top1=N/A, LR=0.020001]2025-11-03 22:07:52,358 - INFO - Step 5104: {'train_loss_batch': 5.310043811798096, 'train_lr': 0.020000800000000003, 'batch_time': 1.6974260051651757, 'data_time': 0.04069465221744953}
2025-11-03 22:07:52 Train Epoch 002:   8%|▊         | 200/2502 [05:37<1:03:10,  1.65s/it, Loss=5.6755, Top1=10.37%, LR=0.020001]2025-11-03 22:10:38,254 - INFO - Step 5204: {'train_loss_batch': 5.328276634216309, 'train_lr': 0.020000800000000003, 'batch_time': 1.678286436185315, 'data_time': 0.020939797311279904}
2025-11-03 22:10:38 Train Epoch 002:  12%|█▏        | 300/2502 [08:22<1:01:05,  1.66s/it, Loss=5.6622, Top1=N/A, LR=0.020001]   2025-11-03 22:13:23,163 - INFO - Step 5304: {'train_loss_batch': 6.0315752029418945, 'train_lr': 0.020000800000000003, 'batch_time': 1.6685823515008058, 'data_time': 0.014323575948163917}
2025-11-03 22:13:23 Train Epoch 002:  16%|█▌        | 400/2502 [11:08<57:32,  1.64s/it, Loss=5.6588, Top1=10.75%, LR=0.020001]2025-11-03 22:16:09,104 - INFO - Step 5404: {'train_loss_batch': 5.242534637451172, 'train_lr': 0.020000800000000003, 'batch_time': 1.666296374470813, 'data_time': 0.010992422960048305}
2025-11-03 22:16:09 Train Epoch 002:  20%|█▉        | 500/2502 [13:54<55:51,  1.67s/it, Loss=5.6433, Top1=10.89%, LR=0.020001]2025-11-03 22:18:55,196 - INFO - Step 5504: {'train_loss_batch': 5.258273124694824, 'train_lr': 0.020000800000000003, 'batch_time': 1.6652235470846029, 'data_time': 0.008997201919555664}
2025-11-03 22:18:55 Train Epoch 002:  24%|██▍       | 600/2502 [16:40<52:46,  1.66s/it, Loss=5.6343, Top1=N/A, LR=0.020001]   2025-11-03 22:21:41,702 - INFO - Step 5604: {'train_loss_batch': 6.118147850036621, 'train_lr': 0.020000800000000003, 'batch_time': 1.6651965270621607, 'data_time': 0.007664171114142445}
2025-11-03 22:21:41 Train Epoch 002:  28%|██▊       | 700/2502 [19:26<49:10,  1.64s/it, Loss=5.6225, Top1=N/A, LR=0.020001]2025-11-03 22:24:27,379 - INFO - Step 5704: {'train_loss_batch': 6.201568603515625, 'train_lr': 0.020000800000000003, 'batch_time': 1.66399435820151, 'data_time': 0.0067059524389204385}
2025-11-03 22:24:27 Train Epoch 002:  32%|███▏      | 800/2502 [22:11<47:21,  1.67s/it, Loss=5.6123, Top1=N/A, LR=0.020001]2025-11-03 22:27:12,441 - INFO - Step 5804: {'train_loss_batch': 5.207479476928711, 'train_lr': 0.020000800000000003, 'batch_time': 1.6623241728760032, 'data_time': 0.0059762998168983415}
2025-11-03 22:27:12 Train Epoch 002:  36%|███▌      | 900/2502 [24:57<43:58,  1.65s/it, Loss=5.6042, Top1=N/A, LR=0.020001]2025-11-03 22:29:58,052 - INFO - Step 5904: {'train_loss_batch': 6.099303245544434, 'train_lr': 0.020000800000000003, 'batch_time': 1.661633537029982, 'data_time': 0.005402793630246449}
2025-11-03 22:29:58 Train Epoch 002:  40%|███▉      | 1000/2502 [27:42<41:32,  1.66s/it, Loss=5.5892, Top1=N/A, LR=0.020001]2025-11-03 22:32:43,630 - INFO - Step 6004: {'train_loss_batch': 5.926722526550293, 'train_lr': 0.020000800000000003, 'batch_time': 1.6610485583275825, 'data_time': 0.004944694863928186}
2025-11-03 22:32:43 Train Epoch 002:  44%|████▍     | 1100/2502 [30:28<38:53,  1.66s/it, Loss=5.5862, Top1=N/A, LR=0.020001]2025-11-03 22:35:29,780 - INFO - Step 6104: {'train_loss_batch': 5.911369323730469, 'train_lr': 0.020000800000000003, 'batch_time': 1.6610897800036715, 'data_time': 0.004568703059820995}
2025-11-03 22:35:29 Train Epoch 002:  48%|████▊     | 1200/2502 [33:14<36:05,  1.66s/it, Loss=5.5814, Top1=N/A, LR=0.020001]2025-11-03 22:38:15,285 - INFO - Step 6204: {'train_loss_batch': 5.960651397705078, 'train_lr': 0.020000800000000003, 'batch_time': 1.6605869522698218, 'data_time': 0.004256913306611861}
2025-11-03 22:38:15 Train Epoch 002:  52%|█████▏    | 1300/2502 [35:59<33:22,  1.67s/it, Loss=5.5705, Top1=N/A, LR=0.020001]2025-11-03 22:41:00,209 - INFO - Step 6304: {'train_loss_batch': 6.005825519561768, 'train_lr': 0.020000800000000003, 'batch_time': 1.6597146423480587, 'data_time': 0.003991999505209428}
2025-11-03 22:41:00 Train Epoch 002:  56%|█████▌    | 1400/2502 [38:45<30:42,  1.67s/it, Loss=5.5594, Top1=N/A, LR=0.020001]2025-11-03 22:43:46,070 - INFO - Step 6404: {'train_loss_batch': 6.098828315734863, 'train_lr': 0.020000800000000003, 'batch_time': 1.6596358469092447, 'data_time': 0.0037657959302947147}
2025-11-03 22:43:46 Train Epoch 002:  60%|█████▉    | 1500/2502 [41:30<27:33,  1.65s/it, Loss=5.5500, Top1=12.23%, LR=0.020001]2025-11-03 22:46:31,787 - INFO - Step 6504: {'train_loss_batch': 5.013534069061279, 'train_lr': 0.020000800000000003, 'batch_time': 1.6594711793890642, 'data_time': 0.0035701789195183354}
2025-11-03 22:46:31 Train Epoch 002:  64%|██████▍   | 1600/2502 [44:16<24:56,  1.66s/it, Loss=5.5389, Top1=N/A, LR=0.020001]   2025-11-03 22:49:17,323 - INFO - Step 6604: {'train_loss_batch': 5.752974033355713, 'train_lr': 0.020000800000000003, 'batch_time': 1.6592140294550957, 'data_time': 0.00339856138831001}
2025-11-03 22:49:17 Train Epoch 002:  68%|██████▊   | 1700/2502 [47:02<22:01,  1.65s/it, Loss=5.5268, Top1=N/A, LR=0.020001]2025-11-03 22:52:03,286 - INFO - Step 6704: {'train_loss_batch': 5.055842876434326, 'train_lr': 0.020000800000000003, 'batch_time': 1.6592385903447604, 'data_time': 0.003246777622507713}
2025-11-03 22:52:03 Train Epoch 002:  72%|███████▏  | 1800/2502 [49:48<19:31,  1.67s/it, Loss=5.5186, Top1=N/A, LR=0.020001]2025-11-03 22:54:49,546 - INFO - Step 6804: {'train_loss_batch': 5.828384876251221, 'train_lr': 0.020000800000000003, 'batch_time': 1.6594249726930372, 'data_time': 0.0031124221689498538}
2025-11-03 22:54:49 Train Epoch 002:  76%|███████▌  | 1900/2502 [52:34<16:30,  1.65s/it, Loss=5.5100, Top1=N/A, LR=0.020001]2025-11-03 22:57:35,589 - INFO - Step 6904: {'train_loss_batch': 6.02730131149292, 'train_lr': 0.020000800000000003, 'batch_time': 1.6594777682904882, 'data_time': 0.002993430920740857}
2025-11-03 22:57:35 Train Epoch 002:  80%|███████▉  | 2000/2502 [55:18<13:49,  1.65s/it, Loss=5.5023, Top1=N/A, LR=0.020001]2025-11-03 23:00:19,673 - INFO - Step 7004: {'train_loss_batch': 5.342758655548096, 'train_lr': 0.020000800000000003, 'batch_time': 1.658546568571717, 'data_time': 0.002884508430332258}
2025-11-03 23:00:19 Train Epoch 002:  84%|████████▍ | 2100/2502 [58:04<11:06,  1.66s/it, Loss=5.5005, Top1=N/A, LR=0.020001]2025-11-03 23:03:05,819 - INFO - Step 7104: {'train_loss_batch': 5.975632667541504, 'train_lr': 0.020000800000000003, 'batch_time': 1.6586850264820243, 'data_time': 0.0027864926886070574}
2025-11-03 23:03:05 Train Epoch 002:  88%|████████▊ | 2200/2502 [1:00:50<08:17,  1.65s/it, Loss=5.4943, Top1=N/A, LR=0.020001]2025-11-03 23:05:51,771 - INFO - Step 7204: {'train_loss_batch': 4.960525989532471, 'train_lr': 0.020000800000000003, 'batch_time': 1.6587226365924368, 'data_time': 0.0026983087141911367}
2025-11-03 23:05:51 Train Epoch 002:  92%|█████████▏| 2300/2502 [1:03:36<05:35,  1.66s/it, Loss=5.4861, Top1=13.19%, LR=0.020001]2025-11-03 23:08:37,526 - INFO - Step 7304: {'train_loss_batch': 4.843510150909424, 'train_lr': 0.020000800000000003, 'batch_time': 1.6586716381273596, 'data_time': 0.00261864347180197}
2025-11-03 23:08:37 Train Epoch 002:  96%|█████████▌| 2400/2502 [1:06:22<02:49,  1.66s/it, Loss=5.4821, Top1=N/A, LR=0.020001]   2025-11-03 23:11:23,115 - INFO - Step 7404: {'train_loss_batch': 4.817378997802734, 'train_lr': 0.020000800000000003, 'batch_time': 1.6585557257220527, 'data_time': 0.002549491788585302}
2025-11-03 23:11:23 Train Epoch 002: 100%|█████████▉| 2500/2502 [1:09:07<00:03,  1.67s/it, Loss=5.4760, Top1=N/A, LR=0.020001]2025-11-03 23:14:08,870 - INFO - Step 7504: {'train_loss_batch': 5.431370735168457, 'train_lr': 0.020000800000000003, 'batch_time': 1.6585154117750482, 'data_time': 0.002539027266290749}
2025-11-03 23:14:08 Train Epoch 002: 100%|██████████| 2502/2502 [1:09:09<00:00,  1.66s/it, Loss=5.4760, Top1=N/A, LR=0.020001]
2025-11-03 23:14:11 Val Epoch 002:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-03 23:14:15   with torch.cuda.amp.autocast():
2025-11-03 23:14:15 Val Epoch 002: 100%|██████████| 98/98 [01:51<00:00,  1.13s/it, Loss=4.8281, Top1=16.05%, Top5=36.34%]
2025-11-03 23:16:02 2025-11-03 23:16:02,297 - INFO - Step 2: {'epoch': 2, 'learning_rate': 0.040000600000000004, 'train_loss': 5.475749054115167, 'train_top1': 13.408734309623432, 'train_top5': 31.40567795502092, 'train_precision': 11.236722254070875, 'train_recall': 13.319432671300197, 'train_f1': 11.014665194799546, 'val_loss': 4.828121708526611, 'val_top1': 16.051999993286135, 'val_top5': 36.34199998535156, 'val_precision': 17.5071841863138, 'val_recall': 16.058, 'val_f1': 13.40446904873115}
2025-11-03 23:16:02 2025-11-03 23:16:02,299 - INFO - Epoch 002 Summary - LR: 0.040001, Train Loss: 5.4757, Val Loss: 4.8281, Val F1: 13.40%, Val Precision: 17.51%, Val Recall: 16.06%
2025-11-03 23:16:05 2025-11-03 23:16:05,319 - INFO - New best model saved with validation accuracy: 16.052%
2025-11-03 23:16:05 2025-11-03 23:16:05,320 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_003.pth
2025-11-03 23:16:05 Train Epoch 003:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 2 that is less than the current step 7504. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-03 23:16:09 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-03 23:16:09   with torch.cuda.amp.autocast():
2025-11-03 23:16:10 Train Epoch 003:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.8562, Top1=N/A, LR=0.040001]2025-11-03 23:16:10,753 - INFO - Step 7506: {'train_loss_batch': 4.856157302856445, 'train_lr': 0.040000600000000004, 'batch_time': 5.4306745529174805, 'data_time': 3.783123254776001}
2025-11-03 23:16:10 Train Epoch 003:   4%|▍         | 100/2502 [02:51<1:06:42,  1.67s/it, Loss=5.3146, Top1=N/A, LR=0.040001]2025-11-03 23:18:57,052 - INFO - Step 7606: {'train_loss_batch': 5.0348381996154785, 'train_lr': 0.040000600000000004, 'batch_time': 1.70030680269298, 'data_time': 0.03849920659962267}
2025-11-03 23:18:57 Train Epoch 003:   8%|▊         | 200/2502 [05:37<1:03:18,  1.65s/it, Loss=5.2937, Top1=N/A, LR=0.040001]2025-11-03 23:21:42,907 - INFO - Step 7706: {'train_loss_batch': 5.395061016082764, 'train_lr': 0.040000600000000004, 'batch_time': 1.6795322135906314, 'data_time': 0.019846046741922103}
2025-11-03 23:21:42 Train Epoch 003:  12%|█▏        | 300/2502 [08:23<1:01:11,  1.67s/it, Loss=5.3004, Top1=N/A, LR=0.040001]2025-11-03 23:24:29,098 - INFO - Step 7806: {'train_loss_batch': 5.346922874450684, 'train_lr': 0.040000600000000004, 'batch_time': 1.6736765533586673, 'data_time': 0.013593658656376937}
2025-11-03 23:24:29 Train Epoch 003:  16%|█▌        | 400/2502 [11:07<57:36,  1.64s/it, Loss=5.2703, Top1=N/A, LR=0.040001]2025-11-03 23:27:13,153 - INFO - Step 7906: {'train_loss_batch': 5.587953567504883, 'train_lr': 0.040000600000000004, 'batch_time': 1.665414819693625, 'data_time': 0.010425974306025708}
2025-11-03 23:27:13 Train Epoch 003:  20%|█▉        | 500/2502 [13:53<55:12,  1.65s/it, Loss=5.2980, Top1=N/A, LR=0.040001]2025-11-03 23:29:59,098 - INFO - Step 8006: {'train_loss_batch': 6.031671524047852, 'train_lr': 0.040000600000000004, 'batch_time': 1.66422306563326, 'data_time': 0.008547415514430124}
2025-11-03 23:29:59 Train Epoch 003:  24%|██▍       | 600/2502 [16:40<52:26,  1.65s/it, Loss=5.3073, Top1=N/A, LR=0.040001]2025-11-03 23:32:45,690 - INFO - Step 8106: {'train_loss_batch': 5.418610095977783, 'train_lr': 0.040000600000000004, 'batch_time': 1.6645061862647237, 'data_time': 0.0072881136083364885}
2025-11-03 23:32:45 Train Epoch 003:  28%|██▊       | 700/2502 [19:25<49:51,  1.66s/it, Loss=5.2983, Top1=N/A, LR=0.040001]2025-11-03 23:35:31,112 - INFO - Step 8206: {'train_loss_batch': 4.736310005187988, 'train_lr': 0.040000600000000004, 'batch_time': 1.663037795993299, 'data_time': 0.006383246940144798}
2025-11-03 23:35:31 Train Epoch 003:  32%|███▏      | 800/2502 [22:11<47:17,  1.67s/it, Loss=5.2868, Top1=N/A, LR=0.040001]2025-11-03 23:38:16,732 - INFO - Step 8306: {'train_loss_batch': 4.934534072875977, 'train_lr': 0.040000600000000004, 'batch_time': 1.6621840026941193, 'data_time': 0.00569470902060748}
2025-11-03 23:38:16 Train Epoch 003:  36%|███▌      | 900/2502 [24:57<44:14,  1.66s/it, Loss=5.2843, Top1=N/A, LR=0.040001]2025-11-03 23:41:02,937 - INFO - Step 8406: {'train_loss_batch': 5.899380207061768, 'train_lr': 0.040000600000000004, 'batch_time': 1.6621692402910577, 'data_time': 0.0051569835459617085}
2025-11-03 23:41:02 Train Epoch 003:  40%|███▉      | 1000/2502 [27:43<41:32,  1.66s/it, Loss=5.2638, Top1=N/A, LR=0.040001]2025-11-03 23:43:49,257 - INFO - Step 8506: {'train_loss_batch': 4.9062089920043945, 'train_lr': 0.040000600000000004, 'batch_time': 1.662272356845044, 'data_time': 0.0047304897041587565}
2025-11-03 23:43:49 Train Epoch 003:  44%|████▍     | 1100/2502 [30:29<38:44,  1.66s/it, Loss=5.2551, Top1=N/A, LR=0.040001]2025-11-03 23:46:35,181 - INFO - Step 8606: {'train_loss_batch': 5.760093688964844, 'train_lr': 0.040000600000000004, 'batch_time': 1.661996169484387, 'data_time': 0.0043783999918592074}
2025-11-03 23:46:35 Train Epoch 003:  48%|████▊     | 1200/2502 [33:16<36:02,  1.66s/it, Loss=5.2538, Top1=17.06%, LR=0.040001]2025-11-03 23:49:21,614 - INFO - Step 8706: {'train_loss_batch': 4.706894874572754, 'train_lr': 0.040000600000000004, 'batch_time': 1.6621905613898438, 'data_time': 0.004082680344085312}
2025-11-03 23:49:21 Train Epoch 003:  52%|█████▏    | 1300/2502 [36:02<33:14,  1.66s/it, Loss=5.2459, Top1=N/A, LR=0.040001]   2025-11-03 23:52:07,520 - INFO - Step 8806: {'train_loss_batch': 4.80460262298584, 'train_lr': 0.040000600000000004, 'batch_time': 1.6619495533687347, 'data_time': 0.0038345992610969515}
2025-11-03 23:52:07 Train Epoch 003:  56%|█████▌    | 1400/2502 [38:47<30:14,  1.65s/it, Loss=5.2330, Top1=17.45%, LR=0.040001]2025-11-03 23:54:53,149 - INFO - Step 8906: {'train_loss_batch': 4.702958583831787, 'train_lr': 0.040000600000000004, 'batch_time': 1.661545754670246, 'data_time': 0.0036169795118681796}
2025-11-03 23:54:53 Train Epoch 003:  60%|█████▉    | 1500/2502 [41:33<27:27,  1.64s/it, Loss=5.2235, Top1=N/A, LR=0.040001]   2025-11-03 23:57:38,649 - INFO - Step 9006: {'train_loss_batch': 5.160840034484863, 'train_lr': 0.040000600000000004, 'batch_time': 1.6611092147789026, 'data_time': 0.003430257393152693}
2025-11-03 23:57:38 Train Epoch 003:  64%|██████▍   | 1600/2502 [44:18<24:41,  1.64s/it, Loss=5.2220, Top1=N/A, LR=0.040001]2025-11-04 00:00:23,503 - INFO - Step 9106: {'train_loss_batch': 5.91529655456543, 'train_lr': 0.040000600000000004, 'batch_time': 1.6603240814899967, 'data_time': 0.0032675097987921366}
2025-11-04 00:00:23 Train Epoch 003:  68%|██████▊   | 1700/2502 [47:03<22:20,  1.67s/it, Loss=5.2103, Top1=N/A, LR=0.040001]2025-11-04 00:03:09,205 - INFO - Step 9206: {'train_loss_batch': 5.62419319152832, 'train_lr': 0.040000600000000004, 'batch_time': 1.660130070350508, 'data_time': 0.003125486480425835}
2025-11-04 00:03:09 Train Epoch 003:  72%|███████▏  | 1800/2502 [49:49<19:16,  1.65s/it, Loss=5.2019, Top1=N/A, LR=0.040001]2025-11-04 00:05:54,794 - INFO - Step 9306: {'train_loss_batch': 4.546860694885254, 'train_lr': 0.040000600000000004, 'batch_time': 1.659894016965901, 'data_time': 0.0029974977947618484}
2025-11-04 00:05:54 Train Epoch 003:  76%|███████▌  | 1900/2502 [52:35<16:43,  1.67s/it, Loss=5.1928, Top1=N/A, LR=0.040001]2025-11-04 00:08:40,853 - INFO - Step 9406: {'train_loss_batch': 4.492934226989746, 'train_lr': 0.040000600000000004, 'batch_time': 1.6599308640501613, 'data_time': 0.002883608500497207}
2025-11-04 00:08:40 Train Epoch 003:  80%|███████▉  | 2000/2502 [55:21<13:52,  1.66s/it, Loss=5.1858, Top1=N/A, LR=0.040001]2025-11-04 00:11:26,938 - INFO - Step 9506: {'train_loss_batch': 4.664524555206299, 'train_lr': 0.040000600000000004, 'batch_time': 1.6599769222921041, 'data_time': 0.002782215421525077}
2025-11-04 00:11:26 Train Epoch 003:  84%|████████▍ | 2100/2502 [58:07<11:04,  1.65s/it, Loss=5.1736, Top1=N/A, LR=0.040001]2025-11-04 00:14:12,975 - INFO - Step 9606: {'train_loss_batch': 4.476241111755371, 'train_lr': 0.040000600000000004, 'batch_time': 1.6599951984200347, 'data_time': 0.0026892766902584055}
2025-11-04 00:14:12 Train Epoch 003:  88%|████████▊ | 2200/2502 [1:00:53<08:21,  1.66s/it, Loss=5.1680, Top1=N/A, LR=0.040001]2025-11-04 00:16:58,480 - INFO - Step 9706: {'train_loss_batch': 4.419442176818848, 'train_lr': 0.040000600000000004, 'batch_time': 1.659770423312016, 'data_time': 0.002605065168547988}
2025-11-04 00:16:58 Train Epoch 003:  92%|█████████▏| 2300/2502 [1:03:39<05:37,  1.67s/it, Loss=5.1655, Top1=N/A, LR=0.040001]2025-11-04 00:19:45,006 - INFO - Step 9806: {'train_loss_batch': 5.09912109375, 'train_lr': 0.040000600000000004, 'batch_time': 1.6600092199251788, 'data_time': 0.002528370903243919}
2025-11-04 00:19:45 Train Epoch 003:  96%|█████████▌| 2400/2502 [1:06:25<02:49,  1.66s/it, Loss=5.1626, Top1=N/A, LR=0.040001]2025-11-04 00:22:31,110 - INFO - Step 9906: {'train_loss_batch': 6.148202896118164, 'train_lr': 0.040000600000000004, 'batch_time': 1.660051830109831, 'data_time': 0.0024587423887812857}
2025-11-04 00:22:31 Train Epoch 003: 100%|█████████▉| 2500/2502 [1:09:11<00:03,  1.65s/it, Loss=5.1539, Top1=N/A, LR=0.040001]2025-11-04 00:25:16,847 - INFO - Step 10006: {'train_loss_batch': 6.0415472984313965, 'train_lr': 0.040000600000000004, 'batch_time': 1.6599447138068295, 'data_time': 0.002418718734582583}
2025-11-04 00:25:16 Train Epoch 003: 100%|██████████| 2502/2502 [1:09:13<00:00,  1.66s/it, Loss=5.1539, Top1=N/A, LR=0.040001]
2025-11-04 00:25:19 Val Epoch 003:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 00:25:23   with torch.cuda.amp.autocast():
2025-11-04 00:25:23 Val Epoch 003: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=4.3113, Top1=24.10%, Top5=48.09%]
2025-11-04 00:27:09 2025-11-04 00:27:09,488 - INFO - Step 3: {'epoch': 3, 'learning_rate': 0.0600004, 'train_loss': 5.154154593043095, 'train_top1': 18.961169554455445, 'train_top5': 39.93850556930693, 'train_precision': 17.014647841294664, 'train_recall': 18.875362849401576, 'train_f1': 16.97072867915867, 'val_loss': 4.31126229598999, 'val_top1': 24.10199999572754, 'val_top5': 48.091999995117185, 'val_precision': 27.930930432392266, 'val_recall': 24.106, 'val_f1': 21.88584697848347}
2025-11-04 00:27:09 2025-11-04 00:27:09,490 - INFO - Epoch 003 Summary - LR: 0.060000, Train Loss: 5.1542, Val Loss: 4.3113, Val F1: 21.89%, Val Precision: 27.93%, Val Recall: 24.11%
2025-11-04 00:27:13 2025-11-04 00:27:13,012 - INFO - New best model saved with validation accuracy: 24.102%
2025-11-04 00:27:13 2025-11-04 00:27:13,013 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_004.pth
2025-11-04 00:27:13 Train Epoch 004:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 3 that is less than the current step 10006. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 00:27:16 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 00:27:16   with torch.cuda.amp.autocast():
2025-11-04 00:27:18 Train Epoch 004:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.4341, Top1=22.27%, LR=0.060000]2025-11-04 00:27:18,302 - INFO - Step 10008: {'train_loss_batch': 4.434138298034668, 'train_lr': 0.0600004, 'batch_time': 5.287841081619263, 'data_time': 3.6447348594665527}
2025-11-04 00:27:18 Train Epoch 004:   4%|▍         | 100/2502 [02:51<1:06:44,  1.67s/it, Loss=4.8945, Top1=N/A, LR=0.060000]   2025-11-04 00:30:04,361 - INFO - Step 10108: {'train_loss_batch': 4.623457431793213, 'train_lr': 0.0600004, 'batch_time': 1.6965055630938841, 'data_time': 0.03707407252623303}
2025-11-04 00:30:04 Train Epoch 004:   8%|▊         | 200/2502 [05:37<1:03:54,  1.67s/it, Loss=4.9585, Top1=N/A, LR=0.060000]2025-11-04 00:32:50,517 - INFO - Step 10208: {'train_loss_batch': 4.552230358123779, 'train_lr': 0.0600004, 'batch_time': 1.6791157034499136, 'data_time': 0.019054728360911505}
2025-11-04 00:32:50 Train Epoch 004:  12%|█▏        | 300/2502 [08:24<1:01:09,  1.67s/it, Loss=4.9501, Top1=N/A, LR=0.060000]2025-11-04 00:35:37,114 - INFO - Step 10308: {'train_loss_batch': 4.271210193634033, 'train_lr': 0.0600004, 'batch_time': 1.6747492064669283, 'data_time': 0.013023978451953773}
2025-11-04 00:35:37 Train Epoch 004:  16%|█▌        | 400/2502 [11:10<58:05,  1.66s/it, Loss=4.9493, Top1=22.13%, LR=0.060000]2025-11-04 00:38:23,203 - INFO - Step 10408: {'train_loss_batch': 4.376132965087891, 'train_lr': 0.0600004, 'batch_time': 1.6712912668908326, 'data_time': 0.01000125390335806}
2025-11-04 00:38:23 Train Epoch 004:  20%|█▉        | 500/2502 [13:56<55:45,  1.67s/it, Loss=4.9511, Top1=N/A, LR=0.060000]   2025-11-04 00:41:09,062 - INFO - Step 10508: {'train_loss_batch': 5.3615827560424805, 'train_lr': 0.0600004, 'batch_time': 1.668757503855966, 'data_time': 0.008182701236473586}
2025-11-04 00:41:09 Train Epoch 004:  24%|██▍       | 600/2502 [16:41<52:08,  1.65s/it, Loss=4.9456, Top1=N/A, LR=0.060000]2025-11-04 00:43:55,003 - INFO - Step 10608: {'train_loss_batch': 4.38475227355957, 'train_lr': 0.0600004, 'batch_time': 1.667201148888434, 'data_time': 0.006968568843137008}
2025-11-04 00:43:55 Train Epoch 004:  28%|██▊       | 700/2502 [19:27<49:34,  1.65s/it, Loss=4.9449, Top1=22.55%, LR=0.060000]2025-11-04 00:46:40,588 - INFO - Step 10708: {'train_loss_batch': 4.473067760467529, 'train_lr': 0.0600004, 'batch_time': 1.6655817120289496, 'data_time': 0.006098159199604465}
2025-11-04 00:46:40 Train Epoch 004:  32%|███▏      | 800/2502 [22:13<47:05,  1.66s/it, Loss=4.9445, Top1=22.63%, LR=0.060000]2025-11-04 00:49:26,150 - INFO - Step 10808: {'train_loss_batch': 4.409531593322754, 'train_lr': 0.0600004, 'batch_time': 1.6643373046474956, 'data_time': 0.005445978019419086}
2025-11-04 00:49:26 Train Epoch 004:  36%|███▌      | 900/2502 [24:58<44:03,  1.65s/it, Loss=4.9333, Top1=N/A, LR=0.060000]   2025-11-04 00:52:11,799 - INFO - Step 10908: {'train_loss_batch': 5.759210586547852, 'train_lr': 0.0600004, 'batch_time': 1.6634661566536382, 'data_time': 0.004940714343935748}
2025-11-04 00:52:11 Train Epoch 004:  40%|███▉      | 1000/2502 [27:44<41:34,  1.66s/it, Loss=4.9243, Top1=N/A, LR=0.060000]2025-11-04 00:54:57,824 - INFO - Step 11008: {'train_loss_batch': 4.963464736938477, 'train_lr': 0.0600004, 'batch_time': 1.6631449800390345, 'data_time': 0.004528060183301196}
2025-11-04 00:54:57 Train Epoch 004:  44%|████▍     | 1100/2502 [30:30<38:43,  1.66s/it, Loss=4.9167, Top1=N/A, LR=0.060000]2025-11-04 00:57:43,933 - INFO - Step 11108: {'train_loss_batch': 4.226424217224121, 'train_lr': 0.0600004, 'batch_time': 1.662958276802361, 'data_time': 0.004197018239630233}
2025-11-04 00:57:43 Train Epoch 004:  48%|████▊     | 1200/2502 [33:16<35:50,  1.65s/it, Loss=4.9164, Top1=N/A, LR=0.060000]2025-11-04 01:00:29,896 - INFO - Step 11208: {'train_loss_batch': 4.219431400299072, 'train_lr': 0.0600004, 'batch_time': 1.6626809284550066, 'data_time': 0.003921332307699618}
2025-11-04 01:00:29 Train Epoch 004:  52%|█████▏    | 1300/2502 [36:02<33:17,  1.66s/it, Loss=4.9108, Top1=N/A, LR=0.060000]2025-11-04 01:03:15,868 - INFO - Step 11308: {'train_loss_batch': 4.624035358428955, 'train_lr': 0.0600004, 'batch_time': 1.6624531452697942, 'data_time': 0.003691059547969692}
2025-11-04 01:03:15 Train Epoch 004:  56%|█████▌    | 1400/2502 [38:47<30:26,  1.66s/it, Loss=4.9046, Top1=N/A, LR=0.060000]2025-11-04 01:06:00,451 - INFO - Step 11408: {'train_loss_batch': 4.313572883605957, 'train_lr': 0.0600004, 'batch_time': 1.6612660969945892, 'data_time': 0.003494624832883041}
2025-11-04 01:06:00 Train Epoch 004:  60%|█████▉    | 1500/2502 [41:32<27:19,  1.64s/it, Loss=4.8995, Top1=N/A, LR=0.060000]2025-11-04 01:08:45,532 - INFO - Step 11508: {'train_loss_batch': 5.74221134185791, 'train_lr': 0.0600004, 'batch_time': 1.6605693045494796, 'data_time': 0.0033193927538704665}
2025-11-04 01:08:45 Train Epoch 004:  64%|██████▍   | 1600/2502 [44:17<24:44,  1.65s/it, Loss=4.8905, Top1=N/A, LR=0.060000]2025-11-04 01:11:30,401 - INFO - Step 11608: {'train_loss_batch': 4.122565269470215, 'train_lr': 0.0600004, 'batch_time': 1.6598275838085892, 'data_time': 0.0031688645212148443}
2025-11-04 01:11:30 Train Epoch 004:  68%|██████▊   | 1700/2502 [47:02<22:03,  1.65s/it, Loss=4.8911, Top1=24.00%, LR=0.060000]2025-11-04 01:14:15,853 - INFO - Step 11708: {'train_loss_batch': 4.196422576904297, 'train_lr': 0.0600004, 'batch_time': 1.659515547514102, 'data_time': 0.0030344191331992355}
2025-11-04 01:14:15 Train Epoch 004:  72%|███████▏  | 1800/2502 [49:49<19:25,  1.66s/it, Loss=4.8856, Top1=N/A, LR=0.060000]   2025-11-04 01:17:02,251 - INFO - Step 11808: {'train_loss_batch': 4.68761682510376, 'train_lr': 0.0600004, 'batch_time': 1.6597630466639102, 'data_time': 0.0029160613685366445}
2025-11-04 01:17:02 Train Epoch 004:  76%|███████▌  | 1900/2502 [52:35<16:39,  1.66s/it, Loss=4.8733, Top1=N/A, LR=0.060000]2025-11-04 01:19:48,795 - INFO - Step 11908: {'train_loss_batch': 5.0808186531066895, 'train_lr': 0.0600004, 'batch_time': 1.6600617078152033, 'data_time': 0.0028144125559654816}
2025-11-04 01:19:48 Train Epoch 004:  80%|███████▉  | 2000/2502 [55:21<13:51,  1.66s/it, Loss=4.8737, Top1=24.55%, LR=0.060000]2025-11-04 01:22:34,555 - INFO - Step 12008: {'train_loss_batch': 4.1440324783325195, 'train_lr': 0.0600004, 'batch_time': 1.6599382700055079, 'data_time': 0.0027227999864966198}
2025-11-04 01:22:34 Train Epoch 004:  84%|████████▍ | 2100/2502 [58:07<11:10,  1.67s/it, Loss=4.8655, Top1=N/A, LR=0.060000]   2025-11-04 01:25:20,992 - INFO - Step 12108: {'train_loss_batch': 5.586354732513428, 'train_lr': 0.0600004, 'batch_time': 1.6601490402494028, 'data_time': 0.0026384285096609267}
2025-11-04 01:25:20 Train Epoch 004:  88%|████████▊ | 2200/2502 [1:00:54<08:18,  1.65s/it, Loss=4.8551, Top1=N/A, LR=0.060000]2025-11-04 01:28:07,338 - INFO - Step 12208: {'train_loss_batch': 4.419794082641602, 'train_lr': 0.0600004, 'batch_time': 1.6602998438449081, 'data_time': 0.002561357226061962}
2025-11-04 01:28:07 Train Epoch 004:  92%|█████████▏| 2300/2502 [1:03:39<05:33,  1.65s/it, Loss=4.8486, Top1=N/A, LR=0.060000]2025-11-04 01:30:53,012 - INFO - Step 12308: {'train_loss_batch': 4.2327704429626465, 'train_lr': 0.0600004, 'batch_time': 1.660144831501158, 'data_time': 0.002488977854172078}
2025-11-04 01:30:53 Train Epoch 004:  96%|█████████▌| 2400/2502 [1:06:26<02:49,  1.67s/it, Loss=4.8431, Top1=N/A, LR=0.060000]2025-11-04 01:33:39,143 - INFO - Step 12408: {'train_loss_batch': 5.146295547485352, 'train_lr': 0.0600004, 'batch_time': 1.6601930633776887, 'data_time': 0.002425409564868652}
2025-11-04 01:33:39 Train Epoch 004: 100%|█████████▉| 2500/2502 [1:09:12<00:03,  1.66s/it, Loss=4.8403, Top1=N/A, LR=0.060000]2025-11-04 01:36:25,588 - INFO - Step 12508: {'train_loss_batch': 5.497089385986328, 'train_lr': 0.0600004, 'batch_time': 1.6603633131517597, 'data_time': 0.002410681139989073}
2025-11-04 01:36:25 Train Epoch 004: 100%|██████████| 2502/2502 [1:09:14<00:00,  1.66s/it, Loss=4.8403, Top1=N/A, LR=0.060000]
2025-11-04 01:36:27 Val Epoch 004:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 01:36:32   with torch.cuda.amp.autocast():
2025-11-04 01:36:32 Val Epoch 004: 100%|██████████| 98/98 [01:47<00:00,  1.10s/it, Loss=3.9251, Top1=30.79%, Top5=56.73%]
2025-11-04 01:38:15 2025-11-04 01:38:15,243 - INFO - Step 4: {'epoch': 4, 'learning_rate': 0.08000020000000001, 'train_loss': 4.840176125510419, 'train_top1': 25.354369386227546, 'train_top5': 48.86477045908184, 'train_precision': 23.60024419064218, 'train_recall': 25.20394148862818, 'train_f1': 23.674198486323462, 'val_loss': 3.9250694215393067, 'val_top1': 30.794000008544923, 'val_top5': 56.72599998901367, 'val_precision': 36.16640643926764, 'val_recall': 30.8, 'val_f1': 28.66427228445381}
2025-11-04 01:38:15 2025-11-04 01:38:15,244 - INFO - Epoch 004 Summary - LR: 0.080000, Train Loss: 4.8402, Val Loss: 3.9251, Val F1: 28.66%, Val Precision: 36.17%, Val Recall: 30.80%
2025-11-04 01:38:15 wandb: WARNING Tried to log to step 4 that is less than the current step 12508. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 01:38:18 2025-11-04 01:38:18,257 - INFO - New best model saved with validation accuracy: 30.794%
2025-11-04 01:38:18 2025-11-04 01:38:18,258 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_005.pth
2025-11-04 01:38:18 Train Epoch 005:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 01:38:21   with torch.cuda.amp.autocast():
2025-11-04 01:38:23 Train Epoch 005:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=5.6927, Top1=N/A, LR=0.080000]2025-11-04 01:38:23,519 - INFO - Step 12510: {'train_loss_batch': 5.692739486694336, 'train_lr': 0.08000020000000001, 'batch_time': 5.2593162059783936, 'data_time': 3.5972747802734375}
2025-11-04 01:38:23 Train Epoch 005:   4%|▍         | 100/2502 [02:51<1:06:43,  1.67s/it, Loss=4.6876, Top1=N/A, LR=0.080000]2025-11-04 01:41:09,485 - INFO - Step 12610: {'train_loss_batch': 5.306881904602051, 'train_lr': 0.08000020000000001, 'batch_time': 1.6953067165790219, 'data_time': 0.036511768208871975}
2025-11-04 01:41:09 Train Epoch 005:   8%|▊         | 200/2502 [05:36<1:02:46,  1.64s/it, Loss=4.6357, Top1=N/A, LR=0.080000]2025-11-04 01:43:55,218 - INFO - Step 12710: {'train_loss_batch': 4.074099063873291, 'train_lr': 0.08000020000000001, 'batch_time': 1.676412329744937, 'data_time': 0.018774668375651043}
2025-11-04 01:43:55 Train Epoch 005:  12%|█▏        | 300/2502 [08:21<1:00:49,  1.66s/it, Loss=4.6605, Top1=28.45%, LR=0.080000]2025-11-04 01:46:39,528 - INFO - Step 12810: {'train_loss_batch': 4.255916595458984, 'train_lr': 0.08000020000000001, 'batch_time': 1.6653432299528408, 'data_time': 0.012819846207121281}
2025-11-04 01:46:39 Train Epoch 005:  16%|█▌        | 400/2502 [11:06<58:03,  1.66s/it, Loss=4.6625, Top1=N/A, LR=0.080000]   2025-11-04 01:49:25,103 - INFO - Step 12910: {'train_loss_batch': 5.678122520446777, 'train_lr': 0.08000020000000001, 'batch_time': 1.6629510223122308, 'data_time': 0.00984702443244154}
2025-11-04 01:49:25 Train Epoch 005:  20%|█▉        | 500/2502 [13:51<54:53,  1.65s/it, Loss=4.6525, Top1=N/A, LR=0.080000]2025-11-04 01:52:10,022 - INFO - Step 13010: {'train_loss_batch': 4.182797908782959, 'train_lr': 0.08000020000000001, 'batch_time': 1.6602034759140776, 'data_time': 0.008060510049085179}
2025-11-04 01:52:10 Train Epoch 005:  24%|██▍       | 600/2502 [16:37<52:38,  1.66s/it, Loss=4.6708, Top1=N/A, LR=0.080000]2025-11-04 01:54:56,201 - INFO - Step 13110: {'train_loss_batch': 4.117415904998779, 'train_lr': 0.08000020000000001, 'batch_time': 1.6604668812426473, 'data_time': 0.006864289079053628}
2025-11-04 01:54:56 Train Epoch 005:  28%|██▊       | 700/2502 [19:23<49:58,  1.66s/it, Loss=4.6672, Top1=N/A, LR=0.080000]2025-11-04 01:57:41,430 - INFO - Step 13210: {'train_loss_batch': 5.583319187164307, 'train_lr': 0.08000020000000001, 'batch_time': 1.6592999926307233, 'data_time': 0.0060137392961009594}
2025-11-04 01:57:41 Train Epoch 005:  32%|███▏      | 800/2502 [22:08<46:37,  1.64s/it, Loss=4.6672, Top1=N/A, LR=0.080000]2025-11-04 02:00:27,151 - INFO - Step 13310: {'train_loss_batch': 5.486865997314453, 'train_lr': 0.08000020000000001, 'batch_time': 1.6590393399180248, 'data_time': 0.005369739437222332}
2025-11-04 02:00:27 Train Epoch 005:  36%|███▌      | 900/2502 [24:53<44:01,  1.65s/it, Loss=4.6580, Top1=29.17%, LR=0.080000]2025-11-04 02:03:12,036 - INFO - Step 13410: {'train_loss_batch': 3.976031541824341, 'train_lr': 0.08000020000000001, 'batch_time': 1.6579086896978923, 'data_time': 0.00486488162346606}
2025-11-04 02:03:12 Train Epoch 005:  40%|███▉      | 1000/2502 [27:39<41:46,  1.67s/it, Loss=4.6311, Top1=29.33%, LR=0.080000]2025-11-04 02:05:58,074 - INFO - Step 13510: {'train_loss_batch': 3.891961097717285, 'train_lr': 0.08000020000000001, 'batch_time': 1.658155359588303, 'data_time': 0.0044716354850288876}
2025-11-04 02:05:58 Train Epoch 005:  44%|████▍     | 1100/2502 [30:25<38:53,  1.66s/it, Loss=4.6197, Top1=29.42%, LR=0.080000]2025-11-04 02:08:43,681 - INFO - Step 13610: {'train_loss_batch': 3.979694128036499, 'train_lr': 0.08000020000000001, 'batch_time': 1.657965318383573, 'data_time': 0.004149002989891115}
2025-11-04 02:08:43 Train Epoch 005:  48%|████▊     | 1200/2502 [33:11<35:46,  1.65s/it, Loss=4.6235, Top1=N/A, LR=0.080000]   2025-11-04 02:11:29,615 - INFO - Step 13710: {'train_loss_batch': 4.090243339538574, 'train_lr': 0.08000020000000001, 'batch_time': 1.6580798780789086, 'data_time': 0.0038848316341911528}
2025-11-04 02:11:29 Train Epoch 005:  52%|█████▏    | 1300/2502 [35:56<32:54,  1.64s/it, Loss=4.6378, Top1=N/A, LR=0.080000]2025-11-04 02:14:14,920 - INFO - Step 13810: {'train_loss_batch': 4.075647354125977, 'train_lr': 0.08000020000000001, 'batch_time': 1.657692928849322, 'data_time': 0.003659900750314887}
2025-11-04 02:14:14 Train Epoch 005:  56%|█████▌    | 1400/2502 [38:42<30:19,  1.65s/it, Loss=4.6301, Top1=N/A, LR=0.080000]2025-11-04 02:17:00,624 - INFO - Step 13910: {'train_loss_batch': 4.080560207366943, 'train_lr': 0.08000020000000001, 'batch_time': 1.657645844290037, 'data_time': 0.003470949238321766}
2025-11-04 02:17:00 Train Epoch 005:  60%|█████▉    | 1500/2502 [41:28<27:32,  1.65s/it, Loss=4.6206, Top1=N/A, LR=0.080000]2025-11-04 02:19:46,766 - INFO - Step 14010: {'train_loss_batch': 3.9798531532287598, 'train_lr': 0.08000020000000001, 'batch_time': 1.6578972911453502, 'data_time': 0.0033039695655878664}
2025-11-04 02:19:46 Train Epoch 005:  64%|██████▍   | 1600/2502 [44:14<24:59,  1.66s/it, Loss=4.6194, Top1=N/A, LR=0.080000]2025-11-04 02:22:32,312 - INFO - Step 14110: {'train_loss_batch': 3.9539475440979004, 'train_lr': 0.08000020000000001, 'batch_time': 1.6577448679610092, 'data_time': 0.003157038900123992}
2025-11-04 02:22:32 Train Epoch 005:  68%|██████▊   | 1700/2502 [46:59<22:10,  1.66s/it, Loss=4.6142, Top1=30.16%, LR=0.080000]2025-11-04 02:25:17,948 - INFO - Step 14210: {'train_loss_batch': 3.978714942932129, 'train_lr': 0.08000020000000001, 'batch_time': 1.6576635564235573, 'data_time': 0.0030269290614590655}
2025-11-04 02:25:17 Train Epoch 005:  72%|███████▏  | 1800/2502 [49:45<19:30,  1.67s/it, Loss=4.6109, Top1=30.28%, LR=0.080000]2025-11-04 02:28:03,566 - INFO - Step 14310: {'train_loss_batch': 3.8925957679748535, 'train_lr': 0.08000020000000001, 'batch_time': 1.657580832783743, 'data_time': 0.0029141542846662745}
2025-11-04 02:28:03 Train Epoch 005:  76%|███████▌  | 1900/2502 [52:30<16:39,  1.66s/it, Loss=4.5983, Top1=30.53%, LR=0.080000]2025-11-04 02:30:48,373 - INFO - Step 14410: {'train_loss_batch': 3.87984037399292, 'train_lr': 0.08000020000000001, 'batch_time': 1.657080655095453, 'data_time': 0.002812197558319236}
2025-11-04 02:30:48 Train Epoch 005:  80%|███████▉  | 2000/2502 [55:16<13:54,  1.66s/it, Loss=4.5937, Top1=30.67%, LR=0.080000]2025-11-04 02:33:34,443 - INFO - Step 14510: {'train_loss_batch': 3.814729690551758, 'train_lr': 0.08000020000000001, 'batch_time': 1.6572613952041924, 'data_time': 0.002719847694865946}
2025-11-04 02:33:34 Train Epoch 005:  84%|████████▍ | 2100/2502 [58:02<11:08,  1.66s/it, Loss=4.5902, Top1=N/A, LR=0.080000]   2025-11-04 02:36:20,510 - INFO - Step 14610: {'train_loss_batch': 5.049374103546143, 'train_lr': 0.08000020000000001, 'batch_time': 1.6574236871173755, 'data_time': 0.0026379613181853625}
2025-11-04 02:36:20 Train Epoch 005:  88%|████████▊ | 2200/2502 [1:00:48<08:21,  1.66s/it, Loss=4.5896, Top1=N/A, LR=0.080000]2025-11-04 02:39:06,898 - INFO - Step 14710: {'train_loss_batch': 5.34771203994751, 'train_lr': 0.08000020000000001, 'batch_time': 1.6577166698565, 'data_time': 0.00255901355301451}
2025-11-04 02:39:06 Train Epoch 005:  92%|█████████▏| 2300/2502 [1:03:33<05:30,  1.64s/it, Loss=4.5851, Top1=N/A, LR=0.080000]2025-11-04 02:41:52,204 - INFO - Step 14810: {'train_loss_batch': 3.693145751953125, 'train_lr': 0.08000020000000001, 'batch_time': 1.6575142787053034, 'data_time': 0.002487893003010325}
2025-11-04 02:41:52 Train Epoch 005:  96%|█████████▌| 2400/2502 [1:06:19<02:49,  1.66s/it, Loss=4.5794, Top1=N/A, LR=0.080000]2025-11-04 02:44:37,504 - INFO - Step 14910: {'train_loss_batch': 3.7806649208068848, 'train_lr': 0.08000020000000001, 'batch_time': 1.6573261173006795, 'data_time': 0.002424532748519853}
2025-11-04 02:44:37 Train Epoch 005: 100%|█████████▉| 2500/2502 [1:09:04<00:03,  1.67s/it, Loss=4.5771, Top1=N/A, LR=0.080000]2025-11-04 02:47:23,211 - INFO - Step 15010: {'train_loss_batch': 5.227138996124268, 'train_lr': 0.08000020000000001, 'batch_time': 1.6573159566930369, 'data_time': 0.0024113446319165206}
2025-11-04 02:47:23 Train Epoch 005: 100%|██████████| 2502/2502 [1:09:06<00:00,  1.66s/it, Loss=4.5771, Top1=N/A, LR=0.080000]
2025-11-04 02:47:25 Val Epoch 005:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 02:47:29   with torch.cuda.amp.autocast():
2025-11-04 02:47:30 Val Epoch 005: 100%|██████████| 98/98 [01:48<00:00,  1.11s/it, Loss=3.5973, Top1=37.03%, Top5=63.50%]
2025-11-04 02:49:14 2025-11-04 02:49:14,114 - INFO - Step 5: {'epoch': 5, 'learning_rate': 0.1, 'train_loss': 4.576807045822235, 'train_top1': 31.157132862473347, 'train_top5': 55.68738339552239, 'train_precision': 29.7800311252997, 'train_recall': 31.057301039558975, 'train_f1': 29.825065319813817, 'val_loss': 3.5973008590698243, 'val_top1': 37.02600000244141, 'val_top5': 63.49999998046875, 'val_precision': 42.94553488035797, 'val_recall': 37.029999999999994, 'val_f1': 35.648713583061756}
2025-11-04 02:49:14 2025-11-04 02:49:14,117 - INFO - Epoch 005 Summary - LR: 0.100000, Train Loss: 4.5768, Val Loss: 3.5973, Val F1: 35.65%, Val Precision: 42.95%, Val Recall: 37.03%
2025-11-04 02:49:16 wandb: WARNING Tried to log to step 5 that is less than the current step 15010. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 02:49:17 2025-11-04 02:49:17,623 - INFO - New best model saved with validation accuracy: 37.026%
2025-11-04 02:49:17 2025-11-04 02:49:17,623 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_006.pth
2025-11-04 02:49:17 Train Epoch 006:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 02:49:21   with torch.cuda.amp.autocast():
2025-11-04 02:49:23 Train Epoch 006:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.9430, Top1=N/A, LR=0.100000]2025-11-04 02:49:23,284 - INFO - Step 15012: {'train_loss_batch': 3.9430289268493652, 'train_lr': 0.1, 'batch_time': 5.659321546554565, 'data_time': 3.9977493286132812}
2025-11-04 02:49:23 Train Epoch 006:   4%|▍         | 100/2502 [02:51<1:06:28,  1.66s/it, Loss=4.4783, Top1=33.47%, LR=0.100000]2025-11-04 02:52:09,343 - INFO - Step 15112: {'train_loss_batch': 3.905270576477051, 'train_lr': 0.1, 'batch_time': 1.7001851407608184, 'data_time': 0.04055707289440797}
2025-11-04 02:52:09 Train Epoch 006:   8%|▊         | 200/2502 [05:38<1:03:46,  1.66s/it, Loss=4.5045, Top1=N/A, LR=0.100000]   2025-11-04 02:54:55,682 - INFO - Step 15212: {'train_loss_batch': 3.8481943607330322, 'train_lr': 0.1, 'batch_time': 1.6818776356047065, 'data_time': 0.020820776621500652}
2025-11-04 02:54:55 Train Epoch 006:  12%|█▏        | 300/2502 [08:23<1:00:27,  1.65s/it, Loss=4.4889, Top1=N/A, LR=0.100000]2025-11-04 02:57:41,092 - INFO - Step 15312: {'train_loss_batch': 4.041532516479492, 'train_lr': 0.1, 'batch_time': 1.6726476076829473, 'data_time': 0.01419065006547592}
2025-11-04 02:57:41 Train Epoch 006:  16%|█▌        | 400/2502 [11:09<57:50,  1.65s/it, Loss=4.5165, Top1=N/A, LR=0.100000]2025-11-04 03:00:26,812 - INFO - Step 15412: {'train_loss_batch': 5.420413970947266, 'train_lr': 0.1, 'batch_time': 1.6687961005213254, 'data_time': 0.010893126081052862}
2025-11-04 03:00:26 Train Epoch 006:  20%|█▉        | 500/2502 [13:54<55:18,  1.66s/it, Loss=4.4927, Top1=N/A, LR=0.100000]2025-11-04 03:03:12,300 - INFO - Step 15512: {'train_loss_batch': 5.515789985656738, 'train_lr': 0.1, 'batch_time': 1.6660165686807233, 'data_time': 0.008909172640589182}
2025-11-04 03:03:12 Train Epoch 006:  24%|██▍       | 600/2502 [16:40<52:49,  1.67s/it, Loss=4.4806, Top1=N/A, LR=0.100000]2025-11-04 03:05:58,591 - INFO - Step 15612: {'train_loss_batch': 5.254919052124023, 'train_lr': 0.1, 'batch_time': 1.6654998629343094, 'data_time': 0.0075972453131651916}
2025-11-04 03:05:58 Train Epoch 006:  28%|██▊       | 700/2502 [19:26<49:26,  1.65s/it, Loss=4.4760, Top1=N/A, LR=0.100000]2025-11-04 03:08:44,270 - INFO - Step 15712: {'train_loss_batch': 3.7019429206848145, 'train_lr': 0.1, 'batch_time': 1.664257579455192, 'data_time': 0.0066488479581607056}
2025-11-04 03:08:44 Train Epoch 006:  32%|███▏      | 800/2502 [22:10<47:07,  1.66s/it, Loss=4.4686, Top1=N/A, LR=0.100000]2025-11-04 03:11:28,475 - INFO - Step 15812: {'train_loss_batch': 5.199836254119873, 'train_lr': 0.1, 'batch_time': 1.6614840179495747, 'data_time': 0.005940079837851459}
2025-11-04 03:11:28 Train Epoch 006:  36%|███▌      | 900/2502 [24:56<44:08,  1.65s/it, Loss=4.4654, Top1=N/A, LR=0.100000]2025-11-04 03:14:13,939 - INFO - Step 15912: {'train_loss_batch': 4.574275970458984, 'train_lr': 0.1, 'batch_time': 1.6607250582497075, 'data_time': 0.005392578148286165}
2025-11-04 03:14:13 Train Epoch 006:  40%|███▉      | 1000/2502 [27:41<41:16,  1.65s/it, Loss=4.4538, Top1=N/A, LR=0.100000]2025-11-04 03:16:59,141 - INFO - Step 16012: {'train_loss_batch': 5.400783061981201, 'train_lr': 0.1, 'batch_time': 1.6598545692779205, 'data_time': 0.004952419292438518}
2025-11-04 03:16:59 Train Epoch 006:  44%|████▍     | 1100/2502 [30:27<38:46,  1.66s/it, Loss=4.4394, Top1=34.24%, LR=0.100000]2025-11-04 03:19:45,028 - INFO - Step 16112: {'train_loss_batch': 3.897397041320801, 'train_lr': 0.1, 'batch_time': 1.6597650990932233, 'data_time': 0.004596453379978384}
2025-11-04 03:19:45 Train Epoch 006:  48%|████▊     | 1200/2502 [33:12<35:59,  1.66s/it, Loss=4.4335, Top1=34.37%, LR=0.100000]2025-11-04 03:22:30,619 - INFO - Step 16212: {'train_loss_batch': 3.68350887298584, 'train_lr': 0.1, 'batch_time': 1.6594438943934382, 'data_time': 0.004293913249667737}
2025-11-04 03:22:30 Train Epoch 006:  52%|█████▏    | 1300/2502 [35:59<33:22,  1.67s/it, Loss=4.4247, Top1=34.44%, LR=0.100000]2025-11-04 03:25:16,836 - INFO - Step 16312: {'train_loss_batch': 3.889723777770996, 'train_lr': 0.1, 'batch_time': 1.65965314373248, 'data_time': 0.004043874513360741}
2025-11-04 03:25:16 Train Epoch 006:  56%|█████▌    | 1400/2502 [38:44<30:00,  1.63s/it, Loss=4.4193, Top1=34.53%, LR=0.100000]2025-11-04 03:28:01,918 - INFO - Step 16412: {'train_loss_batch': 3.8282647132873535, 'train_lr': 0.1, 'batch_time': 1.6590228751248586, 'data_time': 0.0038262501007994953}
2025-11-04 03:28:01 Train Epoch 006:  60%|█████▉    | 1500/2502 [41:29<27:36,  1.65s/it, Loss=4.4164, Top1=N/A, LR=0.100000]   2025-11-04 03:30:46,913 - INFO - Step 16512: {'train_loss_batch': 3.794935703277588, 'train_lr': 0.1, 'batch_time': 1.6584183794907297, 'data_time': 0.0036361063424147897}
2025-11-04 03:30:46 Train Epoch 006:  64%|██████▍   | 1600/2502 [44:15<25:06,  1.67s/it, Loss=4.4075, Top1=N/A, LR=0.100000]2025-11-04 03:33:33,119 - INFO - Step 16612: {'train_loss_batch': 3.83294939994812, 'train_lr': 0.1, 'batch_time': 1.6586454910907351, 'data_time': 0.003475206334616228}
2025-11-04 03:33:33 Train Epoch 006:  68%|██████▊   | 1700/2502 [47:00<22:06,  1.65s/it, Loss=4.4082, Top1=N/A, LR=0.100000]2025-11-04 03:36:17,878 - INFO - Step 16712: {'train_loss_batch': 3.778388261795044, 'train_lr': 0.1, 'batch_time': 1.6579950813123019, 'data_time': 0.003327644410937902}
2025-11-04 03:36:17 Train Epoch 006:  72%|███████▏  | 1800/2502 [49:46<19:30,  1.67s/it, Loss=4.4025, Top1=N/A, LR=0.100000]2025-11-04 03:39:04,129 - INFO - Step 16812: {'train_loss_batch': 5.214744567871094, 'train_lr': 0.1, 'batch_time': 1.6582456666320513, 'data_time': 0.003197302757402978}
2025-11-04 03:39:04 Train Epoch 006:  76%|███████▌  | 1900/2502 [52:32<16:42,  1.66s/it, Loss=4.4044, Top1=N/A, LR=0.100000]2025-11-04 03:41:50,457 - INFO - Step 16912: {'train_loss_batch': 3.7034220695495605, 'train_lr': 0.1, 'batch_time': 1.658510690736244, 'data_time': 0.0030829960393128054}
2025-11-04 03:41:50 Train Epoch 006:  80%|███████▉  | 2000/2502 [55:18<13:39,  1.63s/it, Loss=4.4038, Top1=N/A, LR=0.100000]2025-11-04 03:44:35,762 - INFO - Step 17012: {'train_loss_batch': 4.200296401977539, 'train_lr': 0.1, 'batch_time': 1.6582377088004383, 'data_time': 0.0029773771733060473}
2025-11-04 03:44:35 Train Epoch 006:  84%|████████▍ | 2100/2502 [58:03<11:10,  1.67s/it, Loss=4.3949, Top1=35.28%, LR=0.100000]2025-11-04 03:47:21,175 - INFO - Step 17112: {'train_loss_batch': 3.7546420097351074, 'train_lr': 0.1, 'batch_time': 1.6580419582392363, 'data_time': 0.0028831689826424264}
2025-11-04 03:47:21 Train Epoch 006:  88%|████████▊ | 2200/2502 [1:00:49<08:20,  1.66s/it, Loss=4.3942, Top1=N/A, LR=0.100000]   2025-11-04 03:50:06,897 - INFO - Step 17212: {'train_loss_batch': 5.5301408767700195, 'train_lr': 0.1, 'batch_time': 1.6580041764487683, 'data_time': 0.002797817761439835}
2025-11-04 03:50:06 Train Epoch 006:  92%|█████████▏| 2300/2502 [1:03:35<05:35,  1.66s/it, Loss=4.3868, Top1=35.47%, LR=0.100000]2025-11-04 03:52:53,063 - INFO - Step 17312: {'train_loss_batch': 3.7136390209198, 'train_lr': 0.1, 'batch_time': 1.6581631242477701, 'data_time': 0.002721335980126465}
2025-11-04 03:52:53 Train Epoch 006:  96%|█████████▌| 2400/2502 [1:06:21<02:49,  1.66s/it, Loss=4.3796, Top1=N/A, LR=0.100000]   2025-11-04 03:55:39,204 - INFO - Step 17412: {'train_loss_batch': 5.7069501876831055, 'train_lr': 0.1, 'batch_time': 1.6582982471613823, 'data_time': 0.0026523708453132728}
2025-11-04 03:55:39 Train Epoch 006: 100%|█████████▉| 2500/2502 [1:09:06<00:03,  1.66s/it, Loss=4.3710, Top1=N/A, LR=0.100000]2025-11-04 03:58:24,011 - INFO - Step 17512: {'train_loss_batch': 3.3941895961761475, 'train_lr': 0.1, 'batch_time': 1.6578892619549774, 'data_time': 0.0026038865574070664}
2025-11-04 03:58:24 Train Epoch 006: 100%|██████████| 2502/2502 [1:09:08<00:00,  1.66s/it, Loss=4.3710, Top1=N/A, LR=0.100000]
2025-11-04 03:58:26 Val Epoch 006:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 03:58:30   with torch.cuda.amp.autocast():
2025-11-04 03:58:31 Val Epoch 006: 100%|██████████| 98/98 [01:47<00:00,  1.10s/it, Loss=3.4453, Top1=40.63%, Top5=67.19%]
2025-11-04 04:00:14 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 04:00:14   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 04:00:14 2025-11-04 04:00:14,147 - INFO - Step 6: {'epoch': 6, 'learning_rate': 0.09996585642185044, 'train_loss': 4.3707333818423475, 'train_top1': 35.74790047989031, 'train_top5': 60.83002456581353, 'train_precision': 34.58047470341995, 'train_recall': 35.54884678715089, 'train_f1': 34.571687976522625, 'val_loss': 3.445295833282471, 'val_top1': 40.62800000610351, 'val_top5': 67.18600000488281, 'val_precision': 46.77416209448977, 'val_recall': 40.624, 'val_f1': 39.586175610588285}
2025-11-04 04:00:14 2025-11-04 04:00:14,149 - INFO - Epoch 006 Summary - LR: 0.099966, Train Loss: 4.3707, Val Loss: 3.4453, Val F1: 39.59%, Val Precision: 46.77%, Val Recall: 40.62%
2025-11-04 04:00:16 wandb: WARNING Tried to log to step 6 that is less than the current step 17512. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 04:00:17 2025-11-04 04:00:17,887 - INFO - New best model saved with validation accuracy: 40.628%
2025-11-04 04:00:17 2025-11-04 04:00:17,887 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_007.pth
2025-11-04 04:00:17 Train Epoch 007:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 04:00:21   with torch.cuda.amp.autocast():
2025-11-04 04:00:23 Train Epoch 007:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.4721, Top1=43.16%, LR=0.099966]2025-11-04 04:00:23,559 - INFO - Step 17514: {'train_loss_batch': 3.472085475921631, 'train_lr': 0.09996585642185044, 'batch_time': 5.6696577072143555, 'data_time': 4.028023719787598}
2025-11-04 04:00:23 Train Epoch 007:   4%|▍         | 100/2502 [02:51<1:06:35,  1.66s/it, Loss=4.0608, Top1=N/A, LR=0.099966]   2025-11-04 04:03:09,728 - INFO - Step 17614: {'train_loss_batch': 4.140727996826172, 'train_lr': 0.09996585642185044, 'batch_time': 1.7013763083089697, 'data_time': 0.04077380482513126}
2025-11-04 04:03:09 Train Epoch 007:   8%|▊         | 200/2502 [05:37<1:03:16,  1.65s/it, Loss=4.1476, Top1=39.45%, LR=0.099966]2025-11-04 04:05:55,640 - INFO - Step 17714: {'train_loss_batch': 3.520864725112915, 'train_lr': 0.09996585642185044, 'batch_time': 1.6803519808830907, 'data_time': 0.020944589406103636}
2025-11-04 04:05:55 Train Epoch 007:  12%|█▏        | 300/2502 [08:23<1:00:57,  1.66s/it, Loss=4.1673, Top1=N/A, LR=0.099966]   2025-11-04 04:08:41,147 - INFO - Step 17814: {'train_loss_batch': 3.5752317905426025, 'train_lr': 0.09996585642185044, 'batch_time': 1.67195303574749, 'data_time': 0.014304404242886261}
2025-11-04 04:08:41 Train Epoch 007:  16%|█▌        | 400/2502 [11:08<57:40,  1.65s/it, Loss=4.1871, Top1=39.34%, LR=0.099966]2025-11-04 04:11:26,864 - INFO - Step 17914: {'train_loss_batch': 3.625129222869873, 'train_lr': 0.09996585642185044, 'batch_time': 1.6682654931360943, 'data_time': 0.010966464468368568}
2025-11-04 04:11:26 Train Epoch 007:  20%|█▉        | 500/2502 [13:54<54:47,  1.64s/it, Loss=4.1921, Top1=N/A, LR=0.099966]   2025-11-04 04:14:12,139 - INFO - Step 18014: {'train_loss_batch': 4.079542636871338, 'train_lr': 0.09996585642185044, 'batch_time': 1.665167408313104, 'data_time': 0.008964391525634035}
2025-11-04 04:14:12 Train Epoch 007:  24%|██▍       | 600/2502 [16:39<52:28,  1.66s/it, Loss=4.1840, Top1=N/A, LR=0.099966]2025-11-04 04:16:57,111 - INFO - Step 18114: {'train_loss_batch': 5.104312896728516, 'train_lr': 0.09996585642185044, 'batch_time': 1.6625971639414199, 'data_time': 0.0076297725893296735}
2025-11-04 04:16:57 Train Epoch 007:  28%|██▊       | 700/2502 [19:25<49:25,  1.65s/it, Loss=4.1935, Top1=N/A, LR=0.099966]2025-11-04 04:19:42,931 - INFO - Step 18214: {'train_loss_batch': 5.067432880401611, 'train_lr': 0.09996585642185044, 'batch_time': 1.6619698889075265, 'data_time': 0.006672026597484202}
2025-11-04 04:19:42 Train Epoch 007:  32%|███▏      | 800/2502 [22:09<46:28,  1.64s/it, Loss=4.2029, Top1=N/A, LR=0.099966]2025-11-04 04:22:27,714 - INFO - Step 18314: {'train_loss_batch': 3.505481004714966, 'train_lr': 0.09996585642185044, 'batch_time': 1.6602048037501609, 'data_time': 0.005958532423859976}
2025-11-04 04:22:27 Train Epoch 007:  36%|███▌      | 900/2502 [24:55<44:30,  1.67s/it, Loss=4.1944, Top1=N/A, LR=0.099966]2025-11-04 04:25:13,559 - INFO - Step 18414: {'train_loss_batch': 5.585267543792725, 'train_lr': 0.09996585642185044, 'batch_time': 1.6600088636565553, 'data_time': 0.0054030950273181435}
2025-11-04 04:25:13 Train Epoch 007:  40%|███▉      | 1000/2502 [27:42<41:36,  1.66s/it, Loss=4.2073, Top1=N/A, LR=0.099966]2025-11-04 04:27:59,950 - INFO - Step 18514: {'train_loss_batch': 5.040204048156738, 'train_lr': 0.09996585642185044, 'batch_time': 1.6603992440245607, 'data_time': 0.004960328310757846}
2025-11-04 04:27:59 Train Epoch 007:  44%|████▍     | 1100/2502 [30:28<38:50,  1.66s/it, Loss=4.1966, Top1=N/A, LR=0.099966]2025-11-04 04:30:46,272 - INFO - Step 18614: {'train_loss_batch': 3.637002468109131, 'train_lr': 0.09996585642185044, 'batch_time': 1.6606549068974106, 'data_time': 0.004594650407144961}
2025-11-04 04:30:46 Train Epoch 007:  48%|████▊     | 1200/2502 [33:13<35:43,  1.65s/it, Loss=4.2022, Top1=N/A, LR=0.099966]2025-11-04 04:33:30,899 - INFO - Step 18714: {'train_loss_batch': 5.319904327392578, 'train_lr': 0.09996585642185044, 'batch_time': 1.6594565966841979, 'data_time': 0.004287558729503673}
2025-11-04 04:33:30 Train Epoch 007:  52%|█████▏    | 1300/2502 [35:58<32:51,  1.64s/it, Loss=4.1957, Top1=N/A, LR=0.099966]2025-11-04 04:36:16,111 - INFO - Step 18814: {'train_loss_batch': 4.883412837982178, 'train_lr': 0.09996585642185044, 'batch_time': 1.6588928310986577, 'data_time': 0.004029781611308421}
2025-11-04 04:36:16 Train Epoch 007:  56%|█████▌    | 1400/2502 [38:43<30:33,  1.66s/it, Loss=4.1957, Top1=N/A, LR=0.099966]2025-11-04 04:39:01,560 - INFO - Step 18914: {'train_loss_batch': 5.259434700012207, 'train_lr': 0.09996585642185044, 'batch_time': 1.6585780927575033, 'data_time': 0.003805701856865022}
2025-11-04 04:39:01 Train Epoch 007:  60%|█████▉    | 1500/2502 [41:29<27:37,  1.65s/it, Loss=4.1925, Top1=N/A, LR=0.099966]2025-11-04 04:41:46,973 - INFO - Step 19014: {'train_loss_batch': 3.5923407077789307, 'train_lr': 0.09996585642185044, 'batch_time': 1.6582818204446446, 'data_time': 0.0036142601480808043}
2025-11-04 04:41:46 Train Epoch 007:  64%|██████▍   | 1600/2502 [44:14<24:41,  1.64s/it, Loss=4.1920, Top1=N/A, LR=0.099966]2025-11-04 04:44:32,318 - INFO - Step 19114: {'train_loss_batch': 5.045331954956055, 'train_lr': 0.09996585642185044, 'batch_time': 1.6579799521051297, 'data_time': 0.003445579438266123}
2025-11-04 04:44:32 Train Epoch 007:  68%|██████▊   | 1700/2502 [47:00<22:13,  1.66s/it, Loss=4.1883, Top1=N/A, LR=0.099966]2025-11-04 04:47:18,536 - INFO - Step 19214: {'train_loss_batch': 3.8201773166656494, 'train_lr': 0.09996585642185044, 'batch_time': 1.6582268988784798, 'data_time': 0.0033005981288329075}
2025-11-04 04:47:18 Train Epoch 007:  72%|███████▏  | 1800/2502 [49:46<19:22,  1.66s/it, Loss=4.1862, Top1=N/A, LR=0.099966]2025-11-04 04:50:04,505 - INFO - Step 19314: {'train_loss_batch': 4.003767013549805, 'train_lr': 0.09996585642185044, 'batch_time': 1.6583077258894272, 'data_time': 0.0031736760189770194}
2025-11-04 04:50:04 Train Epoch 007:  76%|███████▌  | 1900/2502 [52:30<16:20,  1.63s/it, Loss=4.1858, Top1=39.86%, LR=0.099966]2025-11-04 04:52:48,868 - INFO - Step 19414: {'train_loss_batch': 3.4785542488098145, 'train_lr': 0.09996585642185044, 'batch_time': 1.6575355582460487, 'data_time': 0.0030583555230587673}
2025-11-04 04:52:48 Train Epoch 007:  80%|███████▉  | 2000/2502 [55:15<13:52,  1.66s/it, Loss=4.1840, Top1=N/A, LR=0.099966]   2025-11-04 04:55:32,964 - INFO - Step 19514: {'train_loss_batch': 3.9494190216064453, 'train_lr': 0.09996585642185044, 'batch_time': 1.6567069816922975, 'data_time': 0.002950094867384118}
2025-11-04 04:55:32 Train Epoch 007:  84%|████████▍ | 2100/2502 [58:01<11:07,  1.66s/it, Loss=4.1754, Top1=N/A, LR=0.099966]2025-11-04 04:58:19,191 - INFO - Step 19614: {'train_loss_batch': 4.698056697845459, 'train_lr': 0.09996585642185044, 'batch_time': 1.6569720680857771, 'data_time': 0.0028567316417521605}
2025-11-04 04:58:19 Train Epoch 007:  88%|████████▊ | 2200/2502 [1:00:47<08:19,  1.65s/it, Loss=4.1742, Top1=N/A, LR=0.099966]2025-11-04 05:01:05,216 - INFO - Step 19714: {'train_loss_batch': 5.190164566040039, 'train_lr': 0.09996585642185044, 'batch_time': 1.657120849478521, 'data_time': 0.002772417679422284}
2025-11-04 05:01:05 Train Epoch 007:  92%|█████████▏| 2300/2502 [1:03:33<05:36,  1.67s/it, Loss=4.1767, Top1=N/A, LR=0.099966]2025-11-04 05:03:51,193 - INFO - Step 19814: {'train_loss_batch': 3.5658624172210693, 'train_lr': 0.09996585642185044, 'batch_time': 1.657235897918827, 'data_time': 0.0026926652393564872}
2025-11-04 05:03:51 Train Epoch 007:  96%|█████████▌| 2400/2502 [1:06:19<02:49,  1.66s/it, Loss=4.1709, Top1=N/A, LR=0.099966]2025-11-04 05:06:37,188 - INFO - Step 19914: {'train_loss_batch': 3.854848861694336, 'train_lr': 0.09996585642185044, 'batch_time': 1.6573486349970934, 'data_time': 0.0026184375561162463}
2025-11-04 05:06:37 Train Epoch 007: 100%|█████████▉| 2500/2502 [1:09:03<00:03,  1.66s/it, Loss=4.1686, Top1=40.11%, LR=0.099966]2025-11-04 05:09:21,685 - INFO - Step 20014: {'train_loss_batch': 3.3771257400512695, 'train_lr': 0.09996585642185044, 'batch_time': 1.6568538239840171, 'data_time': 0.0025937382767840128}
2025-11-04 05:09:21 Train Epoch 007: 100%|██████████| 2502/2502 [1:09:05<00:00,  1.66s/it, Loss=4.1686, Top1=40.11%, LR=0.099966]
2025-11-04 05:09:23 Val Epoch 007:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 05:09:28   with torch.cuda.amp.autocast():
2025-11-04 05:09:28 Val Epoch 007: 100%|██████████| 98/98 [01:51<00:00,  1.13s/it, Loss=3.1870, Top1=45.70%, Top5=72.09%]
2025-11-04 05:11:15 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 05:11:15   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 05:11:15 2025-11-04 05:11:15,205 - INFO - Step 7: {'epoch': 7, 'learning_rate': 0.09986347232342248, 'train_loss': 4.168333507842012, 'train_top1': 40.111115987827716, 'train_top5': 65.37445868445693, 'train_precision': 39.179595949984524, 'train_recall': 39.96227868960157, 'train_f1': 39.139998811239444, 'val_loss': 3.187036539993286, 'val_top1': 45.70199998657227, 'val_top5': 72.09199997802735, 'val_precision': 50.46819712982307, 'val_recall': 45.702, 'val_f1': 44.89471194292999}
2025-11-04 05:11:15 2025-11-04 05:11:15,207 - INFO - Epoch 007 Summary - LR: 0.099863, Train Loss: 4.1683, Val Loss: 3.1870, Val F1: 44.89%, Val Precision: 50.47%, Val Recall: 45.70%
2025-11-04 05:11:16 wandb: WARNING Tried to log to step 7 that is less than the current step 20014. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 05:11:18 2025-11-04 05:11:18,178 - INFO - New best model saved with validation accuracy: 45.702%
2025-11-04 05:11:18 2025-11-04 05:11:18,178 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_008.pth
2025-11-04 05:11:18 Train Epoch 008:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 05:11:21   with torch.cuda.amp.autocast():
2025-11-04 05:11:23 Train Epoch 008:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.6259, Top1=N/A, LR=0.099863]2025-11-04 05:11:23,617 - INFO - Step 20016: {'train_loss_batch': 3.62593936920166, 'train_lr': 0.09986347232342248, 'batch_time': 5.437074899673462, 'data_time': 3.7816848754882812}
2025-11-04 05:11:23 Train Epoch 008:   4%|▍         | 100/2502 [02:51<1:05:55,  1.65s/it, Loss=4.1148, Top1=N/A, LR=0.099863]2025-11-04 05:14:09,534 - INFO - Step 20116: {'train_loss_batch': 4.879548072814941, 'train_lr': 0.09986347232342248, 'batch_time': 1.696580426527722, 'data_time': 0.03839936114773892}
2025-11-04 05:14:09 Train Epoch 008:   8%|▊         | 200/2502 [05:36<1:03:20,  1.65s/it, Loss=4.1885, Top1=N/A, LR=0.099863]2025-11-04 05:16:54,779 - INFO - Step 20216: {'train_loss_batch': 3.4986765384674072, 'train_lr': 0.09986347232342248, 'batch_time': 1.674625388425381, 'data_time': 0.019769744493475006}
2025-11-04 05:16:54 Train Epoch 008:  12%|█▏        | 300/2502 [08:22<1:01:10,  1.67s/it, Loss=4.2067, Top1=N/A, LR=0.099863]2025-11-04 05:19:40,755 - INFO - Step 20316: {'train_loss_batch': 4.525128364562988, 'train_lr': 0.09986347232342248, 'batch_time': 1.669684418016098, 'data_time': 0.013530133174503365}
2025-11-04 05:19:40 Train Epoch 008:  16%|█▌        | 400/2502 [11:08<58:00,  1.66s/it, Loss=4.1598, Top1=42.06%, LR=0.099863]2025-11-04 05:22:26,500 - INFO - Step 20416: {'train_loss_batch': 3.322157859802246, 'train_lr': 0.09986347232342248, 'batch_time': 1.6666319251357766, 'data_time': 0.010387681666157786}
2025-11-04 05:22:26 Train Epoch 008:  20%|█▉        | 500/2502 [13:53<55:31,  1.66s/it, Loss=4.1196, Top1=N/A, LR=0.099863]   2025-11-04 05:25:11,824 - INFO - Step 20516: {'train_loss_batch': 3.389589548110962, 'train_lr': 0.09986347232342248, 'batch_time': 1.6639591300796843, 'data_time': 0.008495958503372893}
2025-11-04 05:25:11 Train Epoch 008:  24%|██▍       | 600/2502 [16:39<52:47,  1.67s/it, Loss=4.1212, Top1=N/A, LR=0.099863]2025-11-04 05:27:57,803 - INFO - Step 20616: {'train_loss_batch': 4.877619743347168, 'train_lr': 0.09986347232342248, 'batch_time': 1.6632655254021262, 'data_time': 0.007241895710569055}
2025-11-04 05:27:57 Train Epoch 008:  28%|██▊       | 700/2502 [19:25<49:49,  1.66s/it, Loss=4.1129, Top1=N/A, LR=0.099863]2025-11-04 05:30:43,801 - INFO - Step 20716: {'train_loss_batch': 3.300163507461548, 'train_lr': 0.09986347232342248, 'batch_time': 1.6627970027515449, 'data_time': 0.006347196418446583}
2025-11-04 05:30:43 Train Epoch 008:  32%|███▏      | 800/2502 [22:11<46:56,  1.65s/it, Loss=4.1013, Top1=N/A, LR=0.099863]2025-11-04 05:33:29,491 - INFO - Step 20816: {'train_loss_batch': 4.5825042724609375, 'train_lr': 0.09986347232342248, 'batch_time': 1.6620586552423484, 'data_time': 0.005668118949537717}
2025-11-04 05:33:29 Train Epoch 008:  36%|███▌      | 900/2502 [24:57<44:18,  1.66s/it, Loss=4.0937, Top1=N/A, LR=0.099863]2025-11-04 05:36:15,292 - INFO - Step 20916: {'train_loss_batch': 3.367140531539917, 'train_lr': 0.09986347232342248, 'batch_time': 1.6616100103291502, 'data_time': 0.005152665814601886}
2025-11-04 05:36:15 Train Epoch 008:  40%|███▉      | 1000/2502 [27:43<41:33,  1.66s/it, Loss=4.0852, Top1=N/A, LR=0.099863]2025-11-04 05:39:01,193 - INFO - Step 21016: {'train_loss_batch': 3.438157796859741, 'train_lr': 0.09986347232342248, 'batch_time': 1.6613497217218358, 'data_time': 0.004735335484370366}
2025-11-04 05:39:01 Train Epoch 008:  44%|████▍     | 1100/2502 [30:27<38:18,  1.64s/it, Loss=4.0799, Top1=42.30%, LR=0.099863]2025-11-04 05:41:45,281 - INFO - Step 21116: {'train_loss_batch': 3.2686727046966553, 'train_lr': 0.09986347232342248, 'batch_time': 1.65949116220916, 'data_time': 0.004392348669746807}
2025-11-04 05:41:45 Train Epoch 008:  48%|████▊     | 1200/2502 [33:12<36:09,  1.67s/it, Loss=4.0815, Top1=N/A, LR=0.099863]   2025-11-04 05:44:30,513 - INFO - Step 21216: {'train_loss_batch': 5.04334831237793, 'train_lr': 0.09986347232342248, 'batch_time': 1.6588931349691602, 'data_time': 0.004107047675749742}
2025-11-04 05:44:30 Train Epoch 008:  52%|█████▏    | 1300/2502 [35:57<33:07,  1.65s/it, Loss=4.0834, Top1=N/A, LR=0.099863]2025-11-04 05:47:16,141 - INFO - Step 21316: {'train_loss_batch': 4.5477495193481445, 'train_lr': 0.09986347232342248, 'batch_time': 1.6586925739695895, 'data_time': 0.0038688325772003244}
2025-11-04 05:47:16 Train Epoch 008:  56%|█████▌    | 1400/2502 [38:43<30:19,  1.65s/it, Loss=4.0816, Top1=N/A, LR=0.099863]2025-11-04 05:50:01,933 - INFO - Step 21416: {'train_loss_batch': 4.977867126464844, 'train_lr': 0.09986347232342248, 'batch_time': 1.65863661013868, 'data_time': 0.0036635943432521342}
2025-11-04 05:50:01 Train Epoch 008:  60%|█████▉    | 1500/2502 [41:29<27:46,  1.66s/it, Loss=4.0791, Top1=N/A, LR=0.099863]2025-11-04 05:52:47,844 - INFO - Step 21516: {'train_loss_batch': 5.0444416999816895, 'train_lr': 0.09986347232342248, 'batch_time': 1.6586683973481384, 'data_time': 0.0034840029132596497}
2025-11-04 05:52:47 Train Epoch 008:  64%|██████▍   | 1600/2502 [44:15<24:46,  1.65s/it, Loss=4.0862, Top1=N/A, LR=0.099863]2025-11-04 05:55:33,718 - INFO - Step 21616: {'train_loss_batch': 4.941887378692627, 'train_lr': 0.09986347232342248, 'batch_time': 1.6586731427315993, 'data_time': 0.0033190557168916494}
2025-11-04 05:55:33 Train Epoch 008:  68%|██████▊   | 1700/2502 [47:01<22:09,  1.66s/it, Loss=4.0818, Top1=N/A, LR=0.099863]2025-11-04 05:58:19,449 - INFO - Step 21716: {'train_loss_batch': 3.38736629486084, 'train_lr': 0.09986347232342248, 'batch_time': 1.6585926583484647, 'data_time': 0.0031738839101819413}
2025-11-04 05:58:19 Train Epoch 008:  72%|███████▏  | 1800/2502 [49:46<19:21,  1.65s/it, Loss=4.0876, Top1=N/A, LR=0.099863]2025-11-04 06:01:04,458 - INFO - Step 21816: {'train_loss_batch': 5.0295186042785645, 'train_lr': 0.09986347232342248, 'batch_time': 1.658120045987584, 'data_time': 0.0030439654566856438}
2025-11-04 06:01:04 Train Epoch 008:  76%|███████▌  | 1900/2502 [52:32<16:38,  1.66s/it, Loss=4.0889, Top1=N/A, LR=0.099863]2025-11-04 06:03:50,686 - INFO - Step 21916: {'train_loss_batch': 4.356858253479004, 'train_lr': 0.09986347232342248, 'batch_time': 1.6583392176108884, 'data_time': 0.0029268106744516405}
2025-11-04 06:03:50 Train Epoch 008:  80%|███████▉  | 2000/2502 [55:18<13:52,  1.66s/it, Loss=4.0908, Top1=N/A, LR=0.099863]2025-11-04 06:06:36,523 - INFO - Step 22016: {'train_loss_batch': 3.431838274002075, 'train_lr': 0.09986347232342248, 'batch_time': 1.6583404500504721, 'data_time': 0.0028210739562774764}
2025-11-04 06:06:36 Train Epoch 008:  84%|████████▍ | 2100/2502 [58:04<11:07,  1.66s/it, Loss=4.0798, Top1=N/A, LR=0.099863]2025-11-04 06:09:22,411 - INFO - Step 22116: {'train_loss_batch': 3.3572826385498047, 'train_lr': 0.09986347232342248, 'batch_time': 1.6583662804735666, 'data_time': 0.0027264745277430203}
2025-11-04 06:09:22 Train Epoch 008:  88%|████████▊ | 2200/2502 [1:00:50<08:19,  1.65s/it, Loss=4.0757, Top1=N/A, LR=0.099863]2025-11-04 06:12:08,436 - INFO - Step 22216: {'train_loss_batch': 4.66196346282959, 'train_lr': 0.09986347232342248, 'batch_time': 1.6584518591635122, 'data_time': 0.002641881720037257}
2025-11-04 06:12:08 Train Epoch 008:  92%|█████████▏| 2300/2502 [1:03:34<05:31,  1.64s/it, Loss=4.0712, Top1=N/A, LR=0.099863]2025-11-04 06:14:53,145 - INFO - Step 22316: {'train_loss_batch': 4.1634297370910645, 'train_lr': 0.09986347232342248, 'batch_time': 1.6579578323811877, 'data_time': 0.0025636376841386365}
2025-11-04 06:14:53 Train Epoch 008:  96%|█████████▌| 2400/2502 [1:06:20<02:49,  1.66s/it, Loss=4.0690, Top1=N/A, LR=0.099863]2025-11-04 06:17:38,731 - INFO - Step 22416: {'train_loss_batch': 3.295010805130005, 'train_lr': 0.09986347232342248, 'batch_time': 1.657870527209465, 'data_time': 0.002491506026814948}
2025-11-04 06:17:38 Train Epoch 008: 100%|█████████▉| 2500/2502 [1:09:06<00:03,  1.67s/it, Loss=4.0656, Top1=N/A, LR=0.099863]2025-11-04 06:20:24,186 - INFO - Step 22516: {'train_loss_batch': 3.2899093627929688, 'train_lr': 0.09986347232342248, 'batch_time': 1.6577376486157855, 'data_time': 0.0024438881483234342}
2025-11-04 06:20:24 Train Epoch 008: 100%|██████████| 2502/2502 [1:09:07<00:00,  1.66s/it, Loss=4.0656, Top1=N/A, LR=0.099863]
2025-11-04 06:20:26 Val Epoch 008:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 06:20:30   with torch.cuda.amp.autocast():
2025-11-04 06:20:31 Val Epoch 008: 100%|██████████| 98/98 [01:45<00:00,  1.07s/it, Loss=3.0972, Top1=47.64%, Top5=74.07%]
2025-11-04 06:22:11 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 06:22:11   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 06:22:11 2025-11-04 06:22:11,752 - INFO - Step 8: {'epoch': 8, 'learning_rate': 0.09969298754907922, 'train_loss': 4.065843896042529, 'train_top1': 42.713857323232325, 'train_top5': 67.81210542929293, 'train_precision': 41.93845802303025, 'train_recall': 42.60364157932258, 'train_f1': 41.86728073305409, 'val_loss': 3.0971670538330076, 'val_top1': 47.642, 'val_top5': 74.06999997070312, 'val_precision': 51.96667849625819, 'val_recall': 47.64600000000001, 'val_f1': 46.51508404013617}
2025-11-04 06:22:11 2025-11-04 06:22:11,754 - INFO - Epoch 008 Summary - LR: 0.099693, Train Loss: 4.0658, Val Loss: 3.0972, Val F1: 46.52%, Val Precision: 51.97%, Val Recall: 47.65%
2025-11-04 06:22:14 2025-11-04 06:22:14,744 - INFO - New best model saved with validation accuracy: 47.642%
2025-11-04 06:22:14 2025-11-04 06:22:14,744 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_009.pth
2025-11-04 06:22:14 Train Epoch 009:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 8 that is less than the current step 22516. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 06:22:18 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 06:22:18   with torch.cuda.amp.autocast():
2025-11-04 06:22:20 Train Epoch 009:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.4067, Top1=43.55%, LR=0.099693]2025-11-04 06:22:20,322 - INFO - Step 22518: {'train_loss_batch': 3.4066965579986572, 'train_lr': 0.09969298754907922, 'batch_time': 5.5763099193573, 'data_time': 3.9284112453460693}
2025-11-04 06:22:20 Train Epoch 009:   4%|▍         | 100/2502 [02:51<1:06:27,  1.66s/it, Loss=4.0267, Top1=N/A, LR=0.099693]   2025-11-04 06:25:06,451 - INFO - Step 22618: {'train_loss_batch': 3.764812469482422, 'train_lr': 0.09969298754907922, 'batch_time': 1.7000476110099565, 'data_time': 0.039742915937215975}
2025-11-04 06:25:06 Train Epoch 009:   8%|▊         | 200/2502 [05:37<1:03:33,  1.66s/it, Loss=4.0154, Top1=N/A, LR=0.099693]2025-11-04 06:27:52,408 - INFO - Step 22718: {'train_loss_batch': 4.1218767166137695, 'train_lr': 0.09969298754907922, 'batch_time': 1.679910758241492, 'data_time': 0.02038607312672174}
2025-11-04 06:27:52 Train Epoch 009:  12%|█▏        | 300/2502 [08:23<1:00:56,  1.66s/it, Loss=3.9952, Top1=N/A, LR=0.099693]2025-11-04 06:30:38,303 - INFO - Step 22818: {'train_loss_batch': 3.236875057220459, 'train_lr': 0.09969298754907922, 'batch_time': 1.672945339418329, 'data_time': 0.013879350649558033}
2025-11-04 06:30:38 Train Epoch 009:  16%|█▌        | 400/2502 [11:08<57:54,  1.65s/it, Loss=4.0145, Top1=44.23%, LR=0.099693]2025-11-04 06:33:23,651 - INFO - Step 22918: {'train_loss_batch': 3.2270216941833496, 'train_lr': 0.09969298754907922, 'batch_time': 1.6680904279028685, 'data_time': 0.010622097072458625}
2025-11-04 06:33:23 Train Epoch 009:  20%|█▉        | 500/2502 [13:54<54:54,  1.65s/it, Loss=4.0073, Top1=N/A, LR=0.099693]   2025-11-04 06:36:08,948 - INFO - Step 23018: {'train_loss_batch': 4.425650119781494, 'train_lr': 0.09969298754907922, 'batch_time': 1.665073629862772, 'data_time': 0.008663112769821684}
2025-11-04 06:36:08 Train Epoch 009:  24%|██▍       | 600/2502 [16:40<52:47,  1.67s/it, Loss=4.0071, Top1=N/A, LR=0.099693]2025-11-04 06:38:54,916 - INFO - Step 23118: {'train_loss_batch': 3.8081986904144287, 'train_lr': 0.09969298754907922, 'batch_time': 1.6641751946308054, 'data_time': 0.0073630940695967335}
2025-11-04 06:38:54 Train Epoch 009:  28%|██▊       | 700/2502 [19:25<49:44,  1.66s/it, Loss=4.0132, Top1=N/A, LR=0.099693]2025-11-04 06:41:40,003 - INFO - Step 23218: {'train_loss_batch': 4.9714202880859375, 'train_lr': 0.09969298754907922, 'batch_time': 1.66227667682011, 'data_time': 0.006440898321154454}
2025-11-04 06:41:40 Train Epoch 009:  32%|███▏      | 800/2502 [22:11<46:44,  1.65s/it, Loss=3.9908, Top1=N/A, LR=0.099693]2025-11-04 06:44:25,860 - INFO - Step 23318: {'train_loss_batch': 3.469177484512329, 'train_lr': 0.09969298754907922, 'batch_time': 1.6618134853396374, 'data_time': 0.005744064642993103}
2025-11-04 06:44:25 Train Epoch 009:  36%|███▌      | 900/2502 [24:56<44:12,  1.66s/it, Loss=3.9931, Top1=N/A, LR=0.099693]2025-11-04 06:47:11,065 - INFO - Step 23418: {'train_loss_batch': 4.086173057556152, 'train_lr': 0.09969298754907922, 'batch_time': 1.660730249212267, 'data_time': 0.005199151880600873}
2025-11-04 06:47:11 Train Epoch 009:  40%|███▉      | 1000/2502 [27:41<41:24,  1.65s/it, Loss=4.0054, Top1=44.20%, LR=0.099693]2025-11-04 06:49:56,251 - INFO - Step 23518: {'train_loss_batch': 3.3586480617523193, 'train_lr': 0.09969298754907922, 'batch_time': 1.6598436722864995, 'data_time': 0.004761807806603797}
2025-11-04 06:49:56 Train Epoch 009:  44%|████▍     | 1100/2502 [30:26<38:10,  1.63s/it, Loss=4.0008, Top1=N/A, LR=0.099693]   2025-11-04 06:52:40,926 - INFO - Step 23618: {'train_loss_batch': 5.457853317260742, 'train_lr': 0.09969298754907922, 'batch_time': 1.6586542638402761, 'data_time': 0.004404432658820018}
2025-11-04 06:52:40 Train Epoch 009:  48%|████▊     | 1200/2502 [33:11<35:58,  1.66s/it, Loss=3.9975, Top1=N/A, LR=0.099693]2025-11-04 06:55:25,808 - INFO - Step 23718: {'train_loss_batch': 5.204995155334473, 'train_lr': 0.09969298754907922, 'batch_time': 1.657835072422901, 'data_time': 0.004108451188950614}
2025-11-04 06:55:25 Train Epoch 009:  52%|█████▏    | 1300/2502 [35:56<32:52,  1.64s/it, Loss=3.9941, Top1=N/A, LR=0.099693]2025-11-04 06:58:10,782 - INFO - Step 23818: {'train_loss_batch': 3.339230537414551, 'train_lr': 0.09969298754907922, 'batch_time': 1.6572130321998215, 'data_time': 0.003861210696244588}
2025-11-04 06:58:10 Train Epoch 009:  56%|█████▌    | 1400/2502 [38:41<30:16,  1.65s/it, Loss=3.9920, Top1=44.34%, LR=0.099693]2025-11-04 07:00:55,829 - INFO - Step 23918: {'train_loss_batch': 3.1933326721191406, 'train_lr': 0.09969298754907922, 'batch_time': 1.6567317569876976, 'data_time': 0.0036451985034493357}
2025-11-04 07:00:55 Train Epoch 009:  60%|█████▉    | 1500/2502 [41:26<27:25,  1.64s/it, Loss=3.9954, Top1=N/A, LR=0.099693]   2025-11-04 07:03:41,515 - INFO - Step 24018: {'train_loss_batch': 3.234941244125366, 'train_lr': 0.09969298754907922, 'batch_time': 1.6567399072933007, 'data_time': 0.0034571707367817613}
2025-11-04 07:03:41 Train Epoch 009:  64%|██████▍   | 1600/2502 [44:11<24:45,  1.65s/it, Loss=3.9997, Top1=44.34%, LR=0.099693]2025-11-04 07:06:26,201 - INFO - Step 24118: {'train_loss_batch': 3.3514175415039062, 'train_lr': 0.09969298754907922, 'batch_time': 1.6561225063721885, 'data_time': 0.0032933770977356222}
2025-11-04 07:06:26 Train Epoch 009:  68%|██████▊   | 1700/2502 [46:56<21:58,  1.64s/it, Loss=4.0036, Top1=N/A, LR=0.099693]   2025-11-04 07:09:11,166 - INFO - Step 24218: {'train_loss_batch': 4.855400562286377, 'train_lr': 0.09969298754907922, 'batch_time': 1.6557422493010112, 'data_time': 0.0031477861163337253}
2025-11-04 07:09:11 Train Epoch 009:  72%|███████▏  | 1800/2502 [49:41<19:26,  1.66s/it, Loss=3.9959, Top1=N/A, LR=0.099693]2025-11-04 07:11:56,672 - INFO - Step 24318: {'train_loss_batch': 3.4460012912750244, 'train_lr': 0.09969298754907922, 'batch_time': 1.655704141656006, 'data_time': 0.003018067188887779}
2025-11-04 07:11:56 Train Epoch 009:  76%|███████▌  | 1900/2502 [52:27<16:30,  1.64s/it, Loss=3.9994, Top1=44.49%, LR=0.099693]2025-11-04 07:14:42,231 - INFO - Step 24418: {'train_loss_batch': 3.232431173324585, 'train_lr': 0.09969298754907922, 'batch_time': 1.6556980110732333, 'data_time': 0.002903420946461348}
2025-11-04 07:14:42 Train Epoch 009:  80%|███████▉  | 2000/2502 [55:12<13:42,  1.64s/it, Loss=4.0015, Top1=N/A, LR=0.099693]   2025-11-04 07:17:27,031 - INFO - Step 24518: {'train_loss_batch': 3.744046211242676, 'train_lr': 0.09969298754907922, 'batch_time': 1.65531295695822, 'data_time': 0.002799665492990504}
2025-11-04 07:17:27 Train Epoch 009:  84%|████████▍ | 2100/2502 [57:57<11:03,  1.65s/it, Loss=3.9972, Top1=N/A, LR=0.099693]2025-11-04 07:20:12,262 - INFO - Step 24618: {'train_loss_batch': 4.915942668914795, 'train_lr': 0.09969298754907922, 'batch_time': 1.6551700312429698, 'data_time': 0.0027073335216364256}
2025-11-04 07:20:12 Train Epoch 009:  88%|████████▊ | 2200/2502 [1:00:43<08:18,  1.65s/it, Loss=3.9936, Top1=N/A, LR=0.099693]2025-11-04 07:22:58,412 - INFO - Step 24718: {'train_loss_batch': 3.240598678588867, 'train_lr': 0.09969298754907922, 'batch_time': 1.6554576117902493, 'data_time': 0.002621772559000004}
2025-11-04 07:22:58 Train Epoch 009:  92%|█████████▏| 2300/2502 [1:03:29<05:35,  1.66s/it, Loss=3.9909, Top1=N/A, LR=0.099693]2025-11-04 07:25:44,143 - INFO - Step 24818: {'train_loss_batch': 5.350987434387207, 'train_lr': 0.09969298754907922, 'batch_time': 1.6555378444295301, 'data_time': 0.0025442045494454262}
2025-11-04 07:25:44 Train Epoch 009:  96%|█████████▌| 2400/2502 [1:06:15<02:49,  1.67s/it, Loss=3.9910, Top1=44.62%, LR=0.099693]2025-11-04 07:28:29,996 - INFO - Step 24918: {'train_loss_batch': 3.2833309173583984, 'train_lr': 0.09969298754907922, 'batch_time': 1.655662475651078, 'data_time': 0.0024726254798034387}
2025-11-04 07:28:29 Train Epoch 009: 100%|█████████▉| 2500/2502 [1:09:01<00:03,  1.66s/it, Loss=3.9918, Top1=N/A, LR=0.099693]   2025-11-04 07:31:15,942 - INFO - Step 25018: {'train_loss_batch': 3.3177261352539062, 'train_lr': 0.09969298754907922, 'batch_time': 1.6558143552042683, 'data_time': 0.002424966616899383}
2025-11-04 07:31:15 Train Epoch 009: 100%|██████████| 2502/2502 [1:09:03<00:00,  1.66s/it, Loss=3.9918, Top1=N/A, LR=0.099693]
2025-11-04 07:31:18 Val Epoch 009:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 07:31:23   with torch.cuda.amp.autocast():
2025-11-04 07:31:23 Val Epoch 009: 100%|██████████| 98/98 [01:48<00:00,  1.11s/it, Loss=2.9724, Top1=50.51%, Top5=76.35%]
2025-11-04 07:33:06 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 07:33:06   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 07:33:06 2025-11-04 07:33:06,990 - INFO - Step 9: {'epoch': 9, 'learning_rate': 0.09945463496051546, 'train_loss': 3.9921889289868155, 'train_top1': 44.63191414271047, 'train_top5': 69.56934997433265, 'train_precision': 43.967595140004455, 'train_recall': 44.49797375285623, 'train_f1': 43.864979701630354, 'val_loss': 2.972418375854492, 'val_top1': 50.50599999633789, 'val_top5': 76.35199999267579, 'val_precision': 54.91425064992509, 'val_recall': 50.51, 'val_f1': 49.52190316982063}
2025-11-04 07:33:06 2025-11-04 07:33:06,991 - INFO - Epoch 009 Summary - LR: 0.099455, Train Loss: 3.9922, Val Loss: 2.9724, Val F1: 49.52%, Val Precision: 54.91%, Val Recall: 50.51%
2025-11-04 07:33:09 2025-11-04 07:33:09,943 - INFO - New best model saved with validation accuracy: 50.506%
2025-11-04 07:33:09 2025-11-04 07:33:09,943 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_010.pth
2025-11-04 07:33:10 Train Epoch 010:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 07:33:13   with torch.cuda.amp.autocast():
2025-11-04 07:33:15 Train Epoch 010:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.1607, Top1=N/A, LR=0.099455]2025-11-04 07:33:15,344 - INFO - Step 25020: {'train_loss_batch': 3.160706043243408, 'train_lr': 0.09945463496051546, 'batch_time': 5.3980982303619385, 'data_time': 3.7541415691375732}
2025-11-04 07:33:15 Train Epoch 010:   0%|          | 1/2502 [00:05<3:45:14,  5.40s/it, Loss=3.1607, Top1=N/A, LR=0.099455]wandb: WARNING Tried to log to step 9 that is less than the current step 25018. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 07:33:17 Train Epoch 010:   4%|▍         | 100/2502 [02:50<1:06:31,  1.66s/it, Loss=3.8235, Top1=N/A, LR=0.099455]2025-11-04 07:36:00,244 - INFO - Step 25120: {'train_loss_batch': 3.3533313274383545, 'train_lr': 0.09945463496051546, 'batch_time': 1.6861303820468412, 'data_time': 0.038046064943370254}
2025-11-04 07:36:00 Train Epoch 010:   8%|▊         | 200/2502 [05:36<1:03:35,  1.66s/it, Loss=3.8672, Top1=N/A, LR=0.099455]2025-11-04 07:38:45,963 - INFO - Step 25220: {'train_loss_batch': 4.918383598327637, 'train_lr': 0.09945463496051546, 'batch_time': 1.6717323960356452, 'data_time': 0.019527839784005387}
2025-11-04 07:38:45 Train Epoch 010:  12%|█▏        | 300/2502 [08:21<1:00:12,  1.64s/it, Loss=3.8990, Top1=N/A, LR=0.099455]2025-11-04 07:41:31,199 - INFO - Step 25320: {'train_loss_batch': 4.403515338897705, 'train_lr': 0.09945463496051546, 'batch_time': 1.6652976436868459, 'data_time': 0.013327128863413865}
2025-11-04 07:41:31 Train Epoch 010:  16%|█▌        | 400/2502 [11:06<58:04,  1.66s/it, Loss=3.9120, Top1=N/A, LR=0.099455]2025-11-04 07:44:16,023 - INFO - Step 25420: {'train_loss_batch': 4.9855194091796875, 'train_lr': 0.09945463496051546, 'batch_time': 1.6610425612575692, 'data_time': 0.010212470171160235}
2025-11-04 07:44:16 Train Epoch 010:  20%|█▉        | 500/2502 [13:51<55:34,  1.67s/it, Loss=3.8887, Top1=46.14%, LR=0.099455]2025-11-04 07:47:01,853 - INFO - Step 25520: {'train_loss_batch': 3.2011914253234863, 'train_lr': 0.09945463496051546, 'batch_time': 1.6604948010511265, 'data_time': 0.008346064124040736}
2025-11-04 07:47:01 Train Epoch 010:  24%|██▍       | 600/2502 [16:36<52:28,  1.66s/it, Loss=3.9055, Top1=N/A, LR=0.099455]   2025-11-04 07:49:46,159 - INFO - Step 25620: {'train_loss_batch': 3.923403263092041, 'train_lr': 0.09945463496051546, 'batch_time': 1.6575938362051763, 'data_time': 0.007099281333250531}
2025-11-04 07:49:46 Train Epoch 010:  28%|██▊       | 700/2502 [19:22<49:39,  1.65s/it, Loss=3.9035, Top1=N/A, LR=0.099455]2025-11-04 07:52:31,966 - INFO - Step 25720: {'train_loss_batch': 5.483853816986084, 'train_lr': 0.09945463496051546, 'batch_time': 1.6576612546678617, 'data_time': 0.0062079552066819305}
2025-11-04 07:52:31 Train Epoch 010:  32%|███▏      | 800/2502 [22:07<47:13,  1.66s/it, Loss=3.9195, Top1=N/A, LR=0.099455]2025-11-04 07:55:17,278 - INFO - Step 25820: {'train_loss_batch': 4.185859680175781, 'train_lr': 0.09945463496051546, 'batch_time': 1.6570936198835813, 'data_time': 0.005535534109813295}
2025-11-04 07:55:17 Train Epoch 010:  36%|███▌      | 900/2502 [24:52<43:54,  1.64s/it, Loss=3.9473, Top1=N/A, LR=0.099455]2025-11-04 07:58:02,886 - INFO - Step 25920: {'train_loss_batch': 4.6944193840026855, 'train_lr': 0.09945463496051546, 'batch_time': 1.6569808056034339, 'data_time': 0.005010022175034196}
2025-11-04 07:58:02 Train Epoch 010:  40%|███▉      | 1000/2502 [27:36<41:13,  1.65s/it, Loss=3.9506, Top1=46.25%, LR=0.099455]2025-11-04 08:00:46,501 - INFO - Step 26020: {'train_loss_batch': 3.201505661010742, 'train_lr': 0.09945463496051546, 'batch_time': 1.6548998772681176, 'data_time': 0.0045918722848196725}
2025-11-04 08:00:46 Train Epoch 010:  44%|████▍     | 1100/2502 [30:22<38:51,  1.66s/it, Loss=3.9549, Top1=N/A, LR=0.099455]   2025-11-04 08:03:32,048 - INFO - Step 26120: {'train_loss_batch': 3.9731783866882324, 'train_lr': 0.09945463496051546, 'batch_time': 1.6549513214398903, 'data_time': 0.004252514548998979}
2025-11-04 08:03:32 Train Epoch 010:  48%|████▊     | 1200/2502 [33:07<35:35,  1.64s/it, Loss=3.9479, Top1=46.21%, LR=0.099455]2025-11-04 08:06:17,598 - INFO - Step 26220: {'train_loss_batch': 3.2711966037750244, 'train_lr': 0.09945463496051546, 'batch_time': 1.6549967421183083, 'data_time': 0.003965932066295665}
2025-11-04 08:06:17 Train Epoch 010:  52%|█████▏    | 1300/2502 [35:53<33:03,  1.65s/it, Loss=3.9453, Top1=N/A, LR=0.099455]   2025-11-04 08:09:03,021 - INFO - Step 26320: {'train_loss_batch': 4.8752875328063965, 'train_lr': 0.09945463496051546, 'batch_time': 1.654937801133844, 'data_time': 0.003727590185600826}
2025-11-04 08:09:03 Train Epoch 010:  56%|█████▌    | 1400/2502 [38:38<30:24,  1.66s/it, Loss=3.9449, Top1=N/A, LR=0.099455]2025-11-04 08:11:48,897 - INFO - Step 26420: {'train_loss_batch': 4.604994297027588, 'train_lr': 0.09945463496051546, 'batch_time': 1.6552103360493977, 'data_time': 0.00352899778067938}
2025-11-04 08:11:48 Train Epoch 010:  60%|█████▉    | 1500/2502 [41:24<27:32,  1.65s/it, Loss=3.9500, Top1=46.37%, LR=0.099455]2025-11-04 08:14:34,685 - INFO - Step 26520: {'train_loss_batch': 3.103640556335449, 'train_lr': 0.09945463496051546, 'batch_time': 1.6553884885217411, 'data_time': 0.0033506649800096647}
2025-11-04 08:14:34 Train Epoch 010:  64%|██████▍   | 1600/2502 [44:10<24:51,  1.65s/it, Loss=3.9418, Top1=N/A, LR=0.099455]   2025-11-04 08:17:20,471 - INFO - Step 26620: {'train_loss_batch': 3.5125679969787598, 'train_lr': 0.09945463496051546, 'batch_time': 1.6555424079084902, 'data_time': 0.003195857346467418}
2025-11-04 08:17:20 Train Epoch 010:  68%|██████▊   | 1700/2502 [46:55<22:01,  1.65s/it, Loss=3.9336, Top1=46.42%, LR=0.099455]2025-11-04 08:20:05,932 - INFO - Step 26720: {'train_loss_batch': 3.194366931915283, 'train_lr': 0.09945463496051546, 'batch_time': 1.6554874789357397, 'data_time': 0.00306275436136738}
2025-11-04 08:20:05 Train Epoch 010:  72%|███████▏  | 1800/2502 [49:41<19:19,  1.65s/it, Loss=3.9327, Top1=N/A, LR=0.099455]   2025-11-04 08:22:51,190 - INFO - Step 26820: {'train_loss_batch': 3.267518997192383, 'train_lr': 0.09945463496051546, 'batch_time': 1.6553260125695568, 'data_time': 0.002943339181039017}
2025-11-04 08:22:51 Train Epoch 010:  76%|███████▌  | 1900/2502 [52:26<16:42,  1.67s/it, Loss=3.9363, Top1=N/A, LR=0.099455]2025-11-04 08:25:36,723 - INFO - Step 26920: {'train_loss_batch': 4.960683822631836, 'train_lr': 0.09945463496051546, 'batch_time': 1.6553261771696233, 'data_time': 0.002834264006757661}
2025-11-04 08:25:36 Train Epoch 010:  80%|███████▉  | 2000/2502 [55:12<13:48,  1.65s/it, Loss=3.9334, Top1=N/A, LR=0.099455]2025-11-04 08:28:22,783 - INFO - Step 27020: {'train_loss_batch': 3.339566230773926, 'train_lr': 0.09945463496051546, 'batch_time': 1.655589881150619, 'data_time': 0.0027369368618455664}
2025-11-04 08:28:22 Train Epoch 010:  84%|████████▍ | 2100/2502 [57:58<11:05,  1.65s/it, Loss=3.9326, Top1=46.57%, LR=0.099455]2025-11-04 08:31:08,061 - INFO - Step 27120: {'train_loss_batch': 3.4271693229675293, 'train_lr': 0.09945463496051546, 'batch_time': 1.6554557188870167, 'data_time': 0.002648761418817839}
2025-11-04 08:31:08 Train Epoch 010:  88%|████████▊ | 2200/2502 [1:00:43<08:21,  1.66s/it, Loss=3.9257, Top1=N/A, LR=0.099455]   2025-11-04 08:33:53,869 - INFO - Step 27220: {'train_loss_batch': 3.330693244934082, 'train_lr': 0.09945463496051546, 'batch_time': 1.6555746994685823, 'data_time': 0.0025676175281710107}
2025-11-04 08:33:53 Train Epoch 010:  92%|█████████▏| 2300/2502 [1:03:29<05:36,  1.67s/it, Loss=3.9217, Top1=N/A, LR=0.099455]2025-11-04 08:36:39,212 - INFO - Step 27320: {'train_loss_batch': 3.194824457168579, 'train_lr': 0.09945463496051546, 'batch_time': 1.6554815320541734, 'data_time': 0.0024951042065875315}
2025-11-04 08:36:39 Train Epoch 010:  96%|█████████▌| 2400/2502 [1:06:15<02:49,  1.66s/it, Loss=3.9228, Top1=46.55%, LR=0.099455]2025-11-04 08:39:25,080 - INFO - Step 27420: {'train_loss_batch': 3.2067441940307617, 'train_lr': 0.09945463496051546, 'batch_time': 1.6556147430798056, 'data_time': 0.0024283424808004906}
2025-11-04 08:39:25 Train Epoch 010: 100%|█████████▉| 2500/2502 [1:09:00<00:03,  1.66s/it, Loss=3.9209, Top1=46.56%, LR=0.099455]2025-11-04 08:42:10,886 - INFO - Step 27520: {'train_loss_batch': 3.247922420501709, 'train_lr': 0.09945463496051546, 'batch_time': 1.6557127023305669, 'data_time': 0.0023873309906079073}
2025-11-04 08:42:10 Train Epoch 010: 100%|██████████| 2502/2502 [1:09:02<00:00,  1.66s/it, Loss=3.9209, Top1=46.56%, LR=0.099455]
2025-11-04 08:42:13 Val Epoch 010:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 08:42:17   with torch.cuda.amp.autocast():
2025-11-04 08:42:18 Val Epoch 010: 100%|██████████| 98/98 [01:48<00:00,  1.10s/it, Loss=2.9863, Top1=49.94%, Top5=76.25%]
2025-11-04 08:44:01 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 08:44:01   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 08:44:01 2025-11-04 08:44:01,286 - INFO - Step 10: {'epoch': 10, 'learning_rate': 0.09914874011869668, 'train_loss': 3.920660177771327, 'train_top1': 46.5668493960084, 'train_top5': 71.42939206932773, 'train_precision': 45.981188997037684, 'train_recall': 46.4124780813273, 'train_f1': 45.83846530231854, 'val_loss': 2.9863162277221678, 'val_top1': 49.94000000244141, 'val_top5': 76.24800001953125, 'val_precision': 54.63867529086623, 'val_recall': 49.944, 'val_f1': 49.190293295413696}
2025-11-04 08:44:01 2025-11-04 08:44:01,288 - INFO - Epoch 010 Summary - LR: 0.099149, Train Loss: 3.9207, Val Loss: 2.9863, Val F1: 49.19%, Val Precision: 54.64%, Val Recall: 49.94%
2025-11-04 08:44:01 Train Epoch 011:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 08:44:05   with torch.cuda.amp.autocast():
2025-11-04 08:44:06 wandb: WARNING Tried to log to step 10 that is less than the current step 27520. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 08:44:07 Train Epoch 011:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.8454, Top1=N/A, LR=0.099149]2025-11-04 08:44:07,179 - INFO - Step 27522: {'train_loss_batch': 3.8453762531280518, 'train_lr': 0.09914874011869668, 'batch_time': 5.479653835296631, 'data_time': 3.831289052963257}
2025-11-04 08:44:07 Train Epoch 011:   4%|▍         | 100/2502 [02:50<1:06:05,  1.65s/it, Loss=3.8098, Top1=47.72%, LR=0.099149]2025-11-04 08:46:52,287 - INFO - Step 27622: {'train_loss_batch': 3.19986891746521, 'train_lr': 0.09914874011869668, 'batch_time': 1.6889933949649925, 'data_time': 0.03879628559150318}
2025-11-04 08:46:52 Train Epoch 011:   8%|▊         | 200/2502 [05:36<1:02:59,  1.64s/it, Loss=3.8436, Top1=N/A, LR=0.099149]   2025-11-04 08:49:38,011 - INFO - Step 27722: {'train_loss_batch': 3.309788703918457, 'train_lr': 0.09914874011869668, 'batch_time': 1.673195856720654, 'data_time': 0.019910260812560126}
2025-11-04 08:49:38 Train Epoch 011:  12%|█▏        | 300/2502 [08:21<1:00:53,  1.66s/it, Loss=3.8746, Top1=N/A, LR=0.099149]2025-11-04 08:52:23,271 - INFO - Step 27822: {'train_loss_batch': 4.636270523071289, 'train_lr': 0.09914874011869668, 'batch_time': 1.6663518371772132, 'data_time': 0.01356328206996981}
2025-11-04 08:52:23 Train Epoch 011:  16%|█▌        | 400/2502 [11:07<57:55,  1.65s/it, Loss=3.8777, Top1=N/A, LR=0.099149]2025-11-04 08:55:09,387 - INFO - Step 27922: {'train_loss_batch': 3.6443207263946533, 'train_lr': 0.09914874011869668, 'batch_time': 1.6650569647030342, 'data_time': 0.01039673800480336}
2025-11-04 08:55:09 Train Epoch 011:  20%|█▉        | 500/2502 [13:53<55:30,  1.66s/it, Loss=3.8835, Top1=N/A, LR=0.099149]2025-11-04 08:57:54,903 - INFO - Step 28022: {'train_loss_batch': 4.225900650024414, 'train_lr': 0.09914874011869668, 'batch_time': 1.6630822189315826, 'data_time': 0.008493806549650942}
2025-11-04 08:57:54 Train Epoch 011:  24%|██▍       | 600/2502 [16:38<51:54,  1.64s/it, Loss=3.8669, Top1=47.58%, LR=0.099149]2025-11-04 09:00:39,907 - INFO - Step 28122: {'train_loss_batch': 3.2149500846862793, 'train_lr': 0.09914874011869668, 'batch_time': 1.660911476354234, 'data_time': 0.007217544485844312}
2025-11-04 09:00:39 Train Epoch 011:  28%|██▊       | 700/2502 [19:23<49:50,  1.66s/it, Loss=3.8651, Top1=N/A, LR=0.099149]   2025-11-04 09:03:24,806 - INFO - Step 28222: {'train_loss_batch': 3.06561017036438, 'train_lr': 0.09914874011869668, 'batch_time': 1.659210933258122, 'data_time': 0.006318674277987188}
2025-11-04 09:03:24 Train Epoch 011:  32%|███▏      | 800/2502 [22:08<46:42,  1.65s/it, Loss=3.8608, Top1=N/A, LR=0.099149]2025-11-04 09:06:10,348 - INFO - Step 28322: {'train_loss_batch': 3.128326177597046, 'train_lr': 0.09914874011869668, 'batch_time': 1.6587371260634671, 'data_time': 0.0056437755494230845}
2025-11-04 09:06:10 Train Epoch 011:  36%|███▌      | 900/2502 [24:54<44:16,  1.66s/it, Loss=3.8541, Top1=N/A, LR=0.099149]2025-11-04 09:08:56,144 - INFO - Step 28422: {'train_loss_batch': 3.353311061859131, 'train_lr': 0.09914874011869668, 'batch_time': 1.6586507718915018, 'data_time': 0.0051265307986379065}
2025-11-04 09:08:56 Train Epoch 011:  40%|███▉      | 1000/2502 [27:39<41:19,  1.65s/it, Loss=3.8544, Top1=47.53%, LR=0.099149]2025-11-04 09:11:40,700 - INFO - Step 28522: {'train_loss_batch': 3.276909112930298, 'train_lr': 0.09914874011869668, 'batch_time': 1.6573425039544807, 'data_time': 0.0047050289340786165}
2025-11-04 09:11:40 Train Epoch 011:  44%|████▍     | 1100/2502 [30:24<38:35,  1.65s/it, Loss=3.8612, Top1=N/A, LR=0.099149]   2025-11-04 09:14:25,963 - INFO - Step 28622: {'train_loss_batch': 4.252474784851074, 'train_lr': 0.09914874011869668, 'batch_time': 1.656914084743739, 'data_time': 0.0043651100508631845}
2025-11-04 09:14:25 Train Epoch 011:  48%|████▊     | 1200/2502 [33:10<36:02,  1.66s/it, Loss=3.8539, Top1=N/A, LR=0.099149]2025-11-04 09:17:11,950 - INFO - Step 28722: {'train_loss_batch': 4.83730411529541, 'train_lr': 0.09914874011869668, 'batch_time': 1.6571606757936628, 'data_time': 0.0040799108373433925}
2025-11-04 09:17:11 Train Epoch 011:  52%|█████▏    | 1300/2502 [35:55<33:09,  1.66s/it, Loss=3.8472, Top1=N/A, LR=0.099149]2025-11-04 09:19:57,632 - INFO - Step 28822: {'train_loss_batch': 3.709197998046875, 'train_lr': 0.09914874011869668, 'batch_time': 1.657133689025656, 'data_time': 0.0038349390213165166}
2025-11-04 09:19:57 Train Epoch 011:  56%|█████▌    | 1400/2502 [38:40<30:01,  1.63s/it, Loss=3.8478, Top1=N/A, LR=0.099149]2025-11-04 09:22:42,463 - INFO - Step 28922: {'train_loss_batch': 3.0799896717071533, 'train_lr': 0.09914874011869668, 'batch_time': 1.6565042694154422, 'data_time': 0.0036261124239914083}
2025-11-04 09:22:42 Train Epoch 011:  60%|█████▉    | 1500/2502 [41:25<27:47,  1.66s/it, Loss=3.8412, Top1=47.56%, LR=0.099149]2025-11-04 09:25:27,598 - INFO - Step 29022: {'train_loss_batch': 3.134244918823242, 'train_lr': 0.09914874011869668, 'batch_time': 1.6561604904223093, 'data_time': 0.0034450491931262133}
2025-11-04 09:25:27 Train Epoch 011:  64%|██████▍   | 1600/2502 [44:11<24:50,  1.65s/it, Loss=3.8480, Top1=47.59%, LR=0.099149]2025-11-04 09:28:13,250 - INFO - Step 29122: {'train_loss_batch': 3.121922254562378, 'train_lr': 0.09914874011869668, 'batch_time': 1.656182636848321, 'data_time': 0.0032865448641374957}
2025-11-04 09:28:13 Train Epoch 011:  68%|██████▊   | 1700/2502 [46:55<21:52,  1.64s/it, Loss=3.8497, Top1=N/A, LR=0.099149]   2025-11-04 09:30:57,223 - INFO - Step 29222: {'train_loss_batch': 4.77828311920166, 'train_lr': 0.09914874011869668, 'batch_time': 1.6552153286830178, 'data_time': 0.00314250208222817}
2025-11-04 09:30:57 Train Epoch 011:  72%|███████▏  | 1800/2502 [49:41<19:15,  1.65s/it, Loss=3.8560, Top1=N/A, LR=0.099149]2025-11-04 09:33:42,766 - INFO - Step 29322: {'train_loss_batch': 5.213600158691406, 'train_lr': 0.09914874011869668, 'batch_time': 1.6552272383071924, 'data_time': 0.0030162317232579937}
2025-11-04 09:33:42 Train Epoch 011:  76%|███████▌  | 1900/2502 [52:25<16:36,  1.66s/it, Loss=3.8557, Top1=N/A, LR=0.099149]2025-11-04 09:36:27,495 - INFO - Step 29422: {'train_loss_batch': 4.80095100402832, 'train_lr': 0.09914874011869668, 'batch_time': 1.6548096320931376, 'data_time': 0.002905259315494736}
2025-11-04 09:36:27 Train Epoch 011:  80%|███████▉  | 2000/2502 [55:11<13:51,  1.66s/it, Loss=3.8597, Top1=N/A, LR=0.099149]2025-11-04 09:39:13,145 - INFO - Step 29522: {'train_loss_batch': 3.1674587726593018, 'train_lr': 0.09914874011869668, 'batch_time': 1.6548939205181117, 'data_time': 0.0028073182408658343}
2025-11-04 09:39:13 Train Epoch 011:  84%|████████▍ | 2100/2502 [57:56<11:06,  1.66s/it, Loss=3.8592, Top1=N/A, LR=0.099149]2025-11-04 09:41:58,460 - INFO - Step 29622: {'train_loss_batch': 3.2654526233673096, 'train_lr': 0.09914874011869668, 'batch_time': 1.6548107459056043, 'data_time': 0.002718736761130361}
2025-11-04 09:41:58 Train Epoch 011:  88%|████████▊ | 2200/2502 [1:00:42<08:21,  1.66s/it, Loss=3.8583, Top1=47.64%, LR=0.099149]2025-11-04 09:44:44,445 - INFO - Step 29722: {'train_loss_batch': 3.142507553100586, 'train_lr': 0.09914874011869668, 'batch_time': 1.6550398474116588, 'data_time': 0.0026340678517030943}
2025-11-04 09:44:44 Train Epoch 011:  92%|█████████▏| 2300/2502 [1:03:27<05:34,  1.66s/it, Loss=3.8556, Top1=47.65%, LR=0.099149]2025-11-04 09:47:29,601 - INFO - Step 29822: {'train_loss_batch': 3.2059645652770996, 'train_lr': 0.09914874011869668, 'batch_time': 1.6548884109744089, 'data_time': 0.00255955462350062}
2025-11-04 09:47:29 Train Epoch 011:  96%|█████████▌| 2400/2502 [1:06:13<02:48,  1.65s/it, Loss=3.8546, Top1=47.66%, LR=0.099149]2025-11-04 09:50:14,736 - INFO - Step 29922: {'train_loss_batch': 3.1773271560668945, 'train_lr': 0.09914874011869668, 'batch_time': 1.6547410765174824, 'data_time': 0.0024924059799937893}
2025-11-04 09:50:14 Train Epoch 011: 100%|█████████▉| 2500/2502 [1:08:59<00:03,  1.66s/it, Loss=3.8524, Top1=N/A, LR=0.099149]   2025-11-04 09:53:00,817 - INFO - Step 30022: {'train_loss_batch': 3.91642427444458, 'train_lr': 0.09914874011869668, 'batch_time': 1.6549836111659768, 'data_time': 0.0024473564189131476}
2025-11-04 09:53:00 Train Epoch 011: 100%|██████████| 2502/2502 [1:09:00<00:00,  1.66s/it, Loss=3.8524, Top1=N/A, LR=0.099149]
2025-11-04 09:53:03 Val Epoch 011:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 09:53:07   with torch.cuda.amp.autocast():
2025-11-04 09:53:07 Val Epoch 011: 100%|██████████| 98/98 [01:46<00:00,  1.09s/it, Loss=3.1073, Top1=48.71%, Top5=74.22%]
2025-11-04 09:54:49 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 09:54:49   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 09:54:49 2025-11-04 09:54:49,689 - INFO - Step 11: {'epoch': 11, 'learning_rate': 0.09877572083918194, 'train_loss': 3.852497007825868, 'train_top1': 47.69453898514851, 'train_top5': 72.5100556930693, 'train_precision': 47.16179336220644, 'train_recall': 47.57530462818174, 'train_f1': 47.031794366065185, 'val_loss': 3.1072962449645996, 'val_top1': 48.70800000976563, 'val_top5': 74.22399998779296, 'val_precision': 54.76921152735383, 'val_recall': 48.706, 'val_f1': 47.7503962574127}
2025-11-04 09:54:49 2025-11-04 09:54:49,690 - INFO - Epoch 011 Summary - LR: 0.098776, Train Loss: 3.8525, Val Loss: 3.1073, Val F1: 47.75%, Val Precision: 54.77%, Val Recall: 48.71%
2025-11-04 09:54:50 Train Epoch 012:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 09:54:53   with torch.cuda.amp.autocast():
2025-11-04 09:54:55 Train Epoch 012:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.1305, Top1=N/A, LR=0.098776]2025-11-04 09:54:55,264 - INFO - Step 30024: {'train_loss_batch': 3.130495548248291, 'train_lr': 0.09877572083918194, 'batch_time': 5.163486003875732, 'data_time': 3.513460636138916}
2025-11-04 09:54:55 Train Epoch 012:   0%|          | 1/2502 [00:05<3:35:21,  5.17s/it, Loss=3.1305, Top1=N/A, LR=0.098776]wandb: WARNING Tried to log to step 11 that is less than the current step 30022. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 09:54:56 Train Epoch 012:   4%|▍         | 100/2502 [02:51<1:06:29,  1.66s/it, Loss=3.8719, Top1=48.49%, LR=0.098776]2025-11-04 09:57:41,357 - INFO - Step 30124: {'train_loss_batch': 3.2130398750305176, 'train_lr': 0.09877572083918194, 'batch_time': 1.6956127893806685, 'data_time': 0.03568144599990089}
2025-11-04 09:57:41 Train Epoch 012:   8%|▊         | 200/2502 [05:37<1:03:43,  1.66s/it, Loss=3.8379, Top1=N/A, LR=0.098776]   2025-11-04 10:00:27,266 - INFO - Step 30224: {'train_loss_batch': 3.0293047428131104, 'train_lr': 0.09877572083918194, 'batch_time': 1.6774419824875408, 'data_time': 0.018385309485060657}
2025-11-04 10:00:27 Train Epoch 012:  12%|█▏        | 300/2502 [08:22<1:00:43,  1.65s/it, Loss=3.8426, Top1=N/A, LR=0.098776]2025-11-04 10:03:12,826 - INFO - Step 30324: {'train_loss_batch': 3.40693736076355, 'train_lr': 0.09877572083918194, 'batch_time': 1.670186682792993, 'data_time': 0.012562339884102146}
2025-11-04 10:03:12 Train Epoch 012:  16%|█▌        | 400/2502 [11:08<57:50,  1.65s/it, Loss=3.8584, Top1=N/A, LR=0.098776]2025-11-04 10:05:58,276 - INFO - Step 30424: {'train_loss_batch': 4.242556571960449, 'train_lr': 0.09877572083918194, 'batch_time': 1.6662743656415298, 'data_time': 0.009675571151505087}
2025-11-04 10:05:58 Train Epoch 012:  20%|█▉        | 500/2502 [13:53<55:09,  1.65s/it, Loss=3.8568, Top1=N/A, LR=0.098776]2025-11-04 10:08:44,008 - INFO - Step 30524: {'train_loss_batch': 3.8350837230682373, 'train_lr': 0.09877572083918194, 'batch_time': 1.6644855730547876, 'data_time': 0.007929904732161654}
2025-11-04 10:08:44 Train Epoch 012:  24%|██▍       | 600/2502 [16:39<52:39,  1.66s/it, Loss=3.8534, Top1=N/A, LR=0.098776]2025-11-04 10:11:29,609 - INFO - Step 30624: {'train_loss_batch': 3.4271156787872314, 'train_lr': 0.09877572083918194, 'batch_time': 1.6630760798240065, 'data_time': 0.0067775705689796795}
2025-11-04 10:11:29 Train Epoch 012:  28%|██▊       | 700/2502 [19:24<49:47,  1.66s/it, Loss=3.8559, Top1=N/A, LR=0.098776]2025-11-04 10:14:14,641 - INFO - Step 30724: {'train_loss_batch': 4.042845726013184, 'train_lr': 0.09877572083918194, 'batch_time': 1.6612554960346086, 'data_time': 0.005946657627013203}
2025-11-04 10:14:14 Train Epoch 012:  32%|███▏      | 800/2502 [22:10<46:40,  1.65s/it, Loss=3.8587, Top1=N/A, LR=0.098776]2025-11-04 10:17:00,415 - INFO - Step 30824: {'train_loss_batch': 3.0038881301879883, 'train_lr': 0.09877572083918194, 'batch_time': 1.660816992713271, 'data_time': 0.005328530527083913}
2025-11-04 10:17:00 Train Epoch 012:  36%|███▌      | 900/2502 [24:55<44:18,  1.66s/it, Loss=3.8718, Top1=N/A, LR=0.098776]2025-11-04 10:19:46,054 - INFO - Step 30924: {'train_loss_batch': 3.8979246616363525, 'train_lr': 0.09877572083918194, 'batch_time': 1.6603243885506007, 'data_time': 0.004838492047376558}
2025-11-04 10:19:46 Train Epoch 012:  40%|███▉      | 1000/2502 [27:40<41:15,  1.65s/it, Loss=3.8672, Top1=N/A, LR=0.098776]2025-11-04 10:22:30,526 - INFO - Step 31024: {'train_loss_batch': 3.402320384979248, 'train_lr': 0.09877572083918194, 'batch_time': 1.658765421999799, 'data_time': 0.004445062412486805}
2025-11-04 10:22:30 Train Epoch 012:  44%|████▍     | 1100/2502 [30:26<38:43,  1.66s/it, Loss=3.8587, Top1=N/A, LR=0.098776]2025-11-04 10:25:16,450 - INFO - Step 31124: {'train_loss_batch': 4.089049339294434, 'train_lr': 0.09877572083918194, 'batch_time': 1.6588087517169257, 'data_time': 0.004131339659591245}
2025-11-04 10:25:16 Train Epoch 012:  48%|████▊     | 1200/2502 [33:12<35:59,  1.66s/it, Loss=3.8634, Top1=N/A, LR=0.098776]2025-11-04 10:28:02,188 - INFO - Step 31224: {'train_loss_batch': 3.2187914848327637, 'train_lr': 0.09877572083918194, 'batch_time': 1.6586890194834916, 'data_time': 0.0038653055297445}
2025-11-04 10:28:02 Train Epoch 012:  52%|█████▏    | 1300/2502 [35:58<33:22,  1.67s/it, Loss=3.8633, Top1=48.62%, LR=0.098776]2025-11-04 10:30:48,118 - INFO - Step 31324: {'train_loss_batch': 3.056922435760498, 'train_lr': 0.09877572083918194, 'batch_time': 1.6587360352392293, 'data_time': 0.003642056008470141}
2025-11-04 10:30:48 Train Epoch 012:  56%|█████▌    | 1400/2502 [38:44<30:28,  1.66s/it, Loss=3.8635, Top1=N/A, LR=0.098776]   2025-11-04 10:33:34,226 - INFO - Step 31424: {'train_loss_batch': 3.184157609939575, 'train_lr': 0.09877572083918194, 'batch_time': 1.6589037230149242, 'data_time': 0.0034504991867642673}
2025-11-04 10:33:34 Train Epoch 012:  60%|█████▉    | 1500/2502 [41:30<27:46,  1.66s/it, Loss=3.8570, Top1=N/A, LR=0.098776]2025-11-04 10:36:20,390 - INFO - Step 31524: {'train_loss_batch': 4.162439823150635, 'train_lr': 0.09877572083918194, 'batch_time': 1.659085510255177, 'data_time': 0.003278317569018522}
2025-11-04 10:36:20 Train Epoch 012:  64%|██████▍   | 1600/2502 [44:16<24:58,  1.66s/it, Loss=3.8465, Top1=N/A, LR=0.098776]2025-11-04 10:39:06,551 - INFO - Step 31624: {'train_loss_batch': 4.839151382446289, 'train_lr': 0.09877572083918194, 'batch_time': 1.6592430380416765, 'data_time': 0.0031266160341890657}
2025-11-04 10:39:06 Train Epoch 012:  68%|██████▊   | 1700/2502 [47:02<22:14,  1.66s/it, Loss=3.8412, Top1=N/A, LR=0.098776]2025-11-04 10:41:52,642 - INFO - Step 31724: {'train_loss_batch': 3.291025400161743, 'train_lr': 0.09877572083918194, 'batch_time': 1.6593409005086048, 'data_time': 0.002995556203986812}
2025-11-04 10:41:52 Train Epoch 012:  72%|███████▏  | 1800/2502 [49:48<19:24,  1.66s/it, Loss=3.8353, Top1=N/A, LR=0.098776]2025-11-04 10:44:38,539 - INFO - Step 31824: {'train_loss_batch': 3.3179755210876465, 'train_lr': 0.09877572083918194, 'batch_time': 1.659320242467157, 'data_time': 0.002878222844125429}
2025-11-04 10:44:38 Train Epoch 012:  76%|███████▌  | 1900/2502 [52:34<16:38,  1.66s/it, Loss=3.8348, Top1=N/A, LR=0.098776]2025-11-04 10:47:24,523 - INFO - Step 31924: {'train_loss_batch': 3.11450457572937, 'train_lr': 0.09877572083918194, 'batch_time': 1.6593474792468428, 'data_time': 0.002773512921039836}
2025-11-04 10:47:24 Train Epoch 012:  80%|███████▉  | 2000/2502 [55:20<13:53,  1.66s/it, Loss=3.8325, Top1=N/A, LR=0.098776]2025-11-04 10:50:10,508 - INFO - Step 32024: {'train_loss_batch': 4.464118480682373, 'train_lr': 0.09877572083918194, 'batch_time': 1.6593724576787077, 'data_time': 0.0026797160454120473}
2025-11-04 10:50:10 Train Epoch 012:  84%|████████▍ | 2100/2502 [58:05<11:03,  1.65s/it, Loss=3.8291, Top1=N/A, LR=0.098776]2025-11-04 10:52:55,964 - INFO - Step 32124: {'train_loss_batch': 4.7121429443359375, 'train_lr': 0.09877572083918194, 'batch_time': 1.6591432060530389, 'data_time': 0.0025968854623655657}
2025-11-04 10:52:55 Train Epoch 012:  88%|████████▊ | 2200/2502 [1:00:51<08:21,  1.66s/it, Loss=3.8318, Top1=N/A, LR=0.098776]2025-11-04 10:55:41,676 - INFO - Step 32224: {'train_loss_batch': 3.591301679611206, 'train_lr': 0.09877572083918194, 'batch_time': 1.6590511728448794, 'data_time': 0.002523254232480276}
2025-11-04 10:55:41 Train Epoch 012:  92%|█████████▏| 2300/2502 [1:03:37<05:34,  1.66s/it, Loss=3.8337, Top1=N/A, LR=0.098776]2025-11-04 10:58:27,805 - INFO - Step 32324: {'train_loss_batch': 4.098012924194336, 'train_lr': 0.09877572083918194, 'batch_time': 1.659148670600218, 'data_time': 0.002453419500099789}
2025-11-04 10:58:27 Train Epoch 012:  96%|█████████▌| 2400/2502 [1:06:23<02:48,  1.65s/it, Loss=3.8344, Top1=N/A, LR=0.098776]2025-11-04 11:01:13,520 - INFO - Step 32424: {'train_loss_batch': 4.754216194152832, 'train_lr': 0.09877572083918194, 'batch_time': 1.6590653595850895, 'data_time': 0.0023932938573758633}
2025-11-04 11:01:13 Train Epoch 012: 100%|█████████▉| 2500/2502 [1:09:09<00:03,  1.67s/it, Loss=3.8346, Top1=48.70%, LR=0.098776]2025-11-04 11:03:59,425 - INFO - Step 32524: {'train_loss_batch': 3.1992740631103516, 'train_lr': 0.09877572083918194, 'batch_time': 1.6590646032045862, 'data_time': 0.0023566316195079585}
2025-11-04 11:03:59 Train Epoch 012: 100%|██████████| 2502/2502 [1:09:11<00:00,  1.66s/it, Loss=3.8346, Top1=48.70%, LR=0.098776]
2025-11-04 11:04:01 Val Epoch 012:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 11:04:06   with torch.cuda.amp.autocast():
2025-11-04 11:04:06 Val Epoch 012: 100%|██████████| 98/98 [01:53<00:00,  1.15s/it, Loss=2.9176, Top1=51.28%, Top5=77.15%]
2025-11-04 11:05:54 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 11:05:54   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 11:05:54 2025-11-04 11:05:54,969 - INFO - Step 12: {'epoch': 12, 'learning_rate': 0.09833608662143803, 'train_loss': 3.834409198410315, 'train_top1': 48.69723708246347, 'train_top5': 73.29070981210856, 'train_precision': 48.17867654806447, 'train_recall': 48.55353549237481, 'train_f1': 48.04894730420416, 'val_loss': 2.9175857108306884, 'val_top1': 51.28199998535156, 'val_top5': 77.15000001464844, 'val_precision': 56.550127205164515, 'val_recall': 51.282, 'val_f1': 50.48779782763426}
2025-11-04 11:05:54 2025-11-04 11:05:54,971 - INFO - Epoch 012 Summary - LR: 0.098336, Train Loss: 3.8344, Val Loss: 2.9176, Val F1: 50.49%, Val Precision: 56.55%, Val Recall: 51.28%
2025-11-04 11:05:56 wandb: WARNING Tried to log to step 12 that is less than the current step 32524. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 11:05:58 2025-11-04 11:05:58,475 - INFO - New best model saved with validation accuracy: 51.282%
2025-11-04 11:05:58 2025-11-04 11:05:58,475 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_013.pth
2025-11-04 11:05:58 Train Epoch 013:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 11:06:02   with torch.cuda.amp.autocast():
2025-11-04 11:06:04 Train Epoch 013:   0%|          | 0/2502 [00:06<?, ?it/s, Loss=3.1376, Top1=49.61%, LR=0.098336]2025-11-04 11:06:04,496 - INFO - Step 32526: {'train_loss_batch': 3.1376328468322754, 'train_lr': 0.09833608662143803, 'batch_time': 6.017683029174805, 'data_time': 4.4017298221588135}
2025-11-04 11:06:04 Train Epoch 013:   4%|▍         | 100/2502 [02:50<1:06:19,  1.66s/it, Loss=3.8307, Top1=N/A, LR=0.098336]   2025-11-04 11:08:49,061 - INFO - Step 32626: {'train_loss_batch': 3.100281000137329, 'train_lr': 0.09833608662143803, 'batch_time': 1.6889442240837778, 'data_time': 0.044547687662710056}
2025-11-04 11:08:49 Train Epoch 013:   8%|▊         | 200/2502 [05:35<1:02:59,  1.64s/it, Loss=3.7730, Top1=N/A, LR=0.098336]2025-11-04 11:11:33,499 - INFO - Step 32726: {'train_loss_batch': 3.3709921836853027, 'train_lr': 0.09833608662143803, 'batch_time': 1.6667747106125106, 'data_time': 0.02283867081599449}
2025-11-04 11:11:33 Train Epoch 013:  12%|█▏        | 300/2502 [08:20<1:00:40,  1.65s/it, Loss=3.7815, Top1=N/A, LR=0.098336]2025-11-04 11:14:19,132 - INFO - Step 32826: {'train_loss_batch': 3.632627248764038, 'train_lr': 0.09833608662143803, 'batch_time': 1.6633013490822623, 'data_time': 0.015549478340782597}
2025-11-04 11:14:19 Train Epoch 013:  16%|█▌        | 400/2502 [11:05<57:50,  1.65s/it, Loss=3.7832, Top1=N/A, LR=0.098336]2025-11-04 11:17:03,509 - INFO - Step 32926: {'train_loss_batch': 3.0049469470977783, 'train_lr': 0.09833608662143803, 'batch_time': 1.6584321727181908, 'data_time': 0.011900745425141065}
2025-11-04 11:17:03 Train Epoch 013:  20%|█▉        | 500/2502 [13:50<55:12,  1.65s/it, Loss=3.7623, Top1=N/A, LR=0.098336]2025-11-04 11:19:48,628 - INFO - Step 33026: {'train_loss_batch': 3.2088277339935303, 'train_lr': 0.09833608662143803, 'batch_time': 1.6569863208039792, 'data_time': 0.009719308027012381}
2025-11-04 11:19:48 Train Epoch 013:  24%|██▍       | 600/2502 [16:35<52:17,  1.65s/it, Loss=3.7575, Top1=49.59%, LR=0.098336]2025-11-04 11:22:34,375 - INFO - Step 33126: {'train_loss_batch': 3.0274620056152344, 'train_lr': 0.09833608662143803, 'batch_time': 1.6570653022823238, 'data_time': 0.008251593632626652}
2025-11-04 11:22:34 Train Epoch 013:  28%|██▊       | 700/2502 [19:21<49:47,  1.66s/it, Loss=3.7590, Top1=N/A, LR=0.098336]   2025-11-04 11:25:19,918 - INFO - Step 33226: {'train_loss_batch': 2.983987331390381, 'train_lr': 0.09833608662143803, 'batch_time': 1.6568328987344696, 'data_time': 0.007201540657184944}
2025-11-04 11:25:19 Train Epoch 013:  32%|███▏      | 800/2502 [22:06<46:49,  1.65s/it, Loss=3.7561, Top1=N/A, LR=0.098336]2025-11-04 11:28:05,128 - INFO - Step 33326: {'train_loss_batch': 4.4121994972229, 'train_lr': 0.09833608662143803, 'batch_time': 1.656240901101692, 'data_time': 0.006424332974704166}
2025-11-04 11:28:05 Train Epoch 013:  36%|███▌      | 900/2502 [24:52<44:14,  1.66s/it, Loss=3.7421, Top1=N/A, LR=0.098336]2025-11-04 11:30:50,953 - INFO - Step 33426: {'train_loss_batch': 3.2202255725860596, 'train_lr': 0.09833608662143803, 'batch_time': 1.6564645957735085, 'data_time': 0.00581814313966876}
2025-11-04 11:30:50 Train Epoch 013:  40%|███▉      | 1000/2502 [27:38<41:22,  1.65s/it, Loss=3.7464, Top1=N/A, LR=0.098336]2025-11-04 11:33:36,660 - INFO - Step 33526: {'train_loss_batch': 3.101196765899658, 'train_lr': 0.09833608662143803, 'batch_time': 1.6565247730060773, 'data_time': 0.005335104691755998}
2025-11-04 11:33:36 Train Epoch 013:  44%|████▍     | 1100/2502 [30:23<38:45,  1.66s/it, Loss=3.7469, Top1=49.49%, LR=0.098336]2025-11-04 11:36:21,842 - INFO - Step 33626: {'train_loss_batch': 3.0019168853759766, 'train_lr': 0.09833608662143803, 'batch_time': 1.6560967906186193, 'data_time': 0.0049325378237801394}
2025-11-04 11:36:21 Train Epoch 013:  48%|████▊     | 1200/2502 [33:08<35:38,  1.64s/it, Loss=3.7525, Top1=N/A, LR=0.098336]   2025-11-04 11:39:07,241 - INFO - Step 33726: {'train_loss_batch': 3.3052453994750977, 'train_lr': 0.09833608662143803, 'batch_time': 1.6559211215210596, 'data_time': 0.004598636015765772}
2025-11-04 11:39:07 Train Epoch 013:  52%|█████▏    | 1300/2502 [35:53<33:13,  1.66s/it, Loss=3.7436, Top1=N/A, LR=0.098336]2025-11-04 11:41:51,945 - INFO - Step 33826: {'train_loss_batch': 4.56141471862793, 'train_lr': 0.09833608662143803, 'batch_time': 1.6552383434213556, 'data_time': 0.004307800765041202}
2025-11-04 11:41:51 Train Epoch 013:  56%|█████▌    | 1400/2502 [38:38<30:24,  1.66s/it, Loss=3.7449, Top1=N/A, LR=0.098336]2025-11-04 11:44:37,076 - INFO - Step 33926: {'train_loss_batch': 4.870056629180908, 'train_lr': 0.09833608662143803, 'batch_time': 1.6549581225474845, 'data_time': 0.00406291738397134}
2025-11-04 11:44:37 Train Epoch 013:  60%|█████▉    | 1500/2502 [41:24<27:42,  1.66s/it, Loss=3.7429, Top1=49.44%, LR=0.098336]2025-11-04 11:47:22,490 - INFO - Step 34026: {'train_loss_batch': 2.995647668838501, 'train_lr': 0.09833608662143803, 'batch_time': 1.6549031537505168, 'data_time': 0.0038522952878419594}
2025-11-04 11:47:22 Train Epoch 013:  64%|██████▍   | 1600/2502 [44:10<24:55,  1.66s/it, Loss=3.7434, Top1=N/A, LR=0.098336]   2025-11-04 11:50:08,607 - INFO - Step 34126: {'train_loss_batch': 3.6167349815368652, 'train_lr': 0.09833608662143803, 'batch_time': 1.65529463128847, 'data_time': 0.0036697363868346444}
2025-11-04 11:50:08 Train Epoch 013:  68%|██████▊   | 1700/2502 [46:55<22:01,  1.65s/it, Loss=3.7418, Top1=N/A, LR=0.098336]2025-11-04 11:52:54,266 - INFO - Step 34226: {'train_loss_batch': 3.373507499694824, 'train_lr': 0.09833608662143803, 'batch_time': 1.6553704905411835, 'data_time': 0.003509996919895466}
2025-11-04 11:52:54 Train Epoch 013:  72%|███████▏  | 1800/2502 [49:40<19:08,  1.64s/it, Loss=3.7452, Top1=N/A, LR=0.098336]2025-11-04 11:55:38,795 - INFO - Step 34326: {'train_loss_batch': 4.192164421081543, 'train_lr': 0.09833608662143803, 'batch_time': 1.6548105878739936, 'data_time': 0.003365994691186849}
2025-11-04 11:55:38 Train Epoch 013:  76%|███████▌  | 1900/2502 [52:25<16:40,  1.66s/it, Loss=3.7442, Top1=N/A, LR=0.098336]2025-11-04 11:58:24,187 - INFO - Step 34426: {'train_loss_batch': 3.2291812896728516, 'train_lr': 0.09833608662143803, 'batch_time': 1.6547634979099552, 'data_time': 0.003235797766696021}
2025-11-04 11:58:24 Train Epoch 013:  80%|███████▉  | 2000/2502 [55:11<13:51,  1.66s/it, Loss=3.7457, Top1=N/A, LR=0.098336]2025-11-04 12:01:10,233 - INFO - Step 34526: {'train_loss_batch': 3.8310651779174805, 'train_lr': 0.09833608662143803, 'batch_time': 1.6550482345306534, 'data_time': 0.003122280622231609}
2025-11-04 12:01:10 Train Epoch 013:  84%|████████▍ | 2100/2502 [57:57<11:03,  1.65s/it, Loss=3.7501, Top1=49.42%, LR=0.098336]2025-11-04 12:03:56,043 - INFO - Step 34626: {'train_loss_batch': 3.0800933837890625, 'train_lr': 0.09833608662143803, 'batch_time': 1.6551934090868057, 'data_time': 0.003018624665679278}
2025-11-04 12:03:56 Train Epoch 013:  88%|████████▊ | 2200/2502 [1:00:43<08:20,  1.66s/it, Loss=3.7516, Top1=N/A, LR=0.098336]   2025-11-04 12:06:41,816 - INFO - Step 34726: {'train_loss_batch': 4.864230155944824, 'train_lr': 0.09833608662143803, 'batch_time': 1.65530889611632, 'data_time': 0.002922491396843764}
2025-11-04 12:06:41 Train Epoch 013:  92%|█████████▏| 2300/2502 [1:03:29<05:33,  1.65s/it, Loss=3.7517, Top1=N/A, LR=0.098336]2025-11-04 12:09:27,644 - INFO - Step 34826: {'train_loss_batch': 3.026564836502075, 'train_lr': 0.09833608662143803, 'batch_time': 1.6554376449029584, 'data_time': 0.002834986417721479}
2025-11-04 12:09:27 Train Epoch 013:  96%|█████████▌| 2400/2502 [1:06:14<02:47,  1.64s/it, Loss=3.7510, Top1=49.52%, LR=0.098336]2025-11-04 12:12:12,570 - INFO - Step 34926: {'train_loss_batch': 3.010697364807129, 'train_lr': 0.09833608662143803, 'batch_time': 1.6551804048029, 'data_time': 0.0027549199291389717}
2025-11-04 12:12:12 Train Epoch 013: 100%|█████████▉| 2500/2502 [1:08:59<00:03,  1.64s/it, Loss=3.7521, Top1=N/A, LR=0.098336]   2025-11-04 12:14:57,502 - INFO - Step 35026: {'train_loss_batch': 3.6821632385253906, 'train_lr': 0.09833608662143803, 'batch_time': 1.6549458959397199, 'data_time': 0.002711420962925865}
2025-11-04 12:14:57 Train Epoch 013: 100%|██████████| 2502/2502 [1:09:00<00:00,  1.65s/it, Loss=3.7521, Top1=N/A, LR=0.098336]
2025-11-04 12:14:59 Val Epoch 013:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 12:15:04   with torch.cuda.amp.autocast():
2025-11-04 12:15:04 Val Epoch 013: 100%|██████████| 98/98 [01:54<00:00,  1.17s/it, Loss=2.9351, Top1=51.20%, Top5=77.00%]
2025-11-04 12:16:53 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 12:16:53   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 12:16:53 2025-11-04 12:16:53,981 - INFO - Step 13: {'epoch': 13, 'learning_rate': 0.09783043795292455, 'train_loss': 3.752770261798831, 'train_top1': 49.529733009708735, 'train_top5': 74.03216019417475, 'train_precision': 49.02976278520469, 'train_recall': 49.380079676848126, 'train_f1': 48.897688620196256, 'val_loss': 2.935108653411865, 'val_top1': 51.19599999267578, 'val_top5': 76.99999999755859, 'val_precision': 56.96647112730705, 'val_recall': 51.188, 'val_f1': 50.586226791385656}
2025-11-04 12:16:53 2025-11-04 12:16:53,983 - INFO - Epoch 013 Summary - LR: 0.097830, Train Loss: 3.7528, Val Loss: 2.9351, Val F1: 50.59%, Val Precision: 56.97%, Val Recall: 51.19%
2025-11-04 12:16:54 Train Epoch 014:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 13 that is less than the current step 35026. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 12:16:58 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 12:16:58   with torch.cuda.amp.autocast():
2025-11-04 12:17:00 Train Epoch 014:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.0400, Top1=N/A, LR=0.097830]2025-11-04 12:17:00,013 - INFO - Step 35028: {'train_loss_batch': 3.0400149822235107, 'train_lr': 0.09783043795292455, 'batch_time': 5.611494302749634, 'data_time': 3.9515750408172607}
2025-11-04 12:17:00 Train Epoch 014:   4%|▍         | 100/2502 [02:51<1:06:28,  1.66s/it, Loss=3.7144, Top1=N/A, LR=0.097830]2025-11-04 12:19:45,857 - INFO - Step 35128: {'train_loss_batch': 4.494251728057861, 'train_lr': 0.09783043795292455, 'batch_time': 1.6975844732605585, 'data_time': 0.040108312474619046}
2025-11-04 12:19:45 Train Epoch 014:   8%|▊         | 200/2502 [05:37<1:02:57,  1.64s/it, Loss=3.7279, Top1=49.94%, LR=0.097830]2025-11-04 12:22:31,745 - INFO - Step 35228: {'train_loss_batch': 3.110328435897827, 'train_lr': 0.09783043795292455, 'batch_time': 1.678327578217236, 'data_time': 0.02064102917761352}
2025-11-04 12:22:31 Train Epoch 014:  12%|█▏        | 300/2502 [08:22<1:00:56,  1.66s/it, Loss=3.7535, Top1=N/A, LR=0.097830]   2025-11-04 12:25:16,901 - INFO - Step 35328: {'train_loss_batch': 3.0480782985687256, 'train_lr': 0.09783043795292455, 'batch_time': 1.6694331089919587, 'data_time': 0.014103019752375706}
2025-11-04 12:25:16 Train Epoch 014:  16%|█▌        | 400/2502 [11:08<58:05,  1.66s/it, Loss=3.7512, Top1=N/A, LR=0.097830]2025-11-04 12:28:02,924 - INFO - Step 35428: {'train_loss_batch': 4.661631107330322, 'train_lr': 0.09783043795292455, 'batch_time': 1.6671383261977883, 'data_time': 0.010823639253725733}
2025-11-04 12:28:02 Train Epoch 014:  20%|█▉        | 500/2502 [13:53<55:09,  1.65s/it, Loss=3.7288, Top1=N/A, LR=0.097830]2025-11-04 12:30:47,896 - INFO - Step 35528: {'train_loss_batch': 5.407557964324951, 'train_lr': 0.09783043795292455, 'batch_time': 1.663661550856874, 'data_time': 0.008854534335716993}
2025-11-04 12:30:47 Train Epoch 014:  24%|██▍       | 600/2502 [16:39<52:42,  1.66s/it, Loss=3.7282, Top1=50.38%, LR=0.097830]2025-11-04 12:33:33,818 - INFO - Step 35628: {'train_loss_batch': 2.935904026031494, 'train_lr': 0.09783043795292455, 'batch_time': 1.6629207162016044, 'data_time': 0.007552110811636571}
2025-11-04 12:33:33 Train Epoch 014:  28%|██▊       | 700/2502 [19:25<49:48,  1.66s/it, Loss=3.7376, Top1=50.46%, LR=0.097830]2025-11-04 12:36:19,767 - INFO - Step 35728: {'train_loss_batch': 3.091958522796631, 'train_lr': 0.09783043795292455, 'batch_time': 1.6624308105202101, 'data_time': 0.006614134417110775}
2025-11-04 12:36:19 Train Epoch 014:  32%|███▏      | 800/2502 [22:09<46:20,  1.63s/it, Loss=3.7262, Top1=N/A, LR=0.097830]   2025-11-04 12:39:04,400 - INFO - Step 35828: {'train_loss_batch': 3.6663904190063477, 'train_lr': 0.09783043795292455, 'batch_time': 1.660421326812287, 'data_time': 0.005906267856688387}
2025-11-04 12:39:04 Train Epoch 014:  36%|███▌      | 900/2502 [24:55<44:33,  1.67s/it, Loss=3.7261, Top1=N/A, LR=0.097830]2025-11-04 12:41:49,567 - INFO - Step 35928: {'train_loss_batch': 5.206944465637207, 'train_lr': 0.09783043795292455, 'batch_time': 1.6594498514732166, 'data_time': 0.005355116529814014}
2025-11-04 12:41:49 Train Epoch 014:  40%|███▉      | 1000/2502 [27:40<41:02,  1.64s/it, Loss=3.7235, Top1=N/A, LR=0.097830]2025-11-04 12:44:34,508 - INFO - Step 36028: {'train_loss_batch': 4.918100357055664, 'train_lr': 0.09783043795292455, 'batch_time': 1.6584464308027025, 'data_time': 0.004920502642651538}
2025-11-04 12:44:34 Train Epoch 014:  44%|████▍     | 1100/2502 [30:24<38:46,  1.66s/it, Loss=3.7191, Top1=50.44%, LR=0.097830]2025-11-04 12:47:19,347 - INFO - Step 36128: {'train_loss_batch': 3.0336413383483887, 'train_lr': 0.09783043795292455, 'batch_time': 1.6575322640580983, 'data_time': 0.004625415931930334}
2025-11-04 12:47:19 Train Epoch 014:  48%|████▊     | 1200/2502 [33:10<36:00,  1.66s/it, Loss=3.7244, Top1=N/A, LR=0.097830]   2025-11-04 12:50:04,686 - INFO - Step 36228: {'train_loss_batch': 3.4955062866210938, 'train_lr': 0.09783043795292455, 'batch_time': 1.657188111995281, 'data_time': 0.004319185222019065}
2025-11-04 12:50:04 Train Epoch 014:  52%|█████▏    | 1300/2502 [35:55<32:50,  1.64s/it, Loss=3.7185, Top1=50.37%, LR=0.097830]2025-11-04 12:52:49,753 - INFO - Step 36328: {'train_loss_batch': 3.044814109802246, 'train_lr': 0.09783043795292455, 'batch_time': 1.6566865733730893, 'data_time': 0.00405671942884605}
2025-11-04 12:52:49 Train Epoch 014:  56%|█████▌    | 1400/2502 [38:40<30:48,  1.68s/it, Loss=3.7182, Top1=N/A, LR=0.097830]   2025-11-04 12:55:35,102 - INFO - Step 36428: {'train_loss_batch': 3.243190288543701, 'train_lr': 0.09783043795292455, 'batch_time': 1.6564582764123865, 'data_time': 0.0039720184712815}
2025-11-04 12:55:35 Train Epoch 014:  60%|█████▉    | 1500/2502 [41:25<27:33,  1.65s/it, Loss=3.7236, Top1=N/A, LR=0.097830]2025-11-04 12:58:19,674 - INFO - Step 36528: {'train_loss_batch': 3.6589531898498535, 'train_lr': 0.09783043795292455, 'batch_time': 1.6557427753534895, 'data_time': 0.0037668188121777863}
2025-11-04 12:58:19 Train Epoch 014:  64%|██████▍   | 1600/2502 [44:10<24:58,  1.66s/it, Loss=3.7306, Top1=N/A, LR=0.097830]2025-11-04 13:01:05,305 - INFO - Step 36628: {'train_loss_batch': 2.980438470840454, 'train_lr': 0.09783043795292455, 'batch_time': 1.6557777161750102, 'data_time': 0.00362399635577038}
2025-11-04 13:01:05 Train Epoch 014:  68%|██████▊   | 1700/2502 [46:57<22:11,  1.66s/it, Loss=3.7273, Top1=50.41%, LR=0.097830]2025-11-04 13:03:51,462 - INFO - Step 36728: {'train_loss_batch': 2.906099796295166, 'train_lr': 0.09783043795292455, 'batch_time': 1.6561181192044858, 'data_time': 0.0035010087777417524}
2025-11-04 13:03:51 Train Epoch 014:  72%|███████▏  | 1800/2502 [49:42<19:27,  1.66s/it, Loss=3.7260, Top1=N/A, LR=0.097830]   2025-11-04 13:06:36,771 - INFO - Step 36828: {'train_loss_batch': 3.024693727493286, 'train_lr': 0.09783043795292455, 'batch_time': 1.655949942341518, 'data_time': 0.0035721626895987148}
2025-11-04 13:06:36 Train Epoch 014:  76%|███████▌  | 1900/2502 [52:27<16:34,  1.65s/it, Loss=3.7275, Top1=N/A, LR=0.097830]2025-11-04 13:09:22,112 - INFO - Step 36928: {'train_loss_batch': 3.003122568130493, 'train_lr': 0.09783043795292455, 'batch_time': 1.655816418318169, 'data_time': 0.003566044998570281}
2025-11-04 13:09:22 Train Epoch 014:  80%|███████▉  | 2000/2502 [55:14<13:54,  1.66s/it, Loss=3.7234, Top1=N/A, LR=0.097830]2025-11-04 13:12:08,647 - INFO - Step 37028: {'train_loss_batch': 4.698286056518555, 'train_lr': 0.09783043795292455, 'batch_time': 1.6562926404658465, 'data_time': 0.003897567679440004}
2025-11-04 13:12:08 Train Epoch 014:  84%|████████▍ | 2100/2502 [57:59<11:05,  1.66s/it, Loss=3.7273, Top1=N/A, LR=0.097830]2025-11-04 13:14:54,346 - INFO - Step 37128: {'train_loss_batch': 4.894972801208496, 'train_lr': 0.09783043795292455, 'batch_time': 1.6563259958369796, 'data_time': 0.004073716980227398}
2025-11-04 13:14:54 Train Epoch 014:  88%|████████▊ | 2200/2502 [1:00:45<08:18,  1.65s/it, Loss=3.7240, Top1=N/A, LR=0.097830]2025-11-04 13:17:39,695 - INFO - Step 37228: {'train_loss_batch': 4.022695064544678, 'train_lr': 0.09783043795292455, 'batch_time': 1.6561969655689897, 'data_time': 0.0039334845293765176}
2025-11-04 13:17:39 Train Epoch 014:  92%|█████████▏| 2300/2502 [1:03:29<05:29,  1.63s/it, Loss=3.7232, Top1=N/A, LR=0.097830]2025-11-04 13:20:24,257 - INFO - Step 37328: {'train_loss_batch': 4.760154724121094, 'train_lr': 0.09783043795292455, 'batch_time': 1.6557370153316877, 'data_time': 0.003942220949805441}
2025-11-04 13:20:24 Train Epoch 014:  96%|█████████▌| 2400/2502 [1:06:15<02:48,  1.65s/it, Loss=3.7253, Top1=N/A, LR=0.097830]2025-11-04 13:23:09,852 - INFO - Step 37428: {'train_loss_batch': 3.6133265495300293, 'train_lr': 0.09783043795292455, 'batch_time': 1.6557459768678982, 'data_time': 0.004091670839849088}
2025-11-04 13:23:09 Train Epoch 014: 100%|█████████▉| 2500/2502 [1:09:02<00:03,  1.66s/it, Loss=3.7216, Top1=N/A, LR=0.097830]2025-11-04 13:25:57,136 - INFO - Step 37528: {'train_loss_batch': 3.480006456375122, 'train_lr': 0.09783043795292455, 'batch_time': 1.6564294538799165, 'data_time': 0.004480735629332252}
2025-11-04 13:25:57 Train Epoch 014: 100%|██████████| 2502/2502 [1:09:04<00:00,  1.66s/it, Loss=3.7216, Top1=N/A, LR=0.097830]
2025-11-04 13:25:59 Val Epoch 014:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 13:26:03   with torch.cuda.amp.autocast():
2025-11-04 13:26:04 Val Epoch 014: 100%|██████████| 98/98 [01:52<00:00,  1.15s/it, Loss=2.8828, Top1=52.42%, Top5=77.92%]
2025-11-04 13:27:51 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 13:27:51   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 13:27:51 2025-11-04 13:27:51,905 - INFO - Step 14: {'epoch': 14, 'learning_rate': 0.09725946548890017, 'train_loss': 3.72190323293352, 'train_top1': 50.43993089530333, 'train_top5': 74.83488258317026, 'train_precision': 50.012651988131104, 'train_recall': 50.329460360211264, 'train_f1': 49.876006815787505, 'val_loss': 2.8828323653411867, 'val_top1': 52.4159999987793, 'val_top5': 77.9219999975586, 'val_precision': 57.779709510291354, 'val_recall': 52.42400000000001, 'val_f1': 51.844553352091246}
2025-11-04 13:27:51 2025-11-04 13:27:51,906 - INFO - Epoch 014 Summary - LR: 0.097259, Train Loss: 3.7219, Val Loss: 2.8828, Val F1: 51.84%, Val Precision: 57.78%, Val Recall: 52.42%
2025-11-04 13:27:54 2025-11-04 13:27:54,906 - INFO - New best model saved with validation accuracy: 52.416%
2025-11-04 13:27:54 2025-11-04 13:27:54,907 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_015.pth
2025-11-04 13:27:54 Train Epoch 015:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 14 that is less than the current step 37528. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 13:27:58 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 13:27:58   with torch.cuda.amp.autocast():
2025-11-04 13:28:00 Train Epoch 015:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.9832, Top1=N/A, LR=0.097259]2025-11-04 13:28:00,295 - INFO - Step 37530: {'train_loss_batch': 2.983152151107788, 'train_lr': 0.09725946548890017, 'batch_time': 5.384963512420654, 'data_time': 3.7455968856811523}
2025-11-04 13:28:00 Train Epoch 015:   4%|▍         | 100/2502 [02:50<1:06:37,  1.66s/it, Loss=3.6590, Top1=N/A, LR=0.097259]2025-11-04 13:30:45,553 - INFO - Step 37630: {'train_loss_batch': 2.9880335330963135, 'train_lr': 0.09725946548890017, 'batch_time': 1.6895465496743078, 'data_time': 0.03811892188421571}
2025-11-04 13:30:45 Train Epoch 015:   8%|▊         | 200/2502 [05:35<1:03:47,  1.66s/it, Loss=3.7165, Top1=N/A, LR=0.097259]2025-11-04 13:33:30,817 - INFO - Step 37730: {'train_loss_batch': 4.35231351852417, 'train_lr': 0.09725946548890017, 'batch_time': 1.6711850664508876, 'data_time': 0.019642006698532485}
2025-11-04 13:33:30 Train Epoch 015:  12%|█▏        | 300/2502 [08:20<1:00:56,  1.66s/it, Loss=3.6995, Top1=N/A, LR=0.097259]2025-11-04 13:36:15,454 - INFO - Step 37830: {'train_loss_batch': 3.0459632873535156, 'train_lr': 0.09725946548890017, 'batch_time': 1.6629391207647481, 'data_time': 0.013430577179918258}
2025-11-04 13:36:15 Train Epoch 015:  16%|█▌        | 400/2502 [11:06<58:06,  1.66s/it, Loss=3.7078, Top1=N/A, LR=0.097259]2025-11-04 13:39:01,419 - INFO - Step 37930: {'train_loss_batch': 5.388080596923828, 'train_lr': 0.09725946548890017, 'batch_time': 1.6621195835960179, 'data_time': 0.010331319751882196}
2025-11-04 13:39:01 Train Epoch 015:  20%|█▉        | 500/2502 [13:51<55:17,  1.66s/it, Loss=3.7178, Top1=N/A, LR=0.097259]2025-11-04 13:41:45,974 - INFO - Step 38030: {'train_loss_batch': 3.02748441696167, 'train_lr': 0.09725946548890017, 'batch_time': 1.6588114259723656, 'data_time': 0.008467310203049711}
2025-11-04 13:41:45 Train Epoch 015:  24%|██▍       | 600/2502 [16:37<52:15,  1.65s/it, Loss=3.7273, Top1=N/A, LR=0.097259]2025-11-04 13:44:31,957 - INFO - Step 38130: {'train_loss_batch': 3.0680623054504395, 'train_lr': 0.09725946548890017, 'batch_time': 1.6589815561069228, 'data_time': 0.007221081887624426}
2025-11-04 13:44:31 Train Epoch 015:  28%|██▊       | 700/2502 [19:21<49:52,  1.66s/it, Loss=3.7393, Top1=N/A, LR=0.097259]2025-11-04 13:47:16,903 - INFO - Step 38230: {'train_loss_batch': 3.102778196334839, 'train_lr': 0.09725946548890017, 'batch_time': 1.657622371012406, 'data_time': 0.006332222303208203}
2025-11-04 13:47:16 Train Epoch 015:  32%|███▏      | 800/2502 [22:07<46:32,  1.64s/it, Loss=3.7238, Top1=N/A, LR=0.097259]2025-11-04 13:50:02,055 - INFO - Step 38330: {'train_loss_batch': 4.735097408294678, 'train_lr': 0.09725946548890017, 'batch_time': 1.6568595746929726, 'data_time': 0.005658636081233602}
2025-11-04 13:50:02 Train Epoch 015:  36%|███▌      | 900/2502 [24:51<44:15,  1.66s/it, Loss=3.7151, Top1=51.27%, LR=0.097259]2025-11-04 13:52:46,815 - INFO - Step 38430: {'train_loss_batch': 3.140019416809082, 'train_lr': 0.09725946548890017, 'batch_time': 1.655832200680139, 'data_time': 0.005137276305474928}
2025-11-04 13:52:46 Train Epoch 015:  40%|███▉      | 1000/2502 [27:37<41:32,  1.66s/it, Loss=3.7133, Top1=N/A, LR=0.097259]   2025-11-04 13:55:32,334 - INFO - Step 38530: {'train_loss_batch': 3.064666748046875, 'train_lr': 0.09725946548890017, 'batch_time': 1.65576752534994, 'data_time': 0.004724509232527726}
2025-11-04 13:55:32 Train Epoch 015:  44%|████▍     | 1100/2502 [30:22<38:25,  1.64s/it, Loss=3.7069, Top1=N/A, LR=0.097259]2025-11-04 13:58:17,637 - INFO - Step 38630: {'train_loss_batch': 3.7107582092285156, 'train_lr': 0.09725946548890017, 'batch_time': 1.655519317649474, 'data_time': 0.004384337502322773}
2025-11-04 13:58:17 Train Epoch 015:  48%|████▊     | 1200/2502 [33:08<36:01,  1.66s/it, Loss=3.6981, Top1=51.18%, LR=0.097259]2025-11-04 14:01:03,436 - INFO - Step 38730: {'train_loss_batch': 2.909604549407959, 'train_lr': 0.09725946548890017, 'batch_time': 1.6557245212828091, 'data_time': 0.004100295129564779}
2025-11-04 14:01:03 Train Epoch 015:  52%|█████▏    | 1300/2502 [35:53<33:11,  1.66s/it, Loss=3.6921, Top1=N/A, LR=0.097259]   2025-11-04 14:03:48,757 - INFO - Step 38830: {'train_loss_batch': 3.665109872817993, 'train_lr': 0.09725946548890017, 'batch_time': 1.6555308908980044, 'data_time': 0.003868181828257673}
2025-11-04 14:03:48 Train Epoch 015:  56%|█████▌    | 1400/2502 [38:39<30:21,  1.65s/it, Loss=3.6823, Top1=N/A, LR=0.097259]2025-11-04 14:06:34,381 - INFO - Step 38930: {'train_loss_batch': 3.07688045501709, 'train_lr': 0.09725946548890017, 'batch_time': 1.6555814309089547, 'data_time': 0.0036650354057273893}
2025-11-04 14:06:34 Train Epoch 015:  60%|█████▉    | 1500/2502 [41:25<27:45,  1.66s/it, Loss=3.6725, Top1=N/A, LR=0.097259]2025-11-04 14:09:20,377 - INFO - Step 39030: {'train_loss_batch': 4.569650650024414, 'train_lr': 0.09725946548890017, 'batch_time': 1.655872764466684, 'data_time': 0.003486223176350044}
2025-11-04 14:09:20 Train Epoch 015:  64%|██████▍   | 1600/2502 [44:10<24:58,  1.66s/it, Loss=3.6731, Top1=N/A, LR=0.097259]2025-11-04 14:12:05,663 - INFO - Step 39130: {'train_loss_batch': 4.730656147003174, 'train_lr': 0.09725946548890017, 'batch_time': 1.6556849238427263, 'data_time': 0.0033334306744319956}
2025-11-04 14:12:05 Train Epoch 015:  68%|██████▊   | 1700/2502 [46:56<21:59,  1.65s/it, Loss=3.6754, Top1=51.18%, LR=0.097259]2025-11-04 14:14:51,033 - INFO - Step 39230: {'train_loss_batch': 2.9688282012939453, 'train_lr': 0.09725946548890017, 'batch_time': 1.6555677630633623, 'data_time': 0.0031999103606693328}
2025-11-04 14:14:51 Train Epoch 015:  72%|███████▏  | 1800/2502 [49:41<19:24,  1.66s/it, Loss=3.6741, Top1=N/A, LR=0.097259]   2025-11-04 14:17:36,406 - INFO - Step 39330: {'train_loss_batch': 4.229290008544922, 'train_lr': 0.09725946548890017, 'batch_time': 1.655465697527329, 'data_time': 0.003078238160526799}
2025-11-04 14:17:36 Train Epoch 015:  76%|███████▌  | 1900/2502 [52:25<16:33,  1.65s/it, Loss=3.6748, Top1=N/A, LR=0.097259]2025-11-04 14:20:20,538 - INFO - Step 39430: {'train_loss_batch': 3.6293606758117676, 'train_lr': 0.09725946548890017, 'batch_time': 1.6547217149599047, 'data_time': 0.0029665501979324205}
2025-11-04 14:20:20 Train Epoch 015:  80%|███████▉  | 2000/2502 [55:11<13:48,  1.65s/it, Loss=3.6816, Top1=N/A, LR=0.097259]2025-11-04 14:23:06,266 - INFO - Step 39530: {'train_loss_batch': 4.2940521240234375, 'train_lr': 0.09725946548890017, 'batch_time': 1.654849233417616, 'data_time': 0.002867392812115976}
2025-11-04 14:23:06 Train Epoch 015:  84%|████████▍ | 2100/2502 [57:57<11:06,  1.66s/it, Loss=3.6775, Top1=51.19%, LR=0.097259]2025-11-04 14:25:52,353 - INFO - Step 39630: {'train_loss_batch': 3.0353245735168457, 'train_lr': 0.09725946548890017, 'batch_time': 1.655135872816824, 'data_time': 0.002782341413756656}
2025-11-04 14:25:52 Train Epoch 015:  88%|████████▊ | 2200/2502 [1:00:43<08:20,  1.66s/it, Loss=3.6826, Top1=N/A, LR=0.097259]   2025-11-04 14:28:38,378 - INFO - Step 39730: {'train_loss_batch': 2.9531919956207275, 'train_lr': 0.09725946548890017, 'batch_time': 1.6553679584969396, 'data_time': 0.002701017976836257}
2025-11-04 14:28:38 Train Epoch 015:  92%|█████████▏| 2300/2502 [1:03:28<05:32,  1.64s/it, Loss=3.6849, Top1=N/A, LR=0.097259]2025-11-04 14:31:23,486 - INFO - Step 39830: {'train_loss_batch': 3.382542610168457, 'train_lr': 0.09725946548890017, 'batch_time': 1.6551814348684193, 'data_time': 0.0026260748576827798}
2025-11-04 14:31:23 Train Epoch 015:  96%|█████████▌| 2400/2502 [1:06:14<02:48,  1.65s/it, Loss=3.6882, Top1=N/A, LR=0.097259]2025-11-04 14:34:08,994 - INFO - Step 39930: {'train_loss_batch': 4.670704364776611, 'train_lr': 0.09725946548890017, 'batch_time': 1.6551770021000884, 'data_time': 0.002559893233534794}
2025-11-04 14:34:08 Train Epoch 015: 100%|█████████▉| 2500/2502 [1:08:59<00:03,  1.65s/it, Loss=3.6881, Top1=N/A, LR=0.097259]2025-11-04 14:36:54,229 - INFO - Step 40030: {'train_loss_batch': 4.600196361541748, 'train_lr': 0.09725946548890017, 'batch_time': 1.6550642163788782, 'data_time': 0.0025159493773901573}
2025-11-04 14:36:54 Train Epoch 015: 100%|██████████| 2502/2502 [1:09:01<00:00,  1.66s/it, Loss=3.6881, Top1=N/A, LR=0.097259]
2025-11-04 14:36:56 Val Epoch 015:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 14:37:00   with torch.cuda.amp.autocast():
2025-11-04 14:37:01 Val Epoch 015: 100%|██████████| 98/98 [01:52<00:00,  1.15s/it, Loss=2.7840, Top1=54.83%, Top5=80.02%]
2025-11-04 14:38:49 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 14:38:49   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 14:38:49 2025-11-04 14:38:49,218 - INFO - Step 15: {'epoch': 15, 'learning_rate': 0.09662394910907077, 'train_loss': 3.688295495881737, 'train_top1': 51.16038602941177, 'train_top5': 75.23782169117646, 'train_precision': 50.73484054539379, 'train_recall': 50.9781233817134, 'train_f1': 50.569074152854185, 'val_loss': 2.784033995666504, 'val_top1': 54.83399998413086, 'val_top5': 80.01600001953125, 'val_precision': 59.61391819552877, 'val_recall': 54.831999999999994, 'val_f1': 54.10757693811128}
2025-11-04 14:38:49 2025-11-04 14:38:49,220 - INFO - Epoch 015 Summary - LR: 0.096624, Train Loss: 3.6883, Val Loss: 2.7840, Val F1: 54.11%, Val Precision: 59.61%, Val Recall: 54.83%
2025-11-04 14:38:52 2025-11-04 14:38:52,220 - INFO - New best model saved with validation accuracy: 54.834%
2025-11-04 14:38:52 2025-11-04 14:38:52,220 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_016.pth
2025-11-04 14:38:52 Train Epoch 016:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 14:38:56   with torch.cuda.amp.autocast():
2025-11-04 14:38:57 wandb: WARNING Tried to log to step 15 that is less than the current step 40030. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 14:38:57 Train Epoch 016:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.9707, Top1=N/A, LR=0.096624]2025-11-04 14:38:57,755 - INFO - Step 40032: {'train_loss_batch': 2.9707043170928955, 'train_lr': 0.09662394910907077, 'batch_time': 5.531799077987671, 'data_time': 3.8827123641967773}
2025-11-04 14:38:57 Train Epoch 016:   4%|▍         | 100/2502 [02:51<1:06:16,  1.66s/it, Loss=3.5913, Top1=51.68%, LR=0.096624]2025-11-04 14:41:43,511 - INFO - Step 40132: {'train_loss_batch': 3.053331136703491, 'train_lr': 0.09662394910907077, 'batch_time': 1.6959310498568092, 'data_time': 0.03943191660512792}
2025-11-04 14:41:43 Train Epoch 016:   8%|▊         | 200/2502 [05:35<1:03:23,  1.65s/it, Loss=3.5961, Top1=N/A, LR=0.096624]   2025-11-04 14:44:27,978 - INFO - Step 40232: {'train_loss_batch': 2.9322752952575684, 'train_lr': 0.09662394910907077, 'batch_time': 1.6704268360612404, 'data_time': 0.020253568146359268}
2025-11-04 14:44:27 Train Epoch 016:  12%|█▏        | 300/2502 [08:20<1:00:47,  1.66s/it, Loss=3.5942, Top1=N/A, LR=0.096624]2025-11-04 14:47:13,106 - INFO - Step 40332: {'train_loss_batch': 4.145564079284668, 'train_lr': 0.09662394910907077, 'batch_time': 1.664065470331135, 'data_time': 0.013833132297097647}
2025-11-04 14:47:13 Train Epoch 016:  16%|█▌        | 400/2502 [11:06<57:40,  1.65s/it, Loss=3.5731, Top1=N/A, LR=0.096624]2025-11-04 14:49:58,282 - INFO - Step 40432: {'train_loss_batch': 4.208901405334473, 'train_lr': 0.09662394910907077, 'batch_time': 1.6609956956563745, 'data_time': 0.010619594569218129}
2025-11-04 14:49:58 Train Epoch 016:  20%|█▉        | 500/2502 [13:50<54:59,  1.65s/it, Loss=3.5908, Top1=N/A, LR=0.096624]2025-11-04 14:52:43,198 - INFO - Step 40532: {'train_loss_batch': 4.660182952880859, 'train_lr': 0.09662394910907077, 'batch_time': 1.6586336804007342, 'data_time': 0.008696043562746333}
2025-11-04 14:52:43 Train Epoch 016:  24%|██▍       | 600/2502 [16:35<52:06,  1.64s/it, Loss=3.5867, Top1=N/A, LR=0.096624]2025-11-04 14:55:28,055 - INFO - Step 40632: {'train_loss_batch': 3.0716986656188965, 'train_lr': 0.09662394910907077, 'batch_time': 1.6569588116917158, 'data_time': 0.007415306945013723}
2025-11-04 14:55:28 Train Epoch 016:  28%|██▊       | 700/2502 [19:20<49:32,  1.65s/it, Loss=3.6079, Top1=52.16%, LR=0.096624]2025-11-04 14:58:12,937 - INFO - Step 40732: {'train_loss_batch': 3.0191128253936768, 'train_lr': 0.09662394910907077, 'batch_time': 1.6557964727643892, 'data_time': 0.006506531452826528}
2025-11-04 14:58:12 Train Epoch 016:  32%|███▏      | 800/2502 [22:05<46:50,  1.65s/it, Loss=3.6123, Top1=N/A, LR=0.096624]   2025-11-04 15:00:57,828 - INFO - Step 40832: {'train_loss_batch': 4.65189790725708, 'train_lr': 0.09662394910907077, 'batch_time': 1.6549365389510784, 'data_time': 0.005821412868714065}
2025-11-04 15:00:57 Train Epoch 016:  36%|███▌      | 900/2502 [24:50<44:01,  1.65s/it, Loss=3.6215, Top1=N/A, LR=0.096624]2025-11-04 15:03:42,775 - INFO - Step 40932: {'train_loss_batch': 3.0094144344329834, 'train_lr': 0.09662394910907077, 'batch_time': 1.6543302636564639, 'data_time': 0.005283040555812146}
2025-11-04 15:03:42 Train Epoch 016:  40%|███▉      | 1000/2502 [27:35<41:19,  1.65s/it, Loss=3.6393, Top1=N/A, LR=0.096624]2025-11-04 15:06:27,883 - INFO - Step 41032: {'train_loss_batch': 4.435673713684082, 'train_lr': 0.09662394910907077, 'batch_time': 1.6540045816819746, 'data_time': 0.004858120099886076}
2025-11-04 15:06:27 Train Epoch 016:  44%|████▍     | 1100/2502 [30:20<38:16,  1.64s/it, Loss=3.6451, Top1=N/A, LR=0.096624]2025-11-04 15:09:12,370 - INFO - Step 41132: {'train_loss_batch': 3.5675430297851562, 'train_lr': 0.09662394910907077, 'batch_time': 1.6531751935423558, 'data_time': 0.004507433382843322}
2025-11-04 15:09:12 Train Epoch 016:  48%|████▊     | 1200/2502 [33:04<35:42,  1.65s/it, Loss=3.6485, Top1=N/A, LR=0.096624]2025-11-04 15:11:57,143 - INFO - Step 41232: {'train_loss_batch': 2.9997150897979736, 'train_lr': 0.09662394910907077, 'batch_time': 1.6527212198131984, 'data_time': 0.0042235495942915405}
2025-11-04 15:11:57 Train Epoch 016:  52%|█████▏    | 1300/2502 [35:49<33:04,  1.65s/it, Loss=3.6468, Top1=51.98%, LR=0.096624]2025-11-04 15:14:42,159 - INFO - Step 41332: {'train_loss_batch': 2.9529190063476562, 'train_lr': 0.09662394910907077, 'batch_time': 1.6525241307897076, 'data_time': 0.003976399123347969}
2025-11-04 15:14:42 Train Epoch 016:  56%|█████▌    | 1400/2502 [38:34<30:22,  1.65s/it, Loss=3.6450, Top1=N/A, LR=0.096624]   2025-11-04 15:17:27,019 - INFO - Step 41432: {'train_loss_batch': 3.0496866703033447, 'train_lr': 0.09662394910907077, 'batch_time': 1.6522439184740219, 'data_time': 0.0037672996180641233}
2025-11-04 15:17:27 Train Epoch 016:  60%|█████▉    | 1500/2502 [41:19<27:31,  1.65s/it, Loss=3.6525, Top1=N/A, LR=0.096624]2025-11-04 15:20:11,922 - INFO - Step 41532: {'train_loss_batch': 2.9635844230651855, 'train_lr': 0.09662394910907077, 'batch_time': 1.6520298191263705, 'data_time': 0.0035862997323175337}
2025-11-04 15:20:11 Train Epoch 016:  64%|██████▍   | 1600/2502 [44:04<24:48,  1.65s/it, Loss=3.6601, Top1=N/A, LR=0.096624]2025-11-04 15:22:56,470 - INFO - Step 41632: {'train_loss_batch': 2.933682918548584, 'train_lr': 0.09662394910907077, 'batch_time': 1.6516207155028706, 'data_time': 0.003426219730508246}
2025-11-04 15:22:56 Train Epoch 016:  68%|██████▊   | 1700/2502 [46:49<22:01,  1.65s/it, Loss=3.6601, Top1=51.97%, LR=0.096624]2025-11-04 15:25:41,281 - INFO - Step 41732: {'train_loss_batch': 2.9535272121429443, 'train_lr': 0.09662394910907077, 'batch_time': 1.6514140752257493, 'data_time': 0.0032849791188719413}
2025-11-04 15:25:41 Train Epoch 016:  72%|███████▏  | 1800/2502 [49:34<19:15,  1.65s/it, Loss=3.6582, Top1=51.92%, LR=0.096624]2025-11-04 15:28:26,235 - INFO - Step 41832: {'train_loss_batch': 2.9825849533081055, 'train_lr': 0.09662394910907077, 'batch_time': 1.6513097123395464, 'data_time': 0.0031485807756660117}
2025-11-04 15:28:26 Train Epoch 016:  76%|███████▌  | 1900/2502 [52:18<16:30,  1.65s/it, Loss=3.6612, Top1=N/A, LR=0.096624]   2025-11-04 15:31:11,044 - INFO - Step 41932: {'train_loss_batch': 4.597777843475342, 'train_lr': 0.09662394910907077, 'batch_time': 1.651140320495453, 'data_time': 0.003027985185024175}
2025-11-04 15:31:11 Train Epoch 016:  80%|███████▉  | 2000/2502 [55:03<13:40,  1.64s/it, Loss=3.6592, Top1=N/A, LR=0.096624]2025-11-04 15:33:55,275 - INFO - Step 42032: {'train_loss_batch': 4.402156829833984, 'train_lr': 0.09662394910907077, 'batch_time': 1.650698960512534, 'data_time': 0.0029186361256627546}
2025-11-04 15:33:55 Train Epoch 016:  84%|████████▍ | 2100/2502 [57:47<11:03,  1.65s/it, Loss=3.6636, Top1=N/A, LR=0.096624]2025-11-04 15:36:39,879 - INFO - Step 42132: {'train_loss_batch': 3.592174768447876, 'train_lr': 0.09662394910907077, 'batch_time': 1.650477472570384, 'data_time': 0.002821383051847288}
2025-11-04 15:36:39 Train Epoch 016:  88%|████████▊ | 2200/2502 [1:00:32<08:19,  1.65s/it, Loss=3.6658, Top1=N/A, LR=0.096624]2025-11-04 15:39:25,136 - INFO - Step 42232: {'train_loss_batch': 5.273679256439209, 'train_lr': 0.09662394910907077, 'batch_time': 1.6505719166027313, 'data_time': 0.0027334170144344126}
2025-11-04 15:39:25 Train Epoch 016:  92%|█████████▏| 2300/2502 [1:03:18<05:33,  1.65s/it, Loss=3.6636, Top1=N/A, LR=0.096624]2025-11-04 15:42:10,453 - INFO - Step 42332: {'train_loss_batch': 4.380416393280029, 'train_lr': 0.09662394910907077, 'batch_time': 1.6506849595226967, 'data_time': 0.0026587267017323057}
2025-11-04 15:42:10 Train Epoch 016:  96%|█████████▌| 2400/2502 [1:06:03<02:47,  1.64s/it, Loss=3.6668, Top1=N/A, LR=0.096624]2025-11-04 15:44:55,281 - INFO - Step 42432: {'train_loss_batch': 3.8075199127197266, 'train_lr': 0.09662394910907077, 'batch_time': 1.6505848909606045, 'data_time': 0.002589014459281899}
2025-11-04 15:44:55 Train Epoch 016: 100%|█████████▉| 2500/2502 [1:08:47<00:03,  1.65s/it, Loss=3.6668, Top1=N/A, LR=0.096624]2025-11-04 15:47:40,015 - INFO - Step 42532: {'train_loss_batch': 5.2795867919921875, 'train_lr': 0.09662394910907077, 'batch_time': 1.6504549048796313, 'data_time': 0.0025432097439954684}
2025-11-04 15:47:40 Train Epoch 016: 100%|██████████| 2502/2502 [1:08:49<00:00,  1.65s/it, Loss=3.6668, Top1=N/A, LR=0.096624]
2025-11-04 15:47:42 Val Epoch 016:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 15:47:46   with torch.cuda.amp.autocast():
2025-11-04 15:47:47 Val Epoch 016: 100%|██████████| 98/98 [01:53<00:00,  1.15s/it, Loss=2.8116, Top1=54.10%, Top5=79.33%]
2025-11-04 15:49:35 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 15:49:35   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 15:49:35 2025-11-04 15:49:35,548 - INFO - Step 16: {'epoch': 16, 'learning_rate': 0.09592475685236743, 'train_loss': 3.666996960064395, 'train_top1': 51.845969114785994, 'train_top5': 75.69499331225681, 'train_precision': 51.41478364426971, 'train_recall': 51.65235909718364, 'train_f1': 51.26036439814502, 'val_loss': 2.811572795562744, 'val_top1': 54.10400000610352, 'val_top5': 79.33399997070312, 'val_precision': 59.34565242303381, 'val_recall': 54.10799999999999, 'val_f1': 53.44752027823279}
2025-11-04 15:49:35 2025-11-04 15:49:35,550 - INFO - Epoch 016 Summary - LR: 0.095925, Train Loss: 3.6670, Val Loss: 2.8116, Val F1: 53.45%, Val Precision: 59.35%, Val Recall: 54.11%
2025-11-04 15:49:36 Train Epoch 017:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 16 that is less than the current step 42532. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 15:49:40 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 15:49:40   with torch.cuda.amp.autocast():
2025-11-04 15:49:42 Train Epoch 017:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.8675, Top1=53.71%, LR=0.095925]2025-11-04 15:49:42,467 - INFO - Step 42534: {'train_loss_batch': 2.86746883392334, 'train_lr': 0.09592475685236743, 'batch_time': 5.7542643547058105, 'data_time': 4.129757881164551}
2025-11-04 15:49:42 Train Epoch 017:   4%|▍         | 100/2502 [02:49<1:05:46,  1.64s/it, Loss=3.6560, Top1=51.89%, LR=0.095925]2025-11-04 15:52:26,489 - INFO - Step 42634: {'train_loss_batch': 2.8980493545532227, 'train_lr': 0.09592475685236743, 'batch_time': 1.6809548198586644, 'data_time': 0.04196487795008291}
2025-11-04 15:52:26 Train Epoch 017:   8%|▊         | 200/2502 [05:34<1:03:22,  1.65s/it, Loss=3.5923, Top1=N/A, LR=0.095925]   2025-11-04 15:55:11,063 - INFO - Step 42734: {'train_loss_batch': 2.952946186065674, 'train_lr': 0.09592475685236743, 'batch_time': 1.6634382264531073, 'data_time': 0.021587166620131156}
2025-11-04 15:55:11 Train Epoch 017:  12%|█▏        | 300/2502 [08:19<1:00:14,  1.64s/it, Loss=3.6217, Top1=N/A, LR=0.095925]2025-11-04 15:57:55,751 - INFO - Step 42834: {'train_loss_batch': 4.8625168800354, 'train_lr': 0.09592475685236743, 'batch_time': 1.6579346680561966, 'data_time': 0.014720476346950595}
2025-11-04 15:57:55 Train Epoch 017:  16%|█▌        | 400/2502 [11:03<57:48,  1.65s/it, Loss=3.6559, Top1=52.26%, LR=0.095925]2025-11-04 16:00:40,276 - INFO - Step 42934: {'train_loss_batch': 2.9491724967956543, 'train_lr': 0.09592475685236743, 'batch_time': 1.6547709438866214, 'data_time': 0.011284020773490468}
2025-11-04 16:00:40 Train Epoch 017:  20%|█▉        | 500/2502 [13:48<55:08,  1.65s/it, Loss=3.6320, Top1=N/A, LR=0.095925]   2025-11-04 16:03:25,423 - INFO - Step 43034: {'train_loss_batch': 5.306797504425049, 'train_lr': 0.09592475685236743, 'batch_time': 1.654112429437999, 'data_time': 0.009221438162341089}
2025-11-04 16:03:25 Train Epoch 017:  24%|██▍       | 600/2502 [16:33<52:14,  1.65s/it, Loss=3.6461, Top1=N/A, LR=0.095925]2025-11-04 16:06:10,287 - INFO - Step 43134: {'train_loss_batch': 4.731024742126465, 'train_lr': 0.09592475685236743, 'batch_time': 1.653199705625334, 'data_time': 0.00785630157902316}
2025-11-04 16:06:10 Train Epoch 017:  28%|██▊       | 700/2502 [19:18<49:36,  1.65s/it, Loss=3.6678, Top1=N/A, LR=0.095925]2025-11-04 16:08:55,182 - INFO - Step 43234: {'train_loss_batch': 2.969155788421631, 'train_lr': 0.09592475685236743, 'batch_time': 1.652594631306625, 'data_time': 0.006875694223205305}
2025-11-04 16:08:55 Train Epoch 017:  32%|███▏      | 800/2502 [22:03<46:47,  1.65s/it, Loss=3.6551, Top1=N/A, LR=0.095925]2025-11-04 16:11:40,381 - INFO - Step 43334: {'train_loss_batch': 3.453812599182129, 'train_lr': 0.09592475685236743, 'batch_time': 1.6525182188226935, 'data_time': 0.006149562854743033}
2025-11-04 16:11:40 Train Epoch 017:  36%|███▌      | 900/2502 [24:48<44:01,  1.65s/it, Loss=3.6664, Top1=N/A, LR=0.095925]2025-11-04 16:14:24,995 - INFO - Step 43434: {'train_loss_batch': 4.155853271484375, 'train_lr': 0.09592475685236743, 'batch_time': 1.651810839226455, 'data_time': 0.005577420288661741}
2025-11-04 16:14:24 Train Epoch 017:  40%|███▉      | 1000/2502 [27:32<41:13,  1.65s/it, Loss=3.6618, Top1=N/A, LR=0.095925]2025-11-04 16:17:09,454 - INFO - Step 43534: {'train_loss_batch': 4.583807945251465, 'train_lr': 0.09592475685236743, 'batch_time': 1.6510892147784466, 'data_time': 0.005114835220855194}
2025-11-04 16:17:09 Train Epoch 017:  44%|████▍     | 1100/2502 [30:17<38:36,  1.65s/it, Loss=3.6647, Top1=52.33%, LR=0.095925]2025-11-04 16:19:54,517 - INFO - Step 43634: {'train_loss_batch': 2.873565196990967, 'train_lr': 0.09592475685236743, 'batch_time': 1.6510472940813943, 'data_time': 0.0047360228799236135}
2025-11-04 16:19:54 Train Epoch 017:  48%|████▊     | 1200/2502 [33:02<35:44,  1.65s/it, Loss=3.6691, Top1=N/A, LR=0.095925]   2025-11-04 16:22:39,620 - INFO - Step 43734: {'train_loss_batch': 3.7196919918060303, 'train_lr': 0.09592475685236743, 'batch_time': 1.651045071890114, 'data_time': 0.004424640280718014}
2025-11-04 16:22:39 Train Epoch 017:  52%|█████▏    | 1300/2502 [35:47<32:57,  1.65s/it, Loss=3.6667, Top1=52.20%, LR=0.095925]2025-11-04 16:25:24,190 - INFO - Step 43834: {'train_loss_batch': 3.062934637069702, 'train_lr': 0.09592475685236743, 'batch_time': 1.6506343919620616, 'data_time': 0.004157384114481686}
2025-11-04 16:25:24 Train Epoch 017:  56%|█████▌    | 1400/2502 [38:32<30:16,  1.65s/it, Loss=3.6644, Top1=N/A, LR=0.095925]   2025-11-04 16:28:08,877 - INFO - Step 43934: {'train_loss_batch': 3.9373087882995605, 'train_lr': 0.09592475685236743, 'batch_time': 1.6503655966650495, 'data_time': 0.003927907630599115}
2025-11-04 16:28:08 Train Epoch 017:  60%|█████▉    | 1500/2502 [41:17<27:34,  1.65s/it, Loss=3.6651, Top1=N/A, LR=0.095925]2025-11-04 16:30:53,725 - INFO - Step 44034: {'train_loss_batch': 5.348722457885742, 'train_lr': 0.09592475685236743, 'batch_time': 1.6502402133738336, 'data_time': 0.0037283120673152308}
2025-11-04 16:30:53 Train Epoch 017:  64%|██████▍   | 1600/2502 [44:01<24:40,  1.64s/it, Loss=3.6663, Top1=N/A, LR=0.095925]2025-11-04 16:33:38,670 - INFO - Step 44134: {'train_loss_batch': 4.34397029876709, 'train_lr': 0.09592475685236743, 'batch_time': 1.6501900519824342, 'data_time': 0.0035580646090772584}
2025-11-04 16:33:38 Train Epoch 017:  68%|██████▊   | 1700/2502 [46:46<21:59,  1.65s/it, Loss=3.6636, Top1=52.24%, LR=0.095925]2025-11-04 16:36:23,483 - INFO - Step 44234: {'train_loss_batch': 2.9347400665283203, 'train_lr': 0.09592475685236743, 'batch_time': 1.6500692181135892, 'data_time': 0.003407595369270589}
2025-11-04 16:36:23 Train Epoch 017:  72%|███████▏  | 1800/2502 [49:31<19:15,  1.65s/it, Loss=3.6647, Top1=N/A, LR=0.095925]   2025-11-04 16:39:08,240 - INFO - Step 44334: {'train_loss_batch': 5.224285125732422, 'train_lr': 0.09592475685236743, 'batch_time': 1.6499303141810508, 'data_time': 0.003271674924529572}
2025-11-04 16:39:08 Train Epoch 017:  76%|███████▌  | 1900/2502 [52:16<16:36,  1.66s/it, Loss=3.6601, Top1=N/A, LR=0.095925]2025-11-04 16:41:53,186 - INFO - Step 44434: {'train_loss_batch': 3.827190399169922, 'train_lr': 0.09592475685236743, 'batch_time': 1.6499055469117623, 'data_time': 0.003152407325612188}
2025-11-04 16:41:53 Train Epoch 017:  80%|███████▉  | 2000/2502 [55:01<13:51,  1.66s/it, Loss=3.6587, Top1=N/A, LR=0.095925]2025-11-04 16:44:38,541 - INFO - Step 44534: {'train_loss_batch': 5.354197025299072, 'train_lr': 0.09592475685236743, 'batch_time': 1.6500874752404986, 'data_time': 0.003042886282193071}
2025-11-04 16:44:38 Train Epoch 017:  84%|████████▍ | 2100/2502 [57:46<11:01,  1.65s/it, Loss=3.6580, Top1=N/A, LR=0.095925]2025-11-04 16:47:23,605 - INFO - Step 44634: {'train_loss_batch': 3.699406623840332, 'train_lr': 0.09592475685236743, 'batch_time': 1.650113887528134, 'data_time': 0.0029434913796847462}
2025-11-04 16:47:23 Train Epoch 017:  88%|████████▊ | 2200/2502 [1:00:31<08:17,  1.65s/it, Loss=3.6626, Top1=52.24%, LR=0.095925]2025-11-04 16:50:08,436 - INFO - Step 44734: {'train_loss_batch': 3.037526845932007, 'train_lr': 0.09592475685236743, 'batch_time': 1.650031588826056, 'data_time': 0.002852201570114836}
2025-11-04 16:50:08 Train Epoch 017:  92%|█████████▏| 2300/2502 [1:03:16<05:33,  1.65s/it, Loss=3.6701, Top1=52.26%, LR=0.095925]2025-11-04 16:52:53,441 - INFO - Step 44834: {'train_loss_batch': 2.9777584075927734, 'train_lr': 0.09592475685236743, 'batch_time': 1.6500320190038436, 'data_time': 0.002771276020578901}
2025-11-04 16:52:53 Train Epoch 017:  96%|█████████▌| 2400/2502 [1:06:01<02:48,  1.65s/it, Loss=3.6693, Top1=N/A, LR=0.095925]   2025-11-04 16:55:38,258 - INFO - Step 44934: {'train_loss_batch': 3.7759830951690674, 'train_lr': 0.09592475685236743, 'batch_time': 1.649954646788553, 'data_time': 0.002696141457865507}
2025-11-04 16:55:38 Train Epoch 017: 100%|█████████▉| 2500/2502 [1:08:46<00:03,  1.64s/it, Loss=3.6731, Top1=N/A, LR=0.095925]2025-11-04 16:58:23,285 - INFO - Step 45034: {'train_loss_batch': 2.9229750633239746, 'train_lr': 0.09592475685236743, 'batch_time': 1.6499673134324455, 'data_time': 0.0026456956051197685}
2025-11-04 16:58:23 Train Epoch 017: 100%|██████████| 2502/2502 [1:08:48<00:00,  1.65s/it, Loss=3.6731, Top1=N/A, LR=0.095925]
2025-11-04 16:58:25 Val Epoch 017:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 16:58:29   with torch.cuda.amp.autocast():
2025-11-04 16:58:30 Val Epoch 017: 100%|██████████| 98/98 [01:53<00:00,  1.16s/it, Loss=2.7623, Top1=55.75%, Top5=80.60%]
2025-11-04 17:00:18 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 17:00:18   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 17:00:18 2025-11-04 17:00:18,831 - INFO - Step 17: {'epoch': 17, 'learning_rate': 0.09516284373130972, 'train_loss': 3.672749927766222, 'train_top1': 52.25522706834533, 'train_top5': 76.26566715377697, 'train_precision': 51.811182232634714, 'train_recall': 52.08473856581667, 'train_f1': 51.685086381381495, 'val_loss': 2.762330235900879, 'val_top1': 55.748, 'val_top5': 80.60200001464844, 'val_precision': 60.164502547326016, 'val_recall': 55.75599999999999, 'val_f1': 55.106332077078065}
2025-11-04 17:00:18 2025-11-04 17:00:18,832 - INFO - Epoch 017 Summary - LR: 0.095163, Train Loss: 3.6727, Val Loss: 2.7623, Val F1: 55.11%, Val Precision: 60.16%, Val Recall: 55.76%
2025-11-04 17:00:21 2025-11-04 17:00:21,824 - INFO - New best model saved with validation accuracy: 55.748%
2025-11-04 17:00:21 2025-11-04 17:00:21,824 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_018.pth
2025-11-04 17:00:21 Train Epoch 018:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 17:00:25   with torch.cuda.amp.autocast():
2025-11-04 17:00:27 wandb: WARNING Tried to log to step 17 that is less than the current step 45034. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 17:00:27 Train Epoch 018:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.8647, Top1=55.27%, LR=0.095163]2025-11-04 17:00:27,404 - INFO - Step 45036: {'train_loss_batch': 2.864729881286621, 'train_lr': 0.09516284373130972, 'batch_time': 5.578256607055664, 'data_time': 3.953113079071045}
2025-11-04 17:00:27 Train Epoch 018:   4%|▍         | 100/2502 [02:49<1:06:03,  1.65s/it, Loss=3.6421, Top1=N/A, LR=0.095163]   2025-11-04 17:03:11,743 - INFO - Step 45136: {'train_loss_batch': 3.4098753929138184, 'train_lr': 0.09516284373130972, 'batch_time': 1.682353949782872, 'data_time': 0.04020397497875856}
2025-11-04 17:03:11 Train Epoch 018:   8%|▊         | 200/2502 [05:34<1:03:14,  1.65s/it, Loss=3.5582, Top1=N/A, LR=0.095163]2025-11-04 17:05:56,404 - INFO - Step 45236: {'train_loss_batch': 2.7778878211975098, 'train_lr': 0.09516284373130972, 'batch_time': 1.6645677255753855, 'data_time': 0.020676914139173518}
2025-11-04 17:05:56 Train Epoch 018:  12%|█▏        | 300/2502 [08:19<1:00:32,  1.65s/it, Loss=3.5688, Top1=N/A, LR=0.095163]2025-11-04 17:08:41,148 - INFO - Step 45336: {'train_loss_batch': 4.827932357788086, 'train_lr': 0.09516284373130972, 'batch_time': 1.6588783002771017, 'data_time': 0.01410509106328717}
2025-11-04 17:08:41 Train Epoch 018:  16%|█▌        | 400/2502 [11:04<57:51,  1.65s/it, Loss=3.6384, Top1=52.89%, LR=0.095163]2025-11-04 17:11:26,096 - INFO - Step 45436: {'train_loss_batch': 2.867626667022705, 'train_lr': 0.09516284373130972, 'batch_time': 1.6565337222710512, 'data_time': 0.010837675627330295}
2025-11-04 17:11:26 Train Epoch 018:  20%|█▉        | 500/2502 [13:48<55:00,  1.65s/it, Loss=3.6354, Top1=52.97%, LR=0.095163]2025-11-04 17:14:10,614 - INFO - Step 45536: {'train_loss_batch': 2.819171905517578, 'train_lr': 0.09516284373130972, 'batch_time': 1.6542673977073319, 'data_time': 0.008864599311661102}
2025-11-04 17:14:10 Train Epoch 018:  24%|██▍       | 600/2502 [16:33<52:14,  1.65s/it, Loss=3.6384, Top1=N/A, LR=0.095163]   2025-11-04 17:16:55,612 - INFO - Step 45636: {'train_loss_batch': 3.381978988647461, 'train_lr': 0.09516284373130972, 'batch_time': 1.6535536612131434, 'data_time': 0.0075595136886825185}
2025-11-04 17:16:55 Train Epoch 018:  28%|██▊       | 700/2502 [19:18<49:34,  1.65s/it, Loss=3.6468, Top1=N/A, LR=0.095163]2025-11-04 17:19:40,633 - INFO - Step 45736: {'train_loss_batch': 4.5342254638671875, 'train_lr': 0.09516284373130972, 'batch_time': 1.653075781086202, 'data_time': 0.006626067590101299}
2025-11-04 17:19:40 Train Epoch 018:  32%|███▏      | 800/2502 [22:03<46:46,  1.65s/it, Loss=3.6395, Top1=52.82%, LR=0.095163]2025-11-04 17:22:25,756 - INFO - Step 45836: {'train_loss_batch': 3.0417444705963135, 'train_lr': 0.09516284373130972, 'batch_time': 1.652845593725102, 'data_time': 0.0059263938375180136}
2025-11-04 17:22:25 Train Epoch 018:  36%|███▌      | 900/2502 [24:49<43:57,  1.65s/it, Loss=3.6561, Top1=N/A, LR=0.095163]   2025-11-04 17:25:10,863 - INFO - Step 45936: {'train_loss_batch': 4.1311564445495605, 'train_lr': 0.09516284373130972, 'batch_time': 1.6526479181253686, 'data_time': 0.005389908971585391}
2025-11-04 17:25:10 Train Epoch 018:  40%|███▉      | 1000/2502 [27:33<41:22,  1.65s/it, Loss=3.6499, Top1=N/A, LR=0.095163]2025-11-04 17:27:55,707 - INFO - Step 46036: {'train_loss_batch': 4.6265106201171875, 'train_lr': 0.09516284373130972, 'batch_time': 1.6522268315295239, 'data_time': 0.004954602453973029}
2025-11-04 17:27:55 Train Epoch 018:  44%|████▍     | 1100/2502 [30:18<38:25,  1.64s/it, Loss=3.6514, Top1=N/A, LR=0.095163]2025-11-04 17:30:40,631 - INFO - Step 46136: {'train_loss_batch': 5.272793769836426, 'train_lr': 0.09516284373130972, 'batch_time': 1.6519557282017319, 'data_time': 0.0045918846650084616}
2025-11-04 17:30:40 Train Epoch 018:  48%|████▊     | 1200/2502 [33:03<35:43,  1.65s/it, Loss=3.6505, Top1=N/A, LR=0.095163]2025-11-04 17:33:25,473 - INFO - Step 46236: {'train_loss_batch': 5.078121185302734, 'train_lr': 0.09516284373130972, 'batch_time': 1.651661176864154, 'data_time': 0.004294576096991317}
2025-11-04 17:33:25 Train Epoch 018:  52%|█████▏    | 1300/2502 [35:48<33:02,  1.65s/it, Loss=3.6435, Top1=N/A, LR=0.095163]2025-11-04 17:36:10,174 - INFO - Step 46336: {'train_loss_batch': 3.039527177810669, 'train_lr': 0.09516284373130972, 'batch_time': 1.6513038773796909, 'data_time': 0.004038514218634591}
2025-11-04 17:36:10 Train Epoch 018:  56%|█████▌    | 1400/2502 [38:33<30:19,  1.65s/it, Loss=3.6497, Top1=N/A, LR=0.095163]2025-11-04 17:38:55,033 - INFO - Step 46436: {'train_loss_batch': 2.807636260986328, 'train_lr': 0.09516284373130972, 'batch_time': 1.6511098191535618, 'data_time': 0.00382168798426234}
2025-11-04 17:38:55 Train Epoch 018:  60%|█████▉    | 1500/2502 [41:18<27:38,  1.65s/it, Loss=3.6555, Top1=N/A, LR=0.095163]2025-11-04 17:41:39,924 - INFO - Step 46536: {'train_loss_batch': 4.5047526359558105, 'train_lr': 0.09516284373130972, 'batch_time': 1.650963256233617, 'data_time': 0.003632243357842005}
2025-11-04 17:41:39 Train Epoch 018:  64%|██████▍   | 1600/2502 [44:02<24:45,  1.65s/it, Loss=3.6597, Top1=N/A, LR=0.095163]2025-11-04 17:44:24,748 - INFO - Step 46636: {'train_loss_batch': 3.4945530891418457, 'train_lr': 0.09516284373130972, 'batch_time': 1.6507926739282268, 'data_time': 0.0034661019019675506}
2025-11-04 17:44:24 Train Epoch 018:  68%|██████▊   | 1700/2502 [46:48<22:04,  1.65s/it, Loss=3.6607, Top1=N/A, LR=0.095163]2025-11-04 17:47:09,836 - INFO - Step 46736: {'train_loss_batch': 3.047497272491455, 'train_lr': 0.09516284373130972, 'batch_time': 1.6507977732905634, 'data_time': 0.0033190118642781215}
2025-11-04 17:47:09 Train Epoch 018:  72%|███████▏  | 1800/2502 [49:33<19:19,  1.65s/it, Loss=3.6632, Top1=N/A, LR=0.095163]2025-11-04 17:49:54,977 - INFO - Step 46836: {'train_loss_batch': 3.0171327590942383, 'train_lr': 0.09516284373130972, 'batch_time': 1.6508315060947552, 'data_time': 0.0031898054793303306}
2025-11-04 17:49:54 Train Epoch 018:  76%|███████▌  | 1900/2502 [52:18<16:33,  1.65s/it, Loss=3.6566, Top1=N/A, LR=0.095163]2025-11-04 17:52:40,049 - INFO - Step 46936: {'train_loss_batch': 3.146709442138672, 'train_lr': 0.09516284373130972, 'batch_time': 1.650825390873427, 'data_time': 0.0030745395166757294}
2025-11-04 17:52:40 Train Epoch 018:  80%|███████▉  | 2000/2502 [55:03<13:46,  1.65s/it, Loss=3.6584, Top1=N/A, LR=0.095163]2025-11-04 17:55:25,079 - INFO - Step 47036: {'train_loss_batch': 5.1501617431640625, 'train_lr': 0.09516284373130972, 'batch_time': 1.6507992880276476, 'data_time': 0.002968531260187777}
2025-11-04 17:55:25 Train Epoch 018:  84%|████████▍ | 2100/2502 [57:48<10:59,  1.64s/it, Loss=3.6590, Top1=N/A, LR=0.095163]2025-11-04 17:58:09,970 - INFO - Step 47136: {'train_loss_batch': 2.899782180786133, 'train_lr': 0.09516284373130972, 'batch_time': 1.650709466897891, 'data_time': 0.002874570480702549}
2025-11-04 17:58:09 Train Epoch 018:  88%|████████▊ | 2200/2502 [1:00:33<08:20,  1.66s/it, Loss=3.6518, Top1=N/A, LR=0.095163]2025-11-04 18:00:54,904 - INFO - Step 47236: {'train_loss_batch': 4.787261009216309, 'train_lr': 0.09516284373130972, 'batch_time': 1.6506470133639748, 'data_time': 0.0027875001188951533}
2025-11-04 18:00:54 Train Epoch 018:  92%|█████████▏| 2300/2502 [1:03:18<05:33,  1.65s/it, Loss=3.6526, Top1=N/A, LR=0.095163]2025-11-04 18:03:39,981 - INFO - Step 47336: {'train_loss_batch': 4.418255805969238, 'train_lr': 0.09516284373130972, 'batch_time': 1.6506521291496545, 'data_time': 0.0027096417612326967}
2025-11-04 18:03:39 Train Epoch 018:  96%|█████████▌| 2400/2502 [1:06:03<02:48,  1.65s/it, Loss=3.6514, Top1=52.66%, LR=0.095163]2025-11-04 18:06:24,996 - INFO - Step 47436: {'train_loss_batch': 2.99904203414917, 'train_lr': 0.09516284373130972, 'batch_time': 1.6506311067289632, 'data_time': 0.002639953020660642}
2025-11-04 18:06:24 Train Epoch 018: 100%|█████████▉| 2500/2502 [1:08:48<00:03,  1.66s/it, Loss=3.6529, Top1=N/A, LR=0.095163]   2025-11-04 18:09:10,065 - INFO - Step 47536: {'train_loss_batch': 4.482889175415039, 'train_lr': 0.09516284373130972, 'batch_time': 1.650633505371655, 'data_time': 0.002594292997980251}
2025-11-04 18:09:10 Train Epoch 018: 100%|██████████| 2502/2502 [1:08:50<00:00,  1.65s/it, Loss=3.6529, Top1=N/A, LR=0.095163]
2025-11-04 18:09:12 Val Epoch 018:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 18:09:16   with torch.cuda.amp.autocast():
2025-11-04 18:09:17 Val Epoch 018: 100%|██████████| 98/98 [01:51<00:00,  1.13s/it, Loss=2.7857, Top1=56.31%, Top5=80.99%]
2025-11-04 18:11:03 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 18:11:03   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 18:11:03 2025-11-04 18:11:03,547 - INFO - Step 18: {'epoch': 18, 'learning_rate': 0.0943392504275735, 'train_loss': 3.6534779376739697, 'train_top1': 52.64644695881226, 'train_top5': 76.4371557710728, 'train_precision': 52.266030209010204, 'train_recall': 52.48108723475975, 'train_f1': 52.09321829528856, 'val_loss': 2.7857415947723387, 'val_top1': 56.31399999145508, 'val_top5': 80.98999998535156, 'val_precision': 60.120942894147255, 'val_recall': 56.322, 'val_f1': 55.42813207366596}
2025-11-04 18:11:03 2025-11-04 18:11:03,548 - INFO - Epoch 018 Summary - LR: 0.094339, Train Loss: 3.6535, Val Loss: 2.7857, Val F1: 55.43%, Val Precision: 60.12%, Val Recall: 56.32%
2025-11-04 18:11:06 2025-11-04 18:11:06,533 - INFO - New best model saved with validation accuracy: 56.314%
2025-11-04 18:11:06 2025-11-04 18:11:06,534 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_019.pth
2025-11-04 18:11:06 Train Epoch 019:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 18 that is less than the current step 47536. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 18:11:10 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 18:11:10   with torch.cuda.amp.autocast():
2025-11-04 18:11:11 Train Epoch 019:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.3929, Top1=N/A, LR=0.094339]2025-11-04 18:11:11,710 - INFO - Step 47538: {'train_loss_batch': 4.392900466918945, 'train_lr': 0.0943392504275735, 'batch_time': 5.174518346786499, 'data_time': 3.5452792644500732}
2025-11-04 18:11:11 Train Epoch 019:   4%|▍         | 100/2502 [02:49<1:05:36,  1.64s/it, Loss=3.7401, Top1=N/A, LR=0.094339]2025-11-04 18:13:55,590 - INFO - Step 47638: {'train_loss_batch': 2.9902663230895996, 'train_lr': 0.0943392504275735, 'batch_time': 1.6738124318642191, 'data_time': 0.0360873028783515}
2025-11-04 18:13:55 Train Epoch 019:   8%|▊         | 200/2502 [05:33<1:03:11,  1.65s/it, Loss=3.6941, Top1=N/A, LR=0.094339]2025-11-04 18:16:40,163 - INFO - Step 47738: {'train_loss_batch': 3.8689751625061035, 'train_lr': 0.0943392504275735, 'batch_time': 1.659839475925882, 'data_time': 0.018615972936449954}
2025-11-04 18:16:40 Train Epoch 019:  12%|█▏        | 300/2502 [08:18<1:00:34,  1.65s/it, Loss=3.6517, Top1=N/A, LR=0.094339]2025-11-04 18:19:24,918 - INFO - Step 47838: {'train_loss_batch': 4.369353294372559, 'train_lr': 0.0943392504275735, 'batch_time': 1.6557549495633654, 'data_time': 0.012764275668071354}
2025-11-04 18:19:24 Train Epoch 019:  16%|█▌        | 400/2502 [11:03<57:38,  1.65s/it, Loss=3.6467, Top1=N/A, LR=0.094339]2025-11-04 18:22:09,859 - INFO - Step 47938: {'train_loss_batch': 3.0318796634674072, 'train_lr': 0.0943392504275735, 'batch_time': 1.6541728872313464, 'data_time': 0.00982942545503155}
2025-11-04 18:22:09 Train Epoch 019:  20%|█▉        | 500/2502 [13:48<54:57,  1.65s/it, Loss=3.6505, Top1=53.33%, LR=0.094339]2025-11-04 18:24:54,760 - INFO - Step 48038: {'train_loss_batch': 2.881641149520874, 'train_lr': 0.0943392504275735, 'batch_time': 1.6531418412031529, 'data_time': 0.0080592108819775}
2025-11-04 18:24:54 Train Epoch 019:  24%|██▍       | 600/2502 [16:33<52:20,  1.65s/it, Loss=3.6479, Top1=53.18%, LR=0.094339]2025-11-04 18:27:39,708 - INFO - Step 48138: {'train_loss_batch': 2.8954219818115234, 'train_lr': 0.0943392504275735, 'batch_time': 1.6525314032733935, 'data_time': 0.006882064552751436}
2025-11-04 18:27:39 Train Epoch 019:  28%|██▊       | 700/2502 [19:17<49:19,  1.64s/it, Loss=3.6427, Top1=N/A, LR=0.094339]   2025-11-04 18:30:24,225 - INFO - Step 48238: {'train_loss_batch': 3.8130440711975098, 'train_lr': 0.0943392504275735, 'batch_time': 1.6514809549960192, 'data_time': 0.006046266256487488}
2025-11-04 18:30:24 Train Epoch 019:  32%|███▏      | 800/2502 [22:02<46:59,  1.66s/it, Loss=3.6176, Top1=N/A, LR=0.094339]2025-11-04 18:33:08,900 - INFO - Step 48338: {'train_loss_batch': 2.934831142425537, 'train_lr': 0.0943392504275735, 'batch_time': 1.6508903943941686, 'data_time': 0.005414070410377226}
2025-11-04 18:33:08 Train Epoch 019:  36%|███▌      | 900/2502 [24:47<44:03,  1.65s/it, Loss=3.6287, Top1=N/A, LR=0.094339]2025-11-04 18:35:53,860 - INFO - Step 48438: {'train_loss_batch': 4.712851524353027, 'train_lr': 0.0943392504275735, 'batch_time': 1.6507473452374355, 'data_time': 0.004928203322382005}
2025-11-04 18:35:53 Train Epoch 019:  40%|███▉      | 1000/2502 [27:32<41:22,  1.65s/it, Loss=3.6305, Top1=53.02%, LR=0.094339]2025-11-04 18:38:38,807 - INFO - Step 48538: {'train_loss_batch': 2.921743869781494, 'train_lr': 0.0943392504275735, 'batch_time': 1.6506196271170388, 'data_time': 0.004536170464057427}
2025-11-04 18:38:38 Train Epoch 019:  44%|████▍     | 1100/2502 [30:16<38:26,  1.65s/it, Loss=3.6305, Top1=N/A, LR=0.094339]   2025-11-04 18:41:23,528 - INFO - Step 48638: {'train_loss_batch': 4.545525074005127, 'train_lr': 0.0943392504275735, 'batch_time': 1.6503093747200477, 'data_time': 0.0042137608108035445}
2025-11-04 18:41:23 Train Epoch 019:  48%|████▊     | 1200/2502 [33:01<35:45,  1.65s/it, Loss=3.6393, Top1=53.04%, LR=0.094339]2025-11-04 18:44:08,424 - INFO - Step 48738: {'train_loss_batch': 2.833808660507202, 'train_lr': 0.0943392504275735, 'batch_time': 1.650196601310241, 'data_time': 0.003948351624208525}
2025-11-04 18:44:08 Train Epoch 019:  52%|█████▏    | 1300/2502 [35:46<32:49,  1.64s/it, Loss=3.6419, Top1=N/A, LR=0.094339]   2025-11-04 18:46:53,156 - INFO - Step 48838: {'train_loss_batch': 2.7931289672851562, 'train_lr': 0.0943392504275735, 'batch_time': 1.6499759969484065, 'data_time': 0.0037196490693147326}
2025-11-04 18:46:53 Train Epoch 019:  56%|█████▌    | 1400/2502 [38:31<30:26,  1.66s/it, Loss=3.6514, Top1=N/A, LR=0.094339]2025-11-04 18:49:37,795 - INFO - Step 48938: {'train_loss_batch': 4.171210765838623, 'train_lr': 0.0943392504275735, 'batch_time': 1.64971974745212, 'data_time': 0.003523113725867806}
2025-11-04 18:49:37 Train Epoch 019:  60%|█████▉    | 1500/2502 [41:16<27:38,  1.66s/it, Loss=3.6431, Top1=53.10%, LR=0.094339]2025-11-04 18:52:23,085 - INFO - Step 49038: {'train_loss_batch': 2.9237112998962402, 'train_lr': 0.0943392504275735, 'batch_time': 1.6499311464933615, 'data_time': 0.003349737514264579}
2025-11-04 18:52:23 Train Epoch 019:  64%|██████▍   | 1600/2502 [44:01<24:49,  1.65s/it, Loss=3.6369, Top1=N/A, LR=0.094339]   2025-11-04 18:55:08,339 - INFO - Step 49138: {'train_loss_batch': 4.709977626800537, 'train_lr': 0.0943392504275735, 'batch_time': 1.6500938931976834, 'data_time': 0.0032038310406581824}
2025-11-04 18:55:08 Train Epoch 019:  68%|██████▊   | 1700/2502 [46:46<21:58,  1.64s/it, Loss=3.6338, Top1=53.03%, LR=0.094339]2025-11-04 18:57:53,374 - INFO - Step 49238: {'train_loss_batch': 3.0059971809387207, 'train_lr': 0.0943392504275735, 'batch_time': 1.6501089835573406, 'data_time': 0.003075830379981703}
2025-11-04 18:57:53 Train Epoch 019:  72%|███████▏  | 1800/2502 [49:31<19:20,  1.65s/it, Loss=3.6265, Top1=53.05%, LR=0.094339]2025-11-04 19:00:37,888 - INFO - Step 49338: {'train_loss_batch': 2.8222804069519043, 'train_lr': 0.0943392504275735, 'batch_time': 1.6498330928827907, 'data_time': 0.0029565982193764154}
2025-11-04 19:00:37 Train Epoch 019:  76%|███████▌  | 1900/2502 [52:15<16:22,  1.63s/it, Loss=3.6212, Top1=N/A, LR=0.094339]   2025-11-04 19:03:22,299 - INFO - Step 49438: {'train_loss_batch': 4.632390975952148, 'train_lr': 0.0943392504275735, 'batch_time': 1.649531869998673, 'data_time': 0.0028488858005738396}
2025-11-04 19:03:22 Train Epoch 019:  80%|███████▉  | 2000/2502 [55:00<13:46,  1.65s/it, Loss=3.6253, Top1=N/A, LR=0.094339]2025-11-04 19:06:06,743 - INFO - Step 49538: {'train_loss_batch': 2.813389301300049, 'train_lr': 0.0943392504275735, 'batch_time': 1.649277366321722, 'data_time': 0.0027562592519276862}
2025-11-04 19:06:06 Train Epoch 019:  84%|████████▍ | 2100/2502 [57:45<11:00,  1.64s/it, Loss=3.6290, Top1=53.05%, LR=0.094339]2025-11-04 19:08:51,568 - INFO - Step 49638: {'train_loss_batch': 2.8969802856445312, 'train_lr': 0.0943392504275735, 'batch_time': 1.649228371988303, 'data_time': 0.0026744402686168555}
2025-11-04 19:08:51 Train Epoch 019:  88%|████████▊ | 2200/2502 [1:00:29<08:16,  1.64s/it, Loss=3.6260, Top1=N/A, LR=0.094339]   2025-11-04 19:11:36,167 - INFO - Step 49738: {'train_loss_batch': 3.2898435592651367, 'train_lr': 0.0943392504275735, 'batch_time': 1.6490810217504228, 'data_time': 0.0025974697873896763}
2025-11-04 19:11:36 Train Epoch 019:  92%|█████████▏| 2300/2502 [1:03:14<05:31,  1.64s/it, Loss=3.6284, Top1=N/A, LR=0.094339]2025-11-04 19:14:20,607 - INFO - Step 49838: {'train_loss_batch': 4.3262834548950195, 'train_lr': 0.0943392504275735, 'batch_time': 1.6488777371605081, 'data_time': 0.0025243606840926946}
2025-11-04 19:14:20 Train Epoch 019:  96%|█████████▌| 2400/2502 [1:05:58<02:48,  1.65s/it, Loss=3.6267, Top1=N/A, LR=0.094339]2025-11-04 19:17:05,308 - INFO - Step 49938: {'train_loss_batch': 4.6291656494140625, 'train_lr': 0.0943392504275735, 'batch_time': 1.6487997273314452, 'data_time': 0.0024615691533340905}
2025-11-04 19:17:05 Train Epoch 019: 100%|█████████▉| 2500/2502 [1:08:43<00:03,  1.65s/it, Loss=3.6305, Top1=N/A, LR=0.094339]2025-11-04 19:19:50,053 - INFO - Step 50038: {'train_loss_batch': 2.6918768882751465, 'train_lr': 0.0943392504275735, 'batch_time': 1.648745529082526, 'data_time': 0.002422285575668414}
2025-11-04 19:19:50 Train Epoch 019: 100%|██████████| 2502/2502 [1:08:45<00:00,  1.65s/it, Loss=3.6305, Top1=N/A, LR=0.094339]
2025-11-04 19:19:52 Val Epoch 019:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 19:19:56   with torch.cuda.amp.autocast():
2025-11-04 19:19:57 Val Epoch 019: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=2.7767, Top1=54.99%, Top5=80.02%]
2025-11-04 19:21:43 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 19:21:43   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 19:21:43 2025-11-04 19:21:43,061 - INFO - Step 19: {'epoch': 19, 'learning_rate': 0.09345510187054488, 'train_loss': 3.630904697781082, 'train_top1': 53.038330078125, 'train_top5': 76.8017578125, 'train_precision': 52.65293818339533, 'train_recall': 52.84775718390681, 'train_f1': 52.48853986102237, 'val_loss': 2.7767269638824463, 'val_top1': 54.99400000976563, 'val_top5': 80.02400000976563, 'val_precision': 60.36291380954656, 'val_recall': 54.99400000000001, 'val_f1': 54.45556908669496}
2025-11-04 19:21:43 2025-11-04 19:21:43,063 - INFO - Epoch 019 Summary - LR: 0.093455, Train Loss: 3.6309, Val Loss: 2.7767, Val F1: 54.46%, Val Precision: 60.36%, Val Recall: 54.99%
2025-11-04 19:21:45 2025-11-04 19:21:45,011 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_020.pth
2025-11-04 19:21:45 Train Epoch 020:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 19 that is less than the current step 50038. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 19:21:49 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 19:21:49   with torch.cuda.amp.autocast():
2025-11-04 19:21:50 Train Epoch 020:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.7521, Top1=N/A, LR=0.093455]2025-11-04 19:21:50,863 - INFO - Step 50040: {'train_loss_batch': 2.7520899772644043, 'train_lr': 0.09345510187054488, 'batch_time': 5.849397659301758, 'data_time': 4.21101450920105}
2025-11-04 19:21:50 Train Epoch 020:   4%|▍         | 100/2502 [02:49<1:05:41,  1.64s/it, Loss=3.4574, Top1=N/A, LR=0.093455]2025-11-04 19:24:34,754 - INFO - Step 50140: {'train_loss_batch': 3.0231943130493164, 'train_lr': 0.09345510187054488, 'batch_time': 1.680606705127376, 'data_time': 0.04263060635859423}
2025-11-04 19:24:34 Train Epoch 020:   8%|▊         | 200/2502 [05:34<1:02:54,  1.64s/it, Loss=3.5605, Top1=54.60%, LR=0.093455]2025-11-04 19:27:19,133 - INFO - Step 50240: {'train_loss_batch': 3.010194778442383, 'train_lr': 0.09345510187054488, 'batch_time': 1.662291267024937, 'data_time': 0.021890458775990045}
2025-11-04 19:27:19 Train Epoch 020:  12%|█▏        | 300/2502 [08:18<1:00:30,  1.65s/it, Loss=3.6017, Top1=N/A, LR=0.093455]   2025-11-04 19:30:03,519 - INFO - Step 50340: {'train_loss_batch': 3.3806333541870117, 'train_lr': 0.09345510187054488, 'batch_time': 1.6561665028036632, 'data_time': 0.014928406655194355}
2025-11-04 19:30:03 Train Epoch 020:  16%|█▌        | 400/2502 [11:03<57:45,  1.65s/it, Loss=3.6188, Top1=N/A, LR=0.093455]2025-11-04 19:32:48,117 - INFO - Step 50440: {'train_loss_batch': 4.565343856811523, 'train_lr': 0.09345510187054488, 'batch_time': 1.6536267689635926, 'data_time': 0.011429953753502291}
2025-11-04 19:32:48 Train Epoch 020:  20%|█▉        | 500/2502 [13:48<55:01,  1.65s/it, Loss=3.6041, Top1=54.06%, LR=0.093455]2025-11-04 19:35:33,140 - INFO - Step 50540: {'train_loss_batch': 3.026425838470459, 'train_lr': 0.09345510187054488, 'batch_time': 1.6529470122978835, 'data_time': 0.009343347625580138}
2025-11-04 19:35:33 Train Epoch 020:  24%|██▍       | 600/2502 [16:33<52:13,  1.65s/it, Loss=3.6167, Top1=N/A, LR=0.093455]   2025-11-04 19:38:18,207 - INFO - Step 50640: {'train_loss_batch': 4.673576354980469, 'train_lr': 0.09345510187054488, 'batch_time': 1.6525688167419688, 'data_time': 0.007953364519826981}
2025-11-04 19:38:18 Train Epoch 020:  28%|██▊       | 700/2502 [19:17<49:18,  1.64s/it, Loss=3.5990, Top1=N/A, LR=0.093455]2025-11-04 19:41:02,768 - INFO - Step 50740: {'train_loss_batch': 3.0993361473083496, 'train_lr': 0.09345510187054488, 'batch_time': 1.651575424191615, 'data_time': 0.006955496084673088}
2025-11-04 19:41:02 Train Epoch 020:  32%|███▏      | 800/2502 [22:01<46:45,  1.65s/it, Loss=3.5971, Top1=N/A, LR=0.093455]2025-11-04 19:43:46,863 - INFO - Step 50840: {'train_loss_batch': 2.8633646965026855, 'train_lr': 0.09345510187054488, 'batch_time': 1.6502476961871657, 'data_time': 0.006210022055998575}
2025-11-04 19:43:46 Train Epoch 020:  36%|███▌      | 900/2502 [24:46<44:12,  1.66s/it, Loss=3.5909, Top1=N/A, LR=0.093455]2025-11-04 19:46:31,699 - INFO - Step 50940: {'train_loss_batch': 2.847470760345459, 'train_lr': 0.09345510187054488, 'batch_time': 1.65003853368177, 'data_time': 0.005625510983144271}
2025-11-04 19:46:31 Train Epoch 020:  40%|███▉      | 1000/2502 [27:31<41:23,  1.65s/it, Loss=3.5995, Top1=N/A, LR=0.093455]2025-11-04 19:49:16,689 - INFO - Step 51040: {'train_loss_batch': 2.923825263977051, 'train_lr': 0.09345510187054488, 'batch_time': 1.6500245205291382, 'data_time': 0.005153233235651677}
2025-11-04 19:49:16 Train Epoch 020:  44%|████▍     | 1100/2502 [30:16<38:30,  1.65s/it, Loss=3.5991, Top1=N/A, LR=0.093455]2025-11-04 19:52:01,487 - INFO - Step 51140: {'train_loss_batch': 4.458524703979492, 'train_lr': 0.09345510187054488, 'batch_time': 1.6498388434192681, 'data_time': 0.004778431503476066}
2025-11-04 19:52:01 Train Epoch 020:  48%|████▊     | 1200/2502 [33:01<35:47,  1.65s/it, Loss=3.6024, Top1=N/A, LR=0.093455]2025-11-04 19:54:46,315 - INFO - Step 51240: {'train_loss_batch': 4.169134140014648, 'train_lr': 0.09345510187054488, 'batch_time': 1.649708887222506, 'data_time': 0.004461228897132841}
2025-11-04 19:54:46 Train Epoch 020:  52%|█████▏    | 1300/2502 [35:46<33:05,  1.65s/it, Loss=3.5990, Top1=53.89%, LR=0.093455]2025-11-04 19:57:31,681 - INFO - Step 51340: {'train_loss_batch': 2.906897783279419, 'train_lr': 0.09345510187054488, 'batch_time': 1.6500124126833096, 'data_time': 0.004203419974911314}
2025-11-04 19:57:31 Train Epoch 020:  56%|█████▌    | 1400/2502 [38:31<30:23,  1.65s/it, Loss=3.5894, Top1=N/A, LR=0.093455]   2025-11-04 20:00:16,720 - INFO - Step 51440: {'train_loss_batch': 2.971224308013916, 'train_lr': 0.09345510187054488, 'batch_time': 1.6500386483834353, 'data_time': 0.003977298055862546}
2025-11-04 20:00:16 Train Epoch 020:  60%|█████▉    | 1500/2502 [41:16<27:29,  1.65s/it, Loss=3.5869, Top1=N/A, LR=0.093455]2025-11-04 20:03:01,567 - INFO - Step 51540: {'train_loss_batch': 3.8782715797424316, 'train_lr': 0.09345510187054488, 'batch_time': 1.6499342756379374, 'data_time': 0.0037809824006387505}
2025-11-04 20:03:01 Train Epoch 020:  64%|██████▍   | 1600/2502 [44:01<24:47,  1.65s/it, Loss=3.5873, Top1=N/A, LR=0.093455]2025-11-04 20:05:46,677 - INFO - Step 51640: {'train_loss_batch': 4.187135696411133, 'train_lr': 0.09345510187054488, 'batch_time': 1.65000717406121, 'data_time': 0.003607720602012887}
2025-11-04 20:05:46 Train Epoch 020:  68%|██████▊   | 1700/2502 [46:46<22:00,  1.65s/it, Loss=3.5890, Top1=53.70%, LR=0.093455]2025-11-04 20:08:31,375 - INFO - Step 51740: {'train_loss_batch': 2.9706711769104004, 'train_lr': 0.09345510187054488, 'batch_time': 1.6498290062791667, 'data_time': 0.003455377900551376}
2025-11-04 20:08:31 Train Epoch 020:  72%|███████▏  | 1800/2502 [49:31<19:16,  1.65s/it, Loss=3.5925, Top1=N/A, LR=0.093455]   2025-11-04 20:11:16,200 - INFO - Step 51840: {'train_loss_batch': 4.752833366394043, 'train_lr': 0.09345510187054488, 'batch_time': 1.649740852825116, 'data_time': 0.0033185754730461836}
2025-11-04 20:11:16 Train Epoch 020:  76%|███████▌  | 1900/2502 [52:15<16:29,  1.64s/it, Loss=3.5890, Top1=53.63%, LR=0.093455]2025-11-04 20:14:00,965 - INFO - Step 51940: {'train_loss_batch': 2.937870979309082, 'train_lr': 0.09345510187054488, 'batch_time': 1.6496310911324072, 'data_time': 0.003195217444606984}
2025-11-04 20:14:00 Train Epoch 020:  80%|███████▉  | 2000/2502 [55:00<13:49,  1.65s/it, Loss=3.5926, Top1=N/A, LR=0.093455]   2025-11-04 20:16:45,531 - INFO - Step 52040: {'train_loss_batch': 3.5695607662200928, 'train_lr': 0.09345510187054488, 'batch_time': 1.6494323148064944, 'data_time': 0.0030869671489404836}
2025-11-04 20:16:45 Train Epoch 020:  84%|████████▍ | 2100/2502 [57:45<11:04,  1.65s/it, Loss=3.5935, Top1=N/A, LR=0.093455]2025-11-04 20:19:30,142 - INFO - Step 52140: {'train_loss_batch': 2.8804726600646973, 'train_lr': 0.09345510187054488, 'batch_time': 1.649274152327242, 'data_time': 0.002987755530791757}
2025-11-04 20:19:30 Train Epoch 020:  88%|████████▊ | 2200/2502 [1:00:29<08:19,  1.66s/it, Loss=3.5916, Top1=N/A, LR=0.093455]2025-11-04 20:22:14,958 - INFO - Step 52240: {'train_loss_batch': 3.5380373001098633, 'train_lr': 0.09345510187054488, 'batch_time': 1.6492232811662189, 'data_time': 0.002897864414962082}
2025-11-04 20:22:14 Train Epoch 020:  92%|█████████▏| 2300/2502 [1:03:14<05:33,  1.65s/it, Loss=3.5967, Top1=N/A, LR=0.093455]2025-11-04 20:24:59,916 - INFO - Step 52340: {'train_loss_batch': 3.0772697925567627, 'train_lr': 0.09345510187054488, 'batch_time': 1.6492386020716767, 'data_time': 0.0028183326571985514}
2025-11-04 20:24:59 Train Epoch 020:  96%|█████████▌| 2400/2502 [1:05:59<02:47,  1.64s/it, Loss=3.6001, Top1=N/A, LR=0.093455]2025-11-04 20:27:44,712 - INFO - Step 52440: {'train_loss_batch': 4.604276180267334, 'train_lr': 0.09345510187054488, 'batch_time': 1.6491853773767677, 'data_time': 0.002742438354079101}
2025-11-04 20:27:44 Train Epoch 020: 100%|█████████▉| 2500/2502 [1:08:44<00:03,  1.66s/it, Loss=3.6019, Top1=N/A, LR=0.093455]2025-11-04 20:30:29,568 - INFO - Step 52540: {'train_loss_batch': 2.9212284088134766, 'train_lr': 0.09345510187054488, 'batch_time': 1.6491603080104322, 'data_time': 0.002717592867790628}
2025-11-04 20:30:29 Train Epoch 020: 100%|██████████| 2502/2502 [1:08:46<00:00,  1.65s/it, Loss=3.6019, Top1=N/A, LR=0.093455]
2025-11-04 20:30:31 Val Epoch 020:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 20:30:36   with torch.cuda.amp.autocast():
2025-11-04 20:30:36 Val Epoch 020: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.6314, Top1=58.10%, Top5=82.42%]
2025-11-04 20:32:23 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 20:32:23   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 20:32:23 2025-11-04 20:32:23,319 - INFO - Step 20: {'epoch': 20, 'learning_rate': 0.09251160570080207, 'train_loss': 3.6016803173710117, 'train_top1': 53.44868321572581, 'train_top5': 77.02242943548387, 'train_precision': 53.06894726590925, 'train_recall': 53.29991890875575, 'train_f1': 52.914324009601856, 'val_loss': 2.631418472442627, 'val_top1': 58.09600001708984, 'val_top5': 82.42199998779297, 'val_precision': 62.131514257358724, 'val_recall': 58.094, 'val_f1': 57.58505757490643}
2025-11-04 20:32:23 2025-11-04 20:32:23,320 - INFO - Epoch 020 Summary - LR: 0.092512, Train Loss: 3.6017, Val Loss: 2.6314, Val F1: 57.59%, Val Precision: 62.13%, Val Recall: 58.09%
2025-11-04 20:32:26 2025-11-04 20:32:26,339 - INFO - New best model saved with validation accuracy: 58.096%
2025-11-04 20:32:26 2025-11-04 20:32:26,339 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_021.pth
2025-11-04 20:32:26 Train Epoch 021:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 20 that is less than the current step 52540. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 20:32:30 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 20:32:30   with torch.cuda.amp.autocast():
2025-11-04 20:32:31 Train Epoch 021:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.7541, Top1=N/A, LR=0.092512]2025-11-04 20:32:31,793 - INFO - Step 52542: {'train_loss_batch': 3.7540717124938965, 'train_lr': 0.09251160570080207, 'batch_time': 5.452591419219971, 'data_time': 3.8177151679992676}
2025-11-04 20:32:31 Train Epoch 021:   4%|▍         | 100/2502 [02:49<1:05:43,  1.64s/it, Loss=3.5116, Top1=N/A, LR=0.092512]2025-11-04 20:35:15,939 - INFO - Step 52642: {'train_loss_batch': 4.533377647399902, 'train_lr': 0.09251160570080207, 'batch_time': 1.6791964356261906, 'data_time': 0.03882058068077163}
2025-11-04 20:35:15 Train Epoch 021:   8%|▊         | 200/2502 [05:34<1:03:16,  1.65s/it, Loss=3.5958, Top1=N/A, LR=0.092512]2025-11-04 20:38:00,612 - INFO - Step 52742: {'train_loss_batch': 4.5537190437316895, 'train_lr': 0.09251160570080207, 'batch_time': 1.6630406118744048, 'data_time': 0.019994440363414252}
2025-11-04 20:38:00 Train Epoch 021:  12%|█▏        | 300/2502 [08:18<1:00:20,  1.64s/it, Loss=3.5572, Top1=53.93%, LR=0.092512]2025-11-04 20:40:45,083 - INFO - Step 52842: {'train_loss_batch': 2.888519525527954, 'train_lr': 0.09251160570080207, 'batch_time': 1.656950085662132, 'data_time': 0.013661322799631923}
2025-11-04 20:40:45 Train Epoch 021:  16%|█▌        | 400/2502 [11:03<57:39,  1.65s/it, Loss=3.5811, Top1=N/A, LR=0.092512]   2025-11-04 20:43:30,041 - INFO - Step 52942: {'train_loss_batch': 4.708461761474609, 'train_lr': 0.09251160570080207, 'batch_time': 1.6551119734224238, 'data_time': 0.01049326304485673}
2025-11-04 20:43:30 Train Epoch 021:  20%|█▉        | 500/2502 [13:48<54:57,  1.65s/it, Loss=3.5775, Top1=N/A, LR=0.092512]2025-11-04 20:46:15,017 - INFO - Step 53042: {'train_loss_batch': 4.56851053237915, 'train_lr': 0.09251160570080207, 'batch_time': 1.654042523302242, 'data_time': 0.008599753865224873}
2025-11-04 20:46:15 Train Epoch 021:  24%|██▍       | 600/2502 [16:33<52:21,  1.65s/it, Loss=3.5765, Top1=N/A, LR=0.092512]2025-11-04 20:48:59,667 - INFO - Step 53142: {'train_loss_batch': 3.0389575958251953, 'train_lr': 0.09251160570080207, 'batch_time': 1.65278717444066, 'data_time': 0.007335315726561078}
2025-11-04 20:48:59 Train Epoch 021:  28%|██▊       | 700/2502 [19:17<49:35,  1.65s/it, Loss=3.5647, Top1=53.89%, LR=0.092512]2025-11-04 20:51:44,275 - INFO - Step 53242: {'train_loss_batch': 2.956920623779297, 'train_lr': 0.09251160570080207, 'batch_time': 1.651831318410419, 'data_time': 0.006434104241930299}
2025-11-04 20:51:44 Train Epoch 021:  32%|███▏      | 800/2502 [22:02<46:35,  1.64s/it, Loss=3.5784, Top1=N/A, LR=0.092512]   2025-11-04 20:54:28,687 - INFO - Step 53342: {'train_loss_batch': 4.371969223022461, 'train_lr': 0.09251160570080207, 'batch_time': 1.6508679074443384, 'data_time': 0.005750818050160688}
2025-11-04 20:54:28 Train Epoch 021:  36%|███▌      | 900/2502 [24:47<44:15,  1.66s/it, Loss=3.5766, Top1=N/A, LR=0.092512]2025-11-04 20:57:13,685 - INFO - Step 53442: {'train_loss_batch': 4.298332691192627, 'train_lr': 0.09251160570080207, 'batch_time': 1.6507689688234826, 'data_time': 0.00522061405118377}
2025-11-04 20:57:13 Train Epoch 021:  40%|███▉      | 1000/2502 [27:32<41:14,  1.65s/it, Loss=3.5703, Top1=N/A, LR=0.092512]2025-11-04 20:59:58,818 - INFO - Step 53542: {'train_loss_batch': 3.164212465286255, 'train_lr': 0.09251160570080207, 'batch_time': 1.650825055805477, 'data_time': 0.0048007622108116495}
2025-11-04 20:59:58 Train Epoch 021:  44%|████▍     | 1100/2502 [30:17<38:34,  1.65s/it, Loss=3.5655, Top1=N/A, LR=0.092512]2025-11-04 21:02:44,025 - INFO - Step 53642: {'train_loss_batch': 2.794894218444824, 'train_lr': 0.09251160570080207, 'batch_time': 1.6509373751041783, 'data_time': 0.00445919379009971}
2025-11-04 21:02:44 Train Epoch 021:  48%|████▊     | 1200/2502 [33:02<35:40,  1.64s/it, Loss=3.5702, Top1=N/A, LR=0.092512]2025-11-04 21:05:28,897 - INFO - Step 53742: {'train_loss_batch': 4.39898157119751, 'train_lr': 0.09251160570080207, 'batch_time': 1.6507524913991123, 'data_time': 0.004173651821508098}
2025-11-04 21:05:28 Train Epoch 021:  52%|█████▏    | 1300/2502 [35:47<33:05,  1.65s/it, Loss=3.5631, Top1=53.74%, LR=0.092512]2025-11-04 21:08:13,482 - INFO - Step 53842: {'train_loss_batch': 2.8584494590759277, 'train_lr': 0.09251160570080207, 'batch_time': 1.6503754657199985, 'data_time': 0.003930362163370706}
2025-11-04 21:08:13 Train Epoch 021:  56%|█████▌    | 1400/2502 [38:32<30:16,  1.65s/it, Loss=3.5604, Top1=N/A, LR=0.092512]   2025-11-04 21:10:58,358 - INFO - Step 53942: {'train_loss_batch': 2.8133292198181152, 'train_lr': 0.09251160570080207, 'batch_time': 1.6502597806795762, 'data_time': 0.0037217126583559525}
2025-11-04 21:10:58 Train Epoch 021:  60%|█████▉    | 1500/2502 [41:16<27:12,  1.63s/it, Loss=3.5598, Top1=N/A, LR=0.092512]2025-11-04 21:13:42,864 - INFO - Step 54042: {'train_loss_batch': 3.1950266361236572, 'train_lr': 0.09251160570080207, 'batch_time': 1.6499137322478576, 'data_time': 0.003546372482889418}
2025-11-04 21:13:42 Train Epoch 021:  64%|██████▍   | 1600/2502 [44:00<24:51,  1.65s/it, Loss=3.5656, Top1=N/A, LR=0.092512]2025-11-04 21:16:26,937 - INFO - Step 54142: {'train_loss_batch': 3.421717405319214, 'train_lr': 0.09251160570080207, 'batch_time': 1.6493397250464379, 'data_time': 0.0033836862134010177}
2025-11-04 21:16:26 Train Epoch 021:  68%|██████▊   | 1700/2502 [46:45<21:56,  1.64s/it, Loss=3.5690, Top1=N/A, LR=0.092512]2025-11-04 21:19:11,848 - INFO - Step 54242: {'train_loss_batch': 3.1574597358703613, 'train_lr': 0.09251160570080207, 'batch_time': 1.6493264328094877, 'data_time': 0.0032424504864853594}
2025-11-04 21:19:11 Train Epoch 021:  72%|███████▏  | 1800/2502 [49:29<19:18,  1.65s/it, Loss=3.5614, Top1=N/A, LR=0.092512]2025-11-04 21:21:56,336 - INFO - Step 54342: {'train_loss_batch': 2.8339200019836426, 'train_lr': 0.09251160570080207, 'batch_time': 1.6490794484447202, 'data_time': 0.0031172779121377745}
2025-11-04 21:21:56 Train Epoch 021:  76%|███████▌  | 1900/2502 [52:14<16:28,  1.64s/it, Loss=3.5613, Top1=N/A, LR=0.092512]2025-11-04 21:24:40,851 - INFO - Step 54442: {'train_loss_batch': 4.307072639465332, 'train_lr': 0.09251160570080207, 'batch_time': 1.6488725754037021, 'data_time': 0.003006868272126191}
2025-11-04 21:24:40 Train Epoch 021:  80%|███████▉  | 2000/2502 [54:59<13:49,  1.65s/it, Loss=3.5635, Top1=N/A, LR=0.092512]2025-11-04 21:27:25,710 - INFO - Step 54542: {'train_loss_batch': 2.851529359817505, 'train_lr': 0.09251160570080207, 'batch_time': 1.6488581498463948, 'data_time': 0.002906236095704894}
2025-11-04 21:27:25 Train Epoch 021:  84%|████████▍ | 2100/2502 [57:43<11:01,  1.65s/it, Loss=3.5666, Top1=N/A, LR=0.092512]2025-11-04 21:30:10,299 - INFO - Step 54642: {'train_loss_batch': 2.94781494140625, 'train_lr': 0.09251160570080207, 'batch_time': 1.6487169305464586, 'data_time': 0.002814039283000759}
2025-11-04 21:30:10 Train Epoch 021:  88%|████████▊ | 2200/2502 [1:00:28<08:18,  1.65s/it, Loss=3.5677, Top1=N/A, LR=0.092512]2025-11-04 21:32:55,169 - INFO - Step 54742: {'train_loss_batch': 3.983119487762451, 'train_lr': 0.09251160570080207, 'batch_time': 1.6487156908277054, 'data_time': 0.0027336582494074516}
2025-11-04 21:32:55 Train Epoch 021:  92%|█████████▏| 2300/2502 [1:03:13<05:32,  1.64s/it, Loss=3.5660, Top1=N/A, LR=0.092512]2025-11-04 21:35:40,053 - INFO - Step 54842: {'train_loss_batch': 5.228838920593262, 'train_lr': 0.09251160570080207, 'batch_time': 1.648721292194, 'data_time': 0.0026594956301440884}
2025-11-04 21:35:40 Train Epoch 021:  96%|█████████▌| 2400/2502 [1:05:58<02:47,  1.64s/it, Loss=3.5663, Top1=N/A, LR=0.092512]2025-11-04 21:38:24,378 - INFO - Step 54942: {'train_loss_batch': 2.8213491439819336, 'train_lr': 0.09251160570080207, 'batch_time': 1.64849332291501, 'data_time': 0.0025886614488492853}
2025-11-04 21:38:24 Train Epoch 021: 100%|█████████▉| 2500/2502 [1:08:42<00:03,  1.64s/it, Loss=3.5666, Top1=N/A, LR=0.092512]2025-11-04 21:41:09,144 - INFO - Step 55042: {'train_loss_batch': 3.841671943664551, 'train_lr': 0.09251160570080207, 'batch_time': 1.6484601192596386, 'data_time': 0.002560787704266438}
2025-11-04 21:41:09 Train Epoch 021: 100%|██████████| 2502/2502 [1:08:44<00:00,  1.65s/it, Loss=3.5666, Top1=N/A, LR=0.092512]
2025-11-04 21:41:11 Val Epoch 021:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 21:41:15   with torch.cuda.amp.autocast():
2025-11-04 21:41:16 Val Epoch 021: 100%|██████████| 98/98 [01:52<00:00,  1.15s/it, Loss=2.6791, Top1=57.05%, Top5=81.60%]
2025-11-04 21:43:03 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 21:43:03   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 21:43:03 2025-11-04 21:43:03,994 - INFO - Step 21: {'epoch': 21, 'learning_rate': 0.09151005062062345, 'train_loss': 3.5665319473813955, 'train_top1': 53.56903698979592, 'train_top5': 77.27519132653062, 'train_precision': 53.18083826222265, 'train_recall': 53.401819226651824, 'train_f1': 53.03577021849028, 'val_loss': 2.679124786682129, 'val_top1': 57.05399998046875, 'val_top5': 81.6040000048828, 'val_precision': 61.65601997369965, 'val_recall': 57.06199999999999, 'val_f1': 56.75607908295001}
2025-11-04 21:43:04 2025-11-04 21:43:03,996 - INFO - Epoch 021 Summary - LR: 0.091510, Train Loss: 3.5665, Val Loss: 2.6791, Val F1: 56.76%, Val Precision: 61.66%, Val Recall: 57.06%
2025-11-04 21:43:05 Train Epoch 022:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 21 that is less than the current step 55042. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 21:43:09 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 21:43:09   with torch.cuda.amp.autocast():
2025-11-04 21:43:10 Train Epoch 022:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.2990, Top1=N/A, LR=0.091510]2025-11-04 21:43:10,765 - INFO - Step 55044: {'train_loss_batch': 4.298957824707031, 'train_lr': 0.09151005062062345, 'batch_time': 5.645658016204834, 'data_time': 3.996854782104492}
2025-11-04 21:43:10 Train Epoch 022:   4%|▍         | 100/2502 [02:50<1:05:47,  1.64s/it, Loss=3.5177, Top1=N/A, LR=0.091510]2025-11-04 21:45:55,577 - INFO - Step 55144: {'train_loss_batch': 3.032759666442871, 'train_lr': 0.09151005062062345, 'batch_time': 1.6877007932946233, 'data_time': 0.04061259137521876}
2025-11-04 21:45:55 Train Epoch 022:   8%|▊         | 200/2502 [05:35<1:03:30,  1.66s/it, Loss=3.5572, Top1=N/A, LR=0.091510]2025-11-04 21:48:40,660 - INFO - Step 55244: {'train_loss_batch': 4.471231937408447, 'train_lr': 0.09151005062062345, 'batch_time': 1.6693576497225027, 'data_time': 0.020919974170514007}
2025-11-04 21:48:40 Train Epoch 022:  12%|█▏        | 300/2502 [08:20<1:00:31,  1.65s/it, Loss=3.5237, Top1=N/A, LR=0.091510]2025-11-04 21:51:25,905 - INFO - Step 55344: {'train_loss_batch': 5.024782180786133, 'train_lr': 0.09151005062062345, 'batch_time': 1.6637394111417854, 'data_time': 0.014282022995806215}
2025-11-04 21:51:25 Train Epoch 022:  16%|█▌        | 400/2502 [11:06<58:05,  1.66s/it, Loss=3.5333, Top1=N/A, LR=0.091510]2025-11-04 21:54:11,481 - INFO - Step 55444: {'train_loss_batch': 5.253305912017822, 'train_lr': 0.09151005062062345, 'batch_time': 1.661749667360301, 'data_time': 0.010997279326517386}
2025-11-04 21:54:11 Train Epoch 022:  20%|█▉        | 500/2502 [13:51<54:38,  1.64s/it, Loss=3.5713, Top1=N/A, LR=0.091510]2025-11-04 21:56:56,573 - INFO - Step 55544: {'train_loss_batch': 4.575531005859375, 'train_lr': 0.09151005062062345, 'batch_time': 1.6595878758116396, 'data_time': 0.008998338333860843}
2025-11-04 21:56:56 Train Epoch 022:  24%|██▍       | 600/2502 [16:36<52:02,  1.64s/it, Loss=3.5582, Top1=N/A, LR=0.091510]2025-11-04 21:59:41,332 - INFO - Step 55644: {'train_loss_batch': 2.944199800491333, 'train_lr': 0.09151005062062345, 'batch_time': 1.6575908105504296, 'data_time': 0.0076656333618671845}
2025-11-04 21:59:41 Train Epoch 022:  28%|██▊       | 700/2502 [19:22<49:42,  1.66s/it, Loss=3.5595, Top1=N/A, LR=0.091510]2025-11-04 22:02:27,164 - INFO - Step 55744: {'train_loss_batch': 3.4137325286865234, 'train_lr': 0.09151005062062345, 'batch_time': 1.6576944679744574, 'data_time': 0.006716492172654787}
2025-11-04 22:02:27 Train Epoch 022:  32%|███▏      | 800/2502 [22:07<47:16,  1.67s/it, Loss=3.5671, Top1=N/A, LR=0.091510]2025-11-04 22:05:13,030 - INFO - Step 55844: {'train_loss_batch': 3.2116594314575195, 'train_lr': 0.09151005062062345, 'batch_time': 1.6578143035874384, 'data_time': 0.006007278159018908}
2025-11-04 22:05:13 Train Epoch 022:  36%|███▌      | 900/2502 [24:53<43:47,  1.64s/it, Loss=3.5609, Top1=N/A, LR=0.091510]2025-11-04 22:07:58,155 - INFO - Step 55944: {'train_loss_batch': 4.2143874168396, 'train_lr': 0.09151005062062345, 'batch_time': 1.657086269440053, 'data_time': 0.0054539324308473715}
2025-11-04 22:07:58 Train Epoch 022:  40%|███▉      | 1000/2502 [27:38<41:35,  1.66s/it, Loss=3.5561, Top1=N/A, LR=0.091510]2025-11-04 22:10:43,605 - INFO - Step 56044: {'train_loss_batch': 3.797527313232422, 'train_lr': 0.09151005062062345, 'batch_time': 1.6568270482264318, 'data_time': 0.005006750623186628}
2025-11-04 22:10:43 Train Epoch 022:  44%|████▍     | 1100/2502 [30:23<38:40,  1.66s/it, Loss=3.5534, Top1=N/A, LR=0.091510]2025-11-04 22:13:29,025 - INFO - Step 56144: {'train_loss_batch': 2.9442338943481445, 'train_lr': 0.09151005062062345, 'batch_time': 1.6565880275227394, 'data_time': 0.004645528195664409}
2025-11-04 22:13:29 Train Epoch 022:  48%|████▊     | 1200/2502 [33:09<35:53,  1.65s/it, Loss=3.5496, Top1=N/A, LR=0.091510]2025-11-04 22:16:14,586 - INFO - Step 56244: {'train_loss_batch': 2.8276727199554443, 'train_lr': 0.09151005062062345, 'batch_time': 1.656506961827274, 'data_time': 0.004342310236058168}
2025-11-04 22:16:14 Train Epoch 022:  52%|█████▏    | 1300/2502 [35:54<32:54,  1.64s/it, Loss=3.5397, Top1=N/A, LR=0.091510]2025-11-04 22:18:59,935 - INFO - Step 56344: {'train_loss_batch': 2.8841896057128906, 'train_lr': 0.09151005062062345, 'batch_time': 1.6562742367787695, 'data_time': 0.00408455246141743}
2025-11-04 22:18:59 Train Epoch 022:  56%|█████▌    | 1400/2502 [38:40<30:26,  1.66s/it, Loss=3.5468, Top1=N/A, LR=0.091510]2025-11-04 22:21:45,142 - INFO - Step 56444: {'train_loss_batch': 4.454488754272461, 'train_lr': 0.09151005062062345, 'batch_time': 1.6559739987565993, 'data_time': 0.003863248886337117}
2025-11-04 22:21:45 Train Epoch 022:  60%|█████▉    | 1500/2502 [41:25<27:31,  1.65s/it, Loss=3.5498, Top1=N/A, LR=0.091510]2025-11-04 22:24:30,257 - INFO - Step 56544: {'train_loss_batch': 2.9084110260009766, 'train_lr': 0.09151005062062345, 'batch_time': 1.6556524655407545, 'data_time': 0.0036722377965165962}
2025-11-04 22:24:30 Train Epoch 022:  64%|██████▍   | 1600/2502 [44:10<24:45,  1.65s/it, Loss=3.5510, Top1=N/A, LR=0.091510]2025-11-04 22:27:15,891 - INFO - Step 56644: {'train_loss_batch': 2.7380940914154053, 'train_lr': 0.09151005062062345, 'batch_time': 1.6556957120972824, 'data_time': 0.0035120175079879426}
2025-11-04 22:27:15 Train Epoch 022:  68%|██████▊   | 1700/2502 [46:55<22:11,  1.66s/it, Loss=3.5476, Top1=N/A, LR=0.091510]2025-11-04 22:30:00,960 - INFO - Step 56744: {'train_loss_batch': 2.87449312210083, 'train_lr': 0.09151005062062345, 'batch_time': 1.6554011599166754, 'data_time': 0.003363538531259114}
2025-11-04 22:30:00 Train Epoch 022:  72%|███████▏  | 1800/2502 [49:41<19:15,  1.65s/it, Loss=3.5447, Top1=N/A, LR=0.091510]2025-11-04 22:32:46,630 - INFO - Step 56844: {'train_loss_batch': 3.355616331100464, 'train_lr': 0.09151005062062345, 'batch_time': 1.6554728997276598, 'data_time': 0.0032334679831801886}
2025-11-04 22:32:46 Train Epoch 022:  76%|███████▌  | 1900/2502 [52:26<16:23,  1.63s/it, Loss=3.5415, Top1=N/A, LR=0.091510]2025-11-04 22:35:31,279 - INFO - Step 56944: {'train_loss_batch': 4.621673107147217, 'train_lr': 0.09151005062062345, 'batch_time': 1.6550007532421755, 'data_time': 0.0031141956375499327}
2025-11-04 22:35:31 Train Epoch 022:  80%|███████▉  | 2000/2502 [55:10<13:50,  1.65s/it, Loss=3.5446, Top1=N/A, LR=0.091510]2025-11-04 22:38:16,076 - INFO - Step 57044: {'train_loss_batch': 5.332945823669434, 'train_lr': 0.09151005062062345, 'batch_time': 1.6546492223915965, 'data_time': 0.003007013043542316}
2025-11-04 22:38:16 Train Epoch 022:  84%|████████▍ | 2100/2502 [57:56<11:04,  1.65s/it, Loss=3.5469, Top1=N/A, LR=0.091510]2025-11-04 22:41:01,361 - INFO - Step 57144: {'train_loss_batch': 4.169610977172852, 'train_lr': 0.09151005062062345, 'batch_time': 1.6545633789699796, 'data_time': 0.002912690218490172}
2025-11-04 22:41:01 Train Epoch 022:  88%|████████▊ | 2200/2502 [1:00:41<08:13,  1.64s/it, Loss=3.5412, Top1=N/A, LR=0.091510]2025-11-04 22:43:46,511 - INFO - Step 57244: {'train_loss_batch': 2.8229987621307373, 'train_lr': 0.09151005062062345, 'batch_time': 1.6544241074376191, 'data_time': 0.0028247899328887813}
2025-11-04 22:43:46 Train Epoch 022:  92%|█████████▏| 2300/2502 [1:03:25<05:31,  1.64s/it, Loss=3.5424, Top1=N/A, LR=0.091510]2025-11-04 22:46:30,868 - INFO - Step 57344: {'train_loss_batch': 4.2320380210876465, 'train_lr': 0.09151005062062345, 'batch_time': 1.6539522494921628, 'data_time': 0.0027446049495864462}
2025-11-04 22:46:30 Train Epoch 022:  96%|█████████▌| 2400/2502 [1:06:11<02:49,  1.66s/it, Loss=3.5416, Top1=54.05%, LR=0.091510]2025-11-04 22:49:16,255 - INFO - Step 57444: {'train_loss_batch': 2.8562443256378174, 'train_lr': 0.09151005062062345, 'batch_time': 1.653948631648072, 'data_time': 0.0026726762437165054}
2025-11-04 22:49:16 Train Epoch 022: 100%|█████████▉| 2500/2502 [1:08:55<00:03,  1.64s/it, Loss=3.5420, Top1=54.03%, LR=0.091510]2025-11-04 22:52:00,904 - INFO - Step 57544: {'train_loss_batch': 2.9327826499938965, 'train_lr': 0.09151005062062345, 'batch_time': 1.6536505958262753, 'data_time': 0.0026251104820828016}
2025-11-04 22:52:00 Train Epoch 022: 100%|██████████| 2502/2502 [1:08:57<00:00,  1.65s/it, Loss=3.5420, Top1=54.03%, LR=0.091510]
2025-11-04 22:52:03 Val Epoch 022:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 22:52:07   with torch.cuda.amp.autocast():
2025-11-04 22:52:07 Val Epoch 022: 100%|██████████| 98/98 [01:52<00:00,  1.15s/it, Loss=2.7173, Top1=56.41%, Top5=80.90%]
2025-11-04 22:53:55 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-04 22:53:55   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-04 22:53:55 2025-11-04 22:53:55,470 - INFO - Step 22: {'epoch': 22, 'learning_rate': 0.0904518046337755, 'train_loss': 3.541829975103017, 'train_top1': 54.03076171875, 'train_top5': 77.62410481770833, 'train_precision': 53.63300417123769, 'train_recall': 53.852987324919695, 'train_f1': 53.493185062507955, 'val_loss': 2.717317980957031, 'val_top1': 56.41200000244141, 'val_top5': 80.90199997314453, 'val_precision': 60.8527970799259, 'val_recall': 56.413999999999994, 'val_f1': 55.76725163240363}
2025-11-04 22:53:55 2025-11-04 22:53:55,472 - INFO - Epoch 022 Summary - LR: 0.090452, Train Loss: 3.5418, Val Loss: 2.7173, Val F1: 55.77%, Val Precision: 60.85%, Val Recall: 56.41%
2025-11-04 22:53:55 Train Epoch 023:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 22 that is less than the current step 57544. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-04 22:53:59 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-04 22:53:59   with torch.cuda.amp.autocast():
2025-11-04 22:54:01 Train Epoch 023:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.8009, Top1=N/A, LR=0.090452]2025-11-04 22:54:01,357 - INFO - Step 57546: {'train_loss_batch': 2.8009297847747803, 'train_lr': 0.0904518046337755, 'batch_time': 5.461912393569946, 'data_time': 3.822138786315918}
2025-11-04 22:54:01 Train Epoch 023:   4%|▍         | 100/2502 [02:50<1:06:14,  1.65s/it, Loss=3.4120, Top1=N/A, LR=0.090452]2025-11-04 22:56:46,823 - INFO - Step 57646: {'train_loss_batch': 3.8859527111053467, 'train_lr': 0.0904518046337755, 'batch_time': 1.6923680234663556, 'data_time': 0.038821017388069985}
2025-11-04 22:56:46 Train Epoch 023:   8%|▊         | 200/2502 [05:37<1:03:34,  1.66s/it, Loss=3.4528, Top1=N/A, LR=0.090452]2025-11-04 22:59:32,939 - INFO - Step 57746: {'train_loss_batch': 2.940539836883545, 'train_lr': 0.0904518046337755, 'batch_time': 1.6768422280971091, 'data_time': 0.019995520957073763}
2025-11-04 22:59:32 Train Epoch 023:  12%|█▏        | 300/2502 [08:22<1:00:57,  1.66s/it, Loss=3.4744, Top1=N/A, LR=0.090452]2025-11-04 23:02:18,205 - INFO - Step 57846: {'train_loss_batch': 4.343235015869141, 'train_lr': 0.0904518046337755, 'batch_time': 1.6688067136808884, 'data_time': 0.01367438829618435}
2025-11-04 23:02:18 Train Epoch 023:  16%|█▌        | 400/2502 [11:07<57:47,  1.65s/it, Loss=3.4770, Top1=N/A, LR=0.090452]2025-11-04 23:05:03,862 - INFO - Step 57946: {'train_loss_batch': 4.8641133308410645, 'train_lr': 0.0904518046337755, 'batch_time': 1.6657550644101644, 'data_time': 0.010511737809216886}
2025-11-04 23:05:03 Train Epoch 023:  20%|█▉        | 500/2502 [13:53<55:22,  1.66s/it, Loss=3.5223, Top1=N/A, LR=0.090452]2025-11-04 23:07:49,114 - INFO - Step 58046: {'train_loss_batch': 4.191830158233643, 'train_lr': 0.0904518046337755, 'batch_time': 1.6631114635162962, 'data_time': 0.008603509552702457}
2025-11-04 23:07:49 Train Epoch 023:  24%|██▍       | 600/2502 [16:38<52:20,  1.65s/it, Loss=3.5179, Top1=N/A, LR=0.090452]2025-11-04 23:10:34,613 - INFO - Step 58146: {'train_loss_batch': 4.154799938201904, 'train_lr': 0.0904518046337755, 'batch_time': 1.661761275543746, 'data_time': 0.007336449107393846}
2025-11-04 23:10:34 Train Epoch 023:  28%|██▊       | 700/2502 [19:24<49:16,  1.64s/it, Loss=3.5232, Top1=N/A, LR=0.090452]2025-11-04 23:13:20,122 - INFO - Step 58246: {'train_loss_batch': 4.095425128936768, 'train_lr': 0.0904518046337755, 'batch_time': 1.6608087676397914, 'data_time': 0.006429286894206483}
2025-11-04 23:13:20 Train Epoch 023:  32%|███▏      | 800/2502 [22:09<46:48,  1.65s/it, Loss=3.5250, Top1=54.56%, LR=0.090452]2025-11-04 23:16:05,064 - INFO - Step 58346: {'train_loss_batch': 2.788358211517334, 'train_lr': 0.0904518046337755, 'batch_time': 1.6593864740950337, 'data_time': 0.005747699856609292}
2025-11-04 23:16:05 Train Epoch 023:  36%|███▌      | 900/2502 [24:54<44:17,  1.66s/it, Loss=3.5256, Top1=N/A, LR=0.090452]   2025-11-04 23:18:50,635 - INFO - Step 58446: {'train_loss_batch': 2.933932065963745, 'train_lr': 0.0904518046337755, 'batch_time': 1.658977967917456, 'data_time': 0.005214834319103042}
2025-11-04 23:18:50 Train Epoch 023:  40%|███▉      | 1000/2502 [27:40<41:06,  1.64s/it, Loss=3.5207, Top1=N/A, LR=0.090452]2025-11-04 23:21:36,447 - INFO - Step 58546: {'train_loss_batch': 4.445708274841309, 'train_lr': 0.0904518046337755, 'batch_time': 1.6588917447851375, 'data_time': 0.004796012417300717}
2025-11-04 23:21:36 Train Epoch 023:  44%|████▍     | 1100/2502 [30:26<38:49,  1.66s/it, Loss=3.5318, Top1=54.47%, LR=0.090452]2025-11-04 23:24:22,096 - INFO - Step 58646: {'train_loss_batch': 2.9063222408294678, 'train_lr': 0.0904518046337755, 'batch_time': 1.658673719099497, 'data_time': 0.004444991968416496}
2025-11-04 23:24:22 Train Epoch 023:  48%|████▊     | 1200/2502 [33:11<35:51,  1.65s/it, Loss=3.5319, Top1=N/A, LR=0.090452]   2025-11-04 23:27:07,653 - INFO - Step 58746: {'train_loss_batch': 3.7781009674072266, 'train_lr': 0.0904518046337755, 'batch_time': 1.6584151753974299, 'data_time': 0.004149751996716095}
2025-11-04 23:27:07 Train Epoch 023:  52%|█████▏    | 1300/2502 [35:56<33:00,  1.65s/it, Loss=3.5443, Top1=N/A, LR=0.090452]2025-11-04 23:29:52,654 - INFO - Step 58846: {'train_loss_batch': 4.047351837158203, 'train_lr': 0.0904518046337755, 'batch_time': 1.6577685909212598, 'data_time': 0.003907872932310933}
2025-11-04 23:29:52 Train Epoch 023:  56%|█████▌    | 1400/2502 [38:41<29:59,  1.63s/it, Loss=3.5323, Top1=54.44%, LR=0.090452]2025-11-04 23:32:37,419 - INFO - Step 58946: {'train_loss_batch': 2.780778408050537, 'train_lr': 0.0904518046337755, 'batch_time': 1.657046586764361, 'data_time': 0.0036996680442815504}
2025-11-04 23:32:37 Train Epoch 023:  60%|█████▉    | 1500/2502 [41:25<27:34,  1.65s/it, Loss=3.5336, Top1=N/A, LR=0.090452]   2025-11-04 23:35:21,874 - INFO - Step 59046: {'train_loss_batch': 4.6253862380981445, 'train_lr': 0.0904518046337755, 'batch_time': 1.656214023097049, 'data_time': 0.003513621934487929}
2025-11-04 23:35:21 Train Epoch 023:  64%|██████▍   | 1600/2502 [44:11<24:40,  1.64s/it, Loss=3.5398, Top1=54.48%, LR=0.090452]2025-11-04 23:38:07,179 - INFO - Step 59146: {'train_loss_batch': 2.811525821685791, 'train_lr': 0.0904518046337755, 'batch_time': 1.6560163146477056, 'data_time': 0.00335328911037314}
2025-11-04 23:38:07 Train Epoch 023:  68%|██████▊   | 1700/2502 [46:56<22:08,  1.66s/it, Loss=3.5371, Top1=N/A, LR=0.090452]   2025-11-04 23:40:52,442 - INFO - Step 59246: {'train_loss_batch': 4.537869930267334, 'train_lr': 0.0904518046337755, 'batch_time': 1.6558167874147303, 'data_time': 0.003213594970389158}
2025-11-04 23:40:52 Train Epoch 023:  72%|███████▏  | 1800/2502 [49:41<19:19,  1.65s/it, Loss=3.5416, Top1=N/A, LR=0.090452]2025-11-04 23:43:37,511 - INFO - Step 59346: {'train_loss_batch': 2.901397466659546, 'train_lr': 0.0904518046337755, 'batch_time': 1.6555320352133878, 'data_time': 0.003087647420574995}
2025-11-04 23:43:37 Train Epoch 023:  76%|███████▌  | 1900/2502 [52:26<16:34,  1.65s/it, Loss=3.5437, Top1=N/A, LR=0.090452]2025-11-04 23:46:22,650 - INFO - Step 59446: {'train_loss_batch': 4.408249855041504, 'train_lr': 0.0904518046337755, 'batch_time': 1.6553141108818394, 'data_time': 0.002975823564444135}
2025-11-04 23:46:22 Train Epoch 023:  80%|███████▉  | 2000/2502 [55:10<13:51,  1.66s/it, Loss=3.5442, Top1=N/A, LR=0.090452]2025-11-04 23:49:06,672 - INFO - Step 59546: {'train_loss_batch': 3.897027015686035, 'train_lr': 0.0904518046337755, 'batch_time': 1.6545598430671673, 'data_time': 0.0028762701807589245}
2025-11-04 23:49:06 Train Epoch 023:  84%|████████▍ | 2100/2502 [57:56<11:00,  1.64s/it, Loss=3.5427, Top1=N/A, LR=0.090452]2025-11-04 23:51:52,127 - INFO - Step 59646: {'train_loss_batch': 3.8094024658203125, 'train_lr': 0.0904518046337755, 'batch_time': 1.6545590560019783, 'data_time': 0.002782487574218285}
2025-11-04 23:51:52 Train Epoch 023:  88%|████████▊ | 2200/2502 [1:00:41<08:20,  1.66s/it, Loss=3.5443, Top1=N/A, LR=0.090452]2025-11-04 23:54:37,663 - INFO - Step 59746: {'train_loss_batch': 2.825671434402466, 'train_lr': 0.0904518046337755, 'batch_time': 1.6545952213509199, 'data_time': 0.002699789487465248}
2025-11-04 23:54:37 Train Epoch 023:  92%|█████████▏| 2300/2502 [1:03:26<05:34,  1.66s/it, Loss=3.5498, Top1=N/A, LR=0.090452]2025-11-04 23:57:22,231 - INFO - Step 59846: {'train_loss_batch': 5.307603359222412, 'train_lr': 0.0904518046337755, 'batch_time': 1.6542079356482422, 'data_time': 0.0026261609619153887}
2025-11-04 23:57:22 Train Epoch 023:  96%|█████████▌| 2400/2502 [1:06:11<02:47,  1.64s/it, Loss=3.5497, Top1=N/A, LR=0.090452]2025-11-05 00:00:07,797 - INFO - Step 59946: {'train_loss_batch': 4.039526462554932, 'train_lr': 0.0904518046337755, 'batch_time': 1.6542683954489126, 'data_time': 0.0025588456217024236}
2025-11-05 00:00:07 Train Epoch 023: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.65s/it, Loss=3.5472, Top1=N/A, LR=0.090452]2025-11-05 00:02:52,777 - INFO - Step 60046: {'train_loss_batch': 2.9246249198913574, 'train_lr': 0.0904518046337755, 'batch_time': 1.6540893671370562, 'data_time': 0.0025138971282214653}
2025-11-05 00:02:52 Train Epoch 023: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=3.5472, Top1=N/A, LR=0.090452]
2025-11-05 00:02:55 Val Epoch 023:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 00:02:59   with torch.cuda.amp.autocast():
2025-11-05 00:02:59 Val Epoch 023: 100%|██████████| 98/98 [01:53<00:00,  1.16s/it, Loss=2.5850, Top1=59.07%, Top5=83.19%]
2025-11-05 00:04:48 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 00:04:48   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 00:04:48 2025-11-05 00:04:48,407 - INFO - Step 23: {'epoch': 23, 'learning_rate': 0.089338313176984, 'train_loss': 3.547192921741403, 'train_top1': 54.45022276606426, 'train_top5': 77.9175294929719, 'train_precision': 54.15469784430006, 'train_recall': 54.36580925499075, 'train_f1': 54.001168213954074, 'val_loss': 2.585046756362915, 'val_top1': 59.06800001464844, 'val_top5': 83.18800001953124, 'val_precision': 62.39097229058187, 'val_recall': 59.06400000000001, 'val_f1': 58.44769158310521}
2025-11-05 00:04:48 2025-11-05 00:04:48,408 - INFO - Epoch 023 Summary - LR: 0.089338, Train Loss: 3.5472, Val Loss: 2.5850, Val F1: 58.45%, Val Precision: 62.39%, Val Recall: 59.06%
2025-11-05 00:04:51 2025-11-05 00:04:51,382 - INFO - New best model saved with validation accuracy: 59.068%
2025-11-05 00:04:51 2025-11-05 00:04:51,382 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_024.pth
2025-11-05 00:04:51 Train Epoch 024:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 00:04:55   with torch.cuda.amp.autocast():
2025-11-05 00:04:56 Train Epoch 024:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.2041, Top1=N/A, LR=0.089338]2025-11-05 00:04:56,756 - INFO - Step 60048: {'train_loss_batch': 3.2040796279907227, 'train_lr': 0.089338313176984, 'batch_time': 5.372236490249634, 'data_time': 3.7328011989593506}
2025-11-05 00:04:56 Train Epoch 024:   0%|          | 1/2502 [00:05<3:44:01,  5.37s/it, Loss=3.2041, Top1=N/A, LR=0.089338]wandb: WARNING Tried to log to step 23 that is less than the current step 60046. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 00:04:58 Train Epoch 024:   4%|▍         | 100/2502 [02:50<1:06:18,  1.66s/it, Loss=3.5102, Top1=55.56%, LR=0.089338]2025-11-05 00:07:42,176 - INFO - Step 60148: {'train_loss_batch': 2.814027786254883, 'train_lr': 0.089338313176984, 'batch_time': 1.6910118466556663, 'data_time': 0.03791484030166475}
2025-11-05 00:07:42 Train Epoch 024:   8%|▊         | 200/2502 [05:36<1:03:38,  1.66s/it, Loss=3.5283, Top1=N/A, LR=0.089338]   2025-11-05 00:10:28,140 - INFO - Step 60248: {'train_loss_batch': 4.199732780456543, 'train_lr': 0.089338313176984, 'batch_time': 1.6754013958261973, 'data_time': 0.019530136193802106}
2025-11-05 00:10:28 Train Epoch 024:  12%|█▏        | 300/2502 [08:22<1:00:49,  1.66s/it, Loss=3.5281, Top1=N/A, LR=0.089338]2025-11-05 00:13:13,984 - INFO - Step 60348: {'train_loss_batch': 3.4342589378356934, 'train_lr': 0.089338313176984, 'batch_time': 1.6697659698435634, 'data_time': 0.013352420242917894}
2025-11-05 00:13:13 Train Epoch 024:  16%|█▌        | 400/2502 [11:08<57:47,  1.65s/it, Loss=3.5343, Top1=N/A, LR=0.089338]2025-11-05 00:15:59,799 - INFO - Step 60448: {'train_loss_batch': 4.390507698059082, 'train_lr': 0.089338313176984, 'batch_time': 1.6668693835004011, 'data_time': 0.010264337806035752}
2025-11-05 00:15:59 Train Epoch 024:  20%|█▉        | 500/2502 [13:54<55:28,  1.66s/it, Loss=3.5466, Top1=N/A, LR=0.089338]2025-11-05 00:18:45,410 - INFO - Step 60548: {'train_loss_batch': 3.021807909011841, 'train_lr': 0.089338313176984, 'batch_time': 1.664722871399687, 'data_time': 0.008423952285401121}
2025-11-05 00:18:45 Train Epoch 024:  24%|██▍       | 600/2502 [16:38<52:28,  1.66s/it, Loss=3.5340, Top1=N/A, LR=0.089338]2025-11-05 00:21:30,324 - INFO - Step 60648: {'train_loss_batch': 4.6891069412231445, 'train_lr': 0.089338313176984, 'batch_time': 1.6621296774726144, 'data_time': 0.007177429072273749}
2025-11-05 00:21:30 Train Epoch 024:  28%|██▊       | 700/2502 [19:24<50:02,  1.67s/it, Loss=3.5433, Top1=N/A, LR=0.089338]2025-11-05 00:24:15,967 - INFO - Step 60748: {'train_loss_batch': 2.8556716442108154, 'train_lr': 0.089338313176984, 'batch_time': 1.661316013880361, 'data_time': 0.006292659103785364}
2025-11-05 00:24:15 Train Epoch 024:  32%|███▏      | 800/2502 [22:10<47:01,  1.66s/it, Loss=3.5501, Top1=N/A, LR=0.089338]2025-11-05 00:27:01,691 - INFO - Step 60848: {'train_loss_batch': 4.443855285644531, 'train_lr': 0.089338313176984, 'batch_time': 1.660806386211838, 'data_time': 0.005632935689480862}
2025-11-05 00:27:01 Train Epoch 024:  36%|███▌      | 900/2502 [24:56<44:18,  1.66s/it, Loss=3.5501, Top1=54.98%, LR=0.089338]2025-11-05 00:29:47,704 - INFO - Step 60948: {'train_loss_batch': 2.802776575088501, 'train_lr': 0.089338313176984, 'batch_time': 1.6607306474586174, 'data_time': 0.005118924690271985}
2025-11-05 00:29:47 Train Epoch 024:  40%|███▉      | 1000/2502 [27:42<41:26,  1.66s/it, Loss=3.5408, Top1=N/A, LR=0.089338]   2025-11-05 00:32:33,614 - INFO - Step 61048: {'train_loss_batch': 3.907785415649414, 'train_lr': 0.089338313176984, 'batch_time': 1.6605679176665924, 'data_time': 0.004713550790563807}
2025-11-05 00:32:33 Train Epoch 024:  44%|████▍     | 1100/2502 [30:27<38:27,  1.65s/it, Loss=3.5347, Top1=N/A, LR=0.089338]2025-11-05 00:35:18,956 - INFO - Step 61148: {'train_loss_batch': 2.892254590988159, 'train_lr': 0.089338313176984, 'batch_time': 1.6599183955266192, 'data_time': 0.004372248749208927}
2025-11-05 00:35:18 Train Epoch 024:  48%|████▊     | 1200/2502 [33:13<35:43,  1.65s/it, Loss=3.5376, Top1=55.03%, LR=0.089338]2025-11-05 00:38:04,404 - INFO - Step 61248: {'train_loss_batch': 2.904047966003418, 'train_lr': 0.089338313176984, 'batch_time': 1.6594658592757734, 'data_time': 0.004090979534025295}
2025-11-05 00:38:04 Train Epoch 024:  52%|█████▏    | 1300/2502 [35:58<33:12,  1.66s/it, Loss=3.5382, Top1=N/A, LR=0.089338]   2025-11-05 00:40:50,221 - INFO - Step 61348: {'train_loss_batch': 4.048650741577148, 'train_lr': 0.089338313176984, 'batch_time': 1.6593658064623047, 'data_time': 0.0038544226389129192}
2025-11-05 00:40:50 Train Epoch 024:  56%|█████▌    | 1400/2502 [38:44<30:16,  1.65s/it, Loss=3.5275, Top1=N/A, LR=0.089338]2025-11-05 00:43:35,878 - INFO - Step 61448: {'train_loss_batch': 4.676971435546875, 'train_lr': 0.089338313176984, 'batch_time': 1.6591662157441274, 'data_time': 0.003650874941796596}
2025-11-05 00:43:35 Train Epoch 024:  60%|█████▉    | 1500/2502 [41:29<27:15,  1.63s/it, Loss=3.5287, Top1=N/A, LR=0.089338]2025-11-05 00:46:20,672 - INFO - Step 61548: {'train_loss_batch': 3.2080647945404053, 'train_lr': 0.089338313176984, 'batch_time': 1.6584178195803425, 'data_time': 0.003471490464791864}
2025-11-05 00:46:20 Train Epoch 024:  64%|██████▍   | 1600/2502 [44:14<24:55,  1.66s/it, Loss=3.5289, Top1=54.99%, LR=0.089338]2025-11-05 00:49:05,580 - INFO - Step 61648: {'train_loss_batch': 2.9501099586486816, 'train_lr': 0.089338313176984, 'batch_time': 1.6578347694866364, 'data_time': 0.0033129156566574602}
2025-11-05 00:49:05 Train Epoch 024:  68%|██████▊   | 1700/2502 [46:59<22:02,  1.65s/it, Loss=3.5271, Top1=N/A, LR=0.089338]   2025-11-05 00:51:50,552 - INFO - Step 61748: {'train_loss_batch': 5.125973701477051, 'train_lr': 0.089338313176984, 'batch_time': 1.6573574932654558, 'data_time': 0.003174354580134101}
2025-11-05 00:51:50 Train Epoch 024:  72%|███████▏  | 1800/2502 [49:44<19:25,  1.66s/it, Loss=3.5347, Top1=N/A, LR=0.089338]2025-11-05 00:54:36,334 - INFO - Step 61848: {'train_loss_batch': 4.007798194885254, 'train_lr': 0.089338313176984, 'batch_time': 1.657383005464163, 'data_time': 0.003050535669067315}
2025-11-05 00:54:36 Train Epoch 024:  76%|███████▌  | 1900/2502 [52:30<16:33,  1.65s/it, Loss=3.5349, Top1=N/A, LR=0.089338]2025-11-05 00:57:22,270 - INFO - Step 61948: {'train_loss_batch': 4.6404290199279785, 'train_lr': 0.089338313176984, 'batch_time': 1.657486962369089, 'data_time': 0.0029425154479787827}
2025-11-05 00:57:22 Train Epoch 024:  80%|███████▉  | 2000/2502 [55:14<13:46,  1.65s/it, Loss=3.5352, Top1=N/A, LR=0.089338]2025-11-05 01:00:05,645 - INFO - Step 62048: {'train_loss_batch': 2.9926912784576416, 'train_lr': 0.089338313176984, 'batch_time': 1.6563006307648636, 'data_time': 0.0028403309331662295}
2025-11-05 01:00:05 Train Epoch 024:  84%|████████▍ | 2100/2502 [57:59<11:05,  1.66s/it, Loss=3.5397, Top1=54.82%, LR=0.089338]2025-11-05 01:02:51,084 - INFO - Step 62148: {'train_loss_batch': 2.8934502601623535, 'train_lr': 0.089338313176984, 'batch_time': 1.6562093407241916, 'data_time': 0.0027518079486249574}
2025-11-05 01:02:51 Train Epoch 024:  88%|████████▊ | 2200/2502 [1:00:44<08:13,  1.63s/it, Loss=3.5389, Top1=N/A, LR=0.089338]   2025-11-05 01:05:35,741 - INFO - Step 62248: {'train_loss_batch': 4.31403112411499, 'train_lr': 0.089338313176984, 'batch_time': 1.6557714066035312, 'data_time': 0.002669316754997562}
2025-11-05 01:05:35 Train Epoch 024:  92%|█████████▏| 2300/2502 [1:03:29<05:35,  1.66s/it, Loss=3.5384, Top1=N/A, LR=0.089338]2025-11-05 01:08:20,562 - INFO - Step 62348: {'train_loss_batch': 3.129354476928711, 'train_lr': 0.089338313176984, 'batch_time': 1.655442651382274, 'data_time': 0.0025944382353794673}
2025-11-05 01:08:20 Train Epoch 024:  96%|█████████▌| 2400/2502 [1:06:14<02:49,  1.66s/it, Loss=3.5417, Top1=N/A, LR=0.089338]2025-11-05 01:11:06,238 - INFO - Step 62448: {'train_loss_batch': 2.9683191776275635, 'train_lr': 0.089338313176984, 'batch_time': 1.6554975109465764, 'data_time': 0.00252696306990465}
2025-11-05 01:11:06 Train Epoch 024: 100%|█████████▉| 2500/2502 [1:09:00<00:03,  1.66s/it, Loss=3.5399, Top1=N/A, LR=0.089338]2025-11-05 01:13:52,326 - INFO - Step 62548: {'train_loss_batch': 3.0083775520324707, 'train_lr': 0.089338313176984, 'batch_time': 1.6557128490423594, 'data_time': 0.0024859815633377995}
2025-11-05 01:13:52 Train Epoch 024: 100%|██████████| 2502/2502 [1:09:02<00:00,  1.66s/it, Loss=3.5399, Top1=N/A, LR=0.089338]
2025-11-05 01:13:54 Val Epoch 024:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 01:13:59   with torch.cuda.amp.autocast():
2025-11-05 01:13:59 Val Epoch 024: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.6104, Top1=58.79%, Top5=82.95%]
2025-11-05 01:15:46 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 01:15:46   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 01:15:46 2025-11-05 01:15:46,596 - INFO - Step 24: {'epoch': 24, 'learning_rate': 0.08817109714564145, 'train_loss': 3.5404601770815707, 'train_top1': 54.81608385316699, 'train_top5': 78.18385616602687, 'train_precision': 54.4975243322139, 'train_recall': 54.661432846835325, 'train_f1': 54.339233611735196, 'val_loss': 2.6104311586761475, 'val_top1': 58.78599999389648, 'val_top5': 82.95199998779297, 'val_precision': 62.6674984127052, 'val_recall': 58.792, 'val_f1': 58.27191594697354}
2025-11-05 01:15:46 2025-11-05 01:15:46,598 - INFO - Epoch 024 Summary - LR: 0.088171, Train Loss: 3.5405, Val Loss: 2.6104, Val F1: 58.27%, Val Precision: 62.67%, Val Recall: 58.79%
2025-11-05 01:15:47 Train Epoch 025:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 24 that is less than the current step 62548. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 01:15:51 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 01:15:51   with torch.cuda.amp.autocast():
2025-11-05 01:15:53 Train Epoch 025:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.9041, Top1=53.32%, LR=0.088171]2025-11-05 01:15:53,056 - INFO - Step 62550: {'train_loss_batch': 2.9040651321411133, 'train_lr': 0.08817109714564145, 'batch_time': 5.638012170791626, 'data_time': 4.002873420715332}
2025-11-05 01:15:53 Train Epoch 025:   4%|▍         | 100/2502 [02:50<1:06:18,  1.66s/it, Loss=3.5861, Top1=N/A, LR=0.088171]   2025-11-05 01:18:38,173 - INFO - Step 62650: {'train_loss_batch': 3.73927640914917, 'train_lr': 0.08817109714564145, 'batch_time': 1.6906544147151532, 'data_time': 0.04067822022013145}
2025-11-05 01:18:38 Train Epoch 025:   8%|▊         | 200/2502 [05:36<1:03:36,  1.66s/it, Loss=3.5489, Top1=N/A, LR=0.088171]2025-11-05 01:21:23,915 - INFO - Step 62750: {'train_loss_batch': 3.324458599090576, 'train_lr': 0.08817109714564145, 'batch_time': 1.6741208555686533, 'data_time': 0.020909513407085665}
2025-11-05 01:21:23 Train Epoch 025:  12%|█▏        | 300/2502 [08:22<1:00:54,  1.66s/it, Loss=3.5281, Top1=N/A, LR=0.088171]2025-11-05 01:24:09,706 - INFO - Step 62850: {'train_loss_batch': 4.241344928741455, 'train_lr': 0.08817109714564145, 'batch_time': 1.6687339785883197, 'data_time': 0.01427968633531336}
2025-11-05 01:24:09 Train Epoch 025:  16%|█▌        | 400/2502 [11:07<57:49,  1.65s/it, Loss=3.5340, Top1=N/A, LR=0.088171]2025-11-05 01:26:55,089 - INFO - Step 62950: {'train_loss_batch': 3.9068479537963867, 'train_lr': 0.08817109714564145, 'batch_time': 1.665017444891228, 'data_time': 0.010974894735283982}
2025-11-05 01:26:55 Train Epoch 025:  20%|█▉        | 500/2502 [13:51<54:55,  1.65s/it, Loss=3.5664, Top1=N/A, LR=0.088171]2025-11-05 01:29:39,227 - INFO - Step 63050: {'train_loss_batch': 3.6744353771209717, 'train_lr': 0.08817109714564145, 'batch_time': 1.6602986106377637, 'data_time': 0.008985290984193721}
2025-11-05 01:29:39 Train Epoch 025:  24%|██▍       | 600/2502 [16:37<52:27,  1.65s/it, Loss=3.5514, Top1=N/A, LR=0.088171]2025-11-05 01:32:25,121 - INFO - Step 63150: {'train_loss_batch': 3.800760269165039, 'train_lr': 0.08817109714564145, 'batch_time': 1.6600723405447657, 'data_time': 0.007668625296848189}
2025-11-05 01:32:25 Train Epoch 025:  28%|██▊       | 700/2502 [19:22<49:32,  1.65s/it, Loss=3.5457, Top1=N/A, LR=0.088171]2025-11-05 01:35:10,196 - INFO - Step 63250: {'train_loss_batch': 4.3823561668396, 'train_lr': 0.08817109714564145, 'batch_time': 1.6587415146249507, 'data_time': 0.006707205071768985}
2025-11-05 01:35:10 Train Epoch 025:  32%|███▏      | 800/2502 [22:08<46:45,  1.65s/it, Loss=3.5375, Top1=N/A, LR=0.088171]2025-11-05 01:37:55,597 - INFO - Step 63350: {'train_loss_batch': 4.502622604370117, 'train_lr': 0.08817109714564145, 'batch_time': 1.6581514414478926, 'data_time': 0.005995032194997189}
2025-11-05 01:37:55 Train Epoch 025:  36%|███▌      | 900/2502 [24:53<44:21,  1.66s/it, Loss=3.5389, Top1=N/A, LR=0.088171]2025-11-05 01:40:41,011 - INFO - Step 63450: {'train_loss_batch': 2.806739091873169, 'train_lr': 0.08817109714564145, 'batch_time': 1.6577052855729792, 'data_time': 0.005440577021184957}
2025-11-05 01:40:41 Train Epoch 025:  40%|███▉      | 1000/2502 [27:36<41:00,  1.64s/it, Loss=3.5401, Top1=N/A, LR=0.088171]2025-11-05 01:43:24,313 - INFO - Step 63550: {'train_loss_batch': 2.897836208343506, 'train_lr': 0.08817109714564145, 'batch_time': 1.6552387689138863, 'data_time': 0.004987748352797715}
2025-11-05 01:43:24 Train Epoch 025:  44%|████▍     | 1100/2502 [30:21<38:08,  1.63s/it, Loss=3.5346, Top1=N/A, LR=0.088171]2025-11-05 01:46:09,186 - INFO - Step 63650: {'train_loss_batch': 2.9767162799835205, 'train_lr': 0.08817109714564145, 'batch_time': 1.654647566858581, 'data_time': 0.00462094098194202}
2025-11-05 01:46:09 Train Epoch 025:  48%|████▊     | 1200/2502 [33:05<35:53,  1.65s/it, Loss=3.5357, Top1=N/A, LR=0.088171]2025-11-05 01:48:52,883 - INFO - Step 63750: {'train_loss_batch': 2.786919355392456, 'train_lr': 0.08817109714564145, 'batch_time': 1.65317572463462, 'data_time': 0.004317202238516446}
2025-11-05 01:48:52 Train Epoch 025:  52%|█████▏    | 1300/2502 [35:51<33:04,  1.65s/it, Loss=3.5340, Top1=N/A, LR=0.088171]2025-11-05 01:51:38,821 - INFO - Step 63850: {'train_loss_batch': 4.088736534118652, 'train_lr': 0.08817109714564145, 'batch_time': 1.6536520936688857, 'data_time': 0.004063356848151569}
2025-11-05 01:51:38 Train Epoch 025:  56%|█████▌    | 1400/2502 [38:36<30:28,  1.66s/it, Loss=3.5252, Top1=N/A, LR=0.088171]2025-11-05 01:54:24,225 - INFO - Step 63950: {'train_loss_batch': 2.7952351570129395, 'train_lr': 0.08817109714564145, 'batch_time': 1.6536799583326145, 'data_time': 0.003842894134140287}
2025-11-05 01:54:24 Train Epoch 025:  60%|█████▉    | 1500/2502 [41:22<27:45,  1.66s/it, Loss=3.5317, Top1=N/A, LR=0.088171]2025-11-05 01:57:09,927 - INFO - Step 64050: {'train_loss_batch': 4.030489444732666, 'train_lr': 0.08817109714564145, 'batch_time': 1.6539025049381142, 'data_time': 0.0036588720605025523}
2025-11-05 01:57:09 Train Epoch 025:  64%|██████▍   | 1600/2502 [44:08<24:54,  1.66s/it, Loss=3.5353, Top1=N/A, LR=0.088171]2025-11-05 01:59:55,836 - INFO - Step 64150: {'train_loss_batch': 4.41533899307251, 'train_lr': 0.08817109714564145, 'batch_time': 1.6542261897735786, 'data_time': 0.0034938880162712635}
2025-11-05 01:59:55 Train Epoch 025:  68%|██████▊   | 1700/2502 [46:54<22:06,  1.65s/it, Loss=3.5347, Top1=N/A, LR=0.088171]2025-11-05 02:02:41,436 - INFO - Step 64250: {'train_loss_batch': 3.29752779006958, 'train_lr': 0.08817109714564145, 'batch_time': 1.6543304764614744, 'data_time': 0.0033491849198473123}
2025-11-05 02:02:41 Train Epoch 025:  72%|███████▏  | 1800/2502 [49:39<19:10,  1.64s/it, Loss=3.5377, Top1=54.94%, LR=0.088171]2025-11-05 02:05:27,198 - INFO - Step 64350: {'train_loss_batch': 2.8216423988342285, 'train_lr': 0.08817109714564145, 'batch_time': 1.654512712122797, 'data_time': 0.0032198937451554826}
2025-11-05 02:05:27 Train Epoch 025:  76%|███████▌  | 1900/2502 [52:24<16:37,  1.66s/it, Loss=3.5386, Top1=N/A, LR=0.088171]   2025-11-05 02:08:11,523 - INFO - Step 64450: {'train_loss_batch': 2.9763894081115723, 'train_lr': 0.08817109714564145, 'batch_time': 1.6539201498157035, 'data_time': 0.003100344032315691}
2025-11-05 02:08:11 Train Epoch 025:  80%|███████▉  | 2000/2502 [55:09<13:54,  1.66s/it, Loss=3.5364, Top1=54.92%, LR=0.088171]2025-11-05 02:10:57,249 - INFO - Step 64550: {'train_loss_batch': 2.9289767742156982, 'train_lr': 0.08817109714564145, 'batch_time': 1.6540867066991025, 'data_time': 0.0029993891298979417}
2025-11-05 02:10:57 Train Epoch 025:  84%|████████▍ | 2100/2502 [57:55<11:01,  1.65s/it, Loss=3.5412, Top1=N/A, LR=0.088171]   2025-11-05 02:13:42,721 - INFO - Step 64650: {'train_loss_batch': 4.240361213684082, 'train_lr': 0.08817109714564145, 'batch_time': 1.6541171361014482, 'data_time': 0.0029060943872232996}
2025-11-05 02:13:42 Train Epoch 025:  88%|████████▊ | 2200/2502 [1:00:40<08:21,  1.66s/it, Loss=3.5383, Top1=N/A, LR=0.088171]2025-11-05 02:16:28,334 - INFO - Step 64750: {'train_loss_batch': 2.676697254180908, 'train_lr': 0.08817109714564145, 'batch_time': 1.6542083857439691, 'data_time': 0.002820164547460505}
2025-11-05 02:16:28 Train Epoch 025:  92%|█████████▏| 2300/2502 [1:03:26<05:34,  1.66s/it, Loss=3.5350, Top1=54.87%, LR=0.088171]2025-11-05 02:19:14,287 - INFO - Step 64850: {'train_loss_batch': 2.85275936126709, 'train_lr': 0.08817109714564145, 'batch_time': 1.6544393545023932, 'data_time': 0.0027418768649410032}
2025-11-05 02:19:14 Train Epoch 025:  96%|█████████▌| 2400/2502 [1:06:12<02:48,  1.66s/it, Loss=3.5332, Top1=N/A, LR=0.088171]   2025-11-05 02:22:00,233 - INFO - Step 64950: {'train_loss_batch': 2.8726470470428467, 'train_lr': 0.08817109714564145, 'batch_time': 1.6546486265110205, 'data_time': 0.0026694326190241473}
2025-11-05 02:22:00 Train Epoch 025: 100%|█████████▉| 2500/2502 [1:08:57<00:03,  1.66s/it, Loss=3.5351, Top1=N/A, LR=0.088171]2025-11-05 02:24:45,159 - INFO - Step 65050: {'train_loss_batch': 4.955310821533203, 'train_lr': 0.08817109714564145, 'batch_time': 1.6544329898922694, 'data_time': 0.0026235702465839837}
2025-11-05 02:24:45 Train Epoch 025: 100%|██████████| 2502/2502 [1:08:59<00:00,  1.65s/it, Loss=3.5351, Top1=N/A, LR=0.088171]
2025-11-05 02:24:47 Val Epoch 025:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 02:24:51   with torch.cuda.amp.autocast():
2025-11-05 02:24:52 Val Epoch 025: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.6123, Top1=58.28%, Top5=82.61%]
2025-11-05 02:26:39 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 02:26:39   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 02:26:39 2025-11-05 02:26:39,209 - INFO - Step 25: {'epoch': 25, 'learning_rate': 0.08695175081644685, 'train_loss': 3.5357075046292312, 'train_top1': 54.8828125, 'train_top5': 78.36745689655173, 'train_precision': 54.555537619871444, 'train_recall': 54.75368776570085, 'train_f1': 54.418672367099084, 'val_loss': 2.6122911041259766, 'val_top1': 58.27999998046875, 'val_top5': 82.60799998291016, 'val_precision': 62.95868322867763, 'val_recall': 58.275999999999996, 'val_f1': 57.974481560165316}
2025-11-05 02:26:39 2025-11-05 02:26:39,211 - INFO - Epoch 025 Summary - LR: 0.086952, Train Loss: 3.5357, Val Loss: 2.6123, Val F1: 57.97%, Val Precision: 62.96%, Val Recall: 58.28%
2025-11-05 02:26:39 Train Epoch 026:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 02:26:43   with torch.cuda.amp.autocast():
2025-11-05 02:26:45 Train Epoch 026:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.8080, Top1=N/A, LR=0.086952]2025-11-05 02:26:45,521 - INFO - Step 65052: {'train_loss_batch': 2.8080215454101562, 'train_lr': 0.08695175081644685, 'batch_time': 5.525206089019775, 'data_time': 3.872260570526123}
2025-11-05 02:26:45 Train Epoch 026:   0%|          | 2/2502 [00:07<2:15:28,  3.25s/it, Loss=2.8080, Top1=N/A, LR=0.086952]wandb: WARNING Tried to log to step 25 that is less than the current step 65050. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 02:26:48 Train Epoch 026:   4%|▍         | 100/2502 [02:50<1:05:45,  1.64s/it, Loss=3.4234, Top1=56.43%, LR=0.086952]2025-11-05 02:29:30,690 - INFO - Step 65152: {'train_loss_batch': 2.867990016937256, 'train_lr': 0.08695175081644685, 'batch_time': 1.6900522331200023, 'data_time': 0.03937725029369392}
2025-11-05 02:29:30 Train Epoch 026:   8%|▊         | 200/2502 [05:36<1:03:10,  1.65s/it, Loss=3.4521, Top1=N/A, LR=0.086952]   2025-11-05 02:32:16,092 - INFO - Step 65252: {'train_loss_batch': 5.006142616271973, 'train_lr': 0.08695175081644685, 'batch_time': 1.6721229624392382, 'data_time': 0.02027057533833518}
2025-11-05 02:32:16 Train Epoch 026:  12%|█▏        | 300/2502 [08:21<1:00:55,  1.66s/it, Loss=3.4293, Top1=N/A, LR=0.086952]2025-11-05 02:35:01,899 - INFO - Step 65352: {'train_loss_batch': 4.2600483894348145, 'train_lr': 0.08695175081644685, 'batch_time': 1.667455135389816, 'data_time': 0.013861658565229753}
2025-11-05 02:35:01 Train Epoch 026:  16%|█▌        | 400/2502 [11:06<57:11,  1.63s/it, Loss=3.4522, Top1=N/A, LR=0.086952]2025-11-05 02:37:46,820 - INFO - Step 65452: {'train_loss_batch': 4.461857795715332, 'train_lr': 0.08695175081644685, 'batch_time': 1.6629045865779506, 'data_time': 0.01065142671960845}
2025-11-05 02:37:46 Train Epoch 026:  20%|█▉        | 500/2502 [13:51<55:23,  1.66s/it, Loss=3.4418, Top1=N/A, LR=0.086952]2025-11-05 02:40:31,908 - INFO - Step 65552: {'train_loss_batch': 2.818962574005127, 'train_lr': 0.08695175081644685, 'batch_time': 1.6605047870300964, 'data_time': 0.008717638765742441}
2025-11-05 02:40:31 Train Epoch 026:  24%|██▍       | 600/2502 [16:37<52:34,  1.66s/it, Loss=3.4519, Top1=N/A, LR=0.086952]2025-11-05 02:43:17,688 - INFO - Step 65652: {'train_loss_batch': 2.897211790084839, 'train_lr': 0.08695175081644685, 'batch_time': 1.6600526002004816, 'data_time': 0.007430697439514262}
2025-11-05 02:43:17 Train Epoch 026:  28%|██▊       | 700/2502 [19:23<49:21,  1.64s/it, Loss=3.4644, Top1=N/A, LR=0.086952]2025-11-05 02:46:03,533 - INFO - Step 65752: {'train_loss_batch': 2.874274253845215, 'train_lr': 0.08695175081644685, 'batch_time': 1.6598249617724887, 'data_time': 0.006509057125249365}
2025-11-05 02:46:03 Train Epoch 026:  32%|███▏      | 800/2502 [22:08<47:03,  1.66s/it, Loss=3.4680, Top1=N/A, LR=0.086952]2025-11-05 02:48:48,337 - INFO - Step 65852: {'train_loss_batch': 4.289226055145264, 'train_lr': 0.08695175081644685, 'batch_time': 1.6583534150236703, 'data_time': 0.005812572629264231}
2025-11-05 02:48:48 Train Epoch 026:  36%|███▌      | 900/2502 [24:53<44:05,  1.65s/it, Loss=3.4706, Top1=N/A, LR=0.086952]2025-11-05 02:51:33,404 - INFO - Step 65952: {'train_loss_batch': 2.8519275188446045, 'train_lr': 0.08695175081644685, 'batch_time': 1.6575003527642356, 'data_time': 0.005278416399685842}
2025-11-05 02:51:33 Train Epoch 026:  40%|███▉      | 1000/2502 [27:38<41:05,  1.64s/it, Loss=3.4877, Top1=N/A, LR=0.086952]2025-11-05 02:54:18,766 - INFO - Step 66052: {'train_loss_batch': 2.798510789871216, 'train_lr': 0.08695175081644685, 'batch_time': 1.657112325226272, 'data_time': 0.004846160585706408}
2025-11-05 02:54:18 Train Epoch 026:  44%|████▍     | 1100/2502 [30:24<38:44,  1.66s/it, Loss=3.4823, Top1=55.83%, LR=0.086952]2025-11-05 02:57:04,073 - INFO - Step 66152: {'train_loss_batch': 2.7943308353424072, 'train_lr': 0.08695175081644685, 'batch_time': 1.656745270747255, 'data_time': 0.0044873442896705236}
2025-11-05 02:57:04 Train Epoch 026:  48%|████▊     | 1200/2502 [33:08<35:32,  1.64s/it, Loss=3.4807, Top1=N/A, LR=0.086952]   2025-11-05 02:59:48,664 - INFO - Step 66252: {'train_loss_batch': 2.813992977142334, 'train_lr': 0.08695175081644685, 'batch_time': 1.6558430707027871, 'data_time': 0.004190737758449075}
2025-11-05 02:59:48 Train Epoch 026:  52%|█████▏    | 1300/2502 [35:54<33:16,  1.66s/it, Loss=3.4857, Top1=N/A, LR=0.086952]2025-11-05 03:02:34,235 - INFO - Step 66352: {'train_loss_batch': 2.8633344173431396, 'train_lr': 0.08695175081644685, 'batch_time': 1.6558317353778578, 'data_time': 0.003938478290988884}
2025-11-05 03:02:34 Train Epoch 026:  56%|█████▌    | 1400/2502 [38:40<30:30,  1.66s/it, Loss=3.4845, Top1=N/A, LR=0.086952]2025-11-05 03:05:20,207 - INFO - Step 66452: {'train_loss_batch': 2.8257524967193604, 'train_lr': 0.08695175081644685, 'batch_time': 1.6561094861980168, 'data_time': 0.0037270121536962823}
2025-11-05 03:05:20 Train Epoch 026:  60%|█████▉    | 1500/2502 [41:25<27:38,  1.66s/it, Loss=3.4852, Top1=55.72%, LR=0.086952]2025-11-05 03:08:05,888 - INFO - Step 66552: {'train_loss_batch': 2.8143296241760254, 'train_lr': 0.08695175081644685, 'batch_time': 1.6561564520785683, 'data_time': 0.0035408043527825526}
2025-11-05 03:08:05 Train Epoch 026:  64%|██████▍   | 1600/2502 [44:11<24:50,  1.65s/it, Loss=3.4851, Top1=N/A, LR=0.086952]   2025-11-05 03:10:51,167 - INFO - Step 66652: {'train_loss_batch': 4.4183268547058105, 'train_lr': 0.08695175081644685, 'batch_time': 1.655946107003035, 'data_time': 0.0033781766742560955}
2025-11-05 03:10:51 Train Epoch 026:  68%|██████▊   | 1700/2502 [46:55<22:11,  1.66s/it, Loss=3.4869, Top1=N/A, LR=0.086952]2025-11-05 03:13:35,592 - INFO - Step 66752: {'train_loss_batch': 3.3484694957733154, 'train_lr': 0.08695175081644685, 'batch_time': 1.6552580395283103, 'data_time': 0.003235961884908435}
2025-11-05 03:13:35 Train Epoch 026:  72%|███████▏  | 1800/2502 [49:41<19:26,  1.66s/it, Loss=3.4904, Top1=N/A, LR=0.086952]2025-11-05 03:16:21,376 - INFO - Step 66852: {'train_loss_batch': 3.7876014709472656, 'train_lr': 0.08695175081644685, 'batch_time': 1.6554013214662033, 'data_time': 0.0031106527615493698}
2025-11-05 03:16:21 Train Epoch 026:  76%|███████▌  | 1900/2502 [52:27<16:39,  1.66s/it, Loss=3.4940, Top1=N/A, LR=0.086952]2025-11-05 03:19:07,299 - INFO - Step 66952: {'train_loss_batch': 3.9525623321533203, 'train_lr': 0.08695175081644685, 'batch_time': 1.6556023683502823, 'data_time': 0.0029970026843990543}
2025-11-05 03:19:07 Train Epoch 026:  80%|███████▉  | 2000/2502 [55:12<13:49,  1.65s/it, Loss=3.4990, Top1=N/A, LR=0.086952]2025-11-05 03:21:52,934 - INFO - Step 67052: {'train_loss_batch': 3.546548366546631, 'train_lr': 0.08695175081644685, 'batch_time': 1.6556399459543376, 'data_time': 0.002895148023255523}
2025-11-05 03:21:52 Train Epoch 026:  84%|████████▍ | 2100/2502 [57:58<11:06,  1.66s/it, Loss=3.4943, Top1=N/A, LR=0.086952]2025-11-05 03:24:38,620 - INFO - Step 67152: {'train_loss_batch': 2.9961764812469482, 'train_lr': 0.08695175081644685, 'batch_time': 1.655697874997923, 'data_time': 0.0028050186405971698}
2025-11-05 03:24:38 Train Epoch 026:  88%|████████▊ | 2200/2502 [1:00:44<08:19,  1.65s/it, Loss=3.5014, Top1=N/A, LR=0.086952]2025-11-05 03:27:24,088 - INFO - Step 67252: {'train_loss_batch': 3.227815628051758, 'train_lr': 0.08695175081644685, 'batch_time': 1.6556514463117045, 'data_time': 0.0027222125327679204}
2025-11-05 03:27:24 Train Epoch 026:  92%|█████████▏| 2300/2502 [1:03:28<05:30,  1.64s/it, Loss=3.5041, Top1=N/A, LR=0.086952]2025-11-05 03:30:08,528 - INFO - Step 67352: {'train_loss_batch': 3.9702630043029785, 'train_lr': 0.08695175081644685, 'batch_time': 1.6551624372284188, 'data_time': 0.0026437241530635988}
2025-11-05 03:30:08 Train Epoch 026:  96%|█████████▌| 2400/2502 [1:06:14<02:49,  1.66s/it, Loss=3.5049, Top1=55.56%, LR=0.086952]2025-11-05 03:32:54,032 - INFO - Step 67452: {'train_loss_batch': 2.839263439178467, 'train_lr': 0.08695175081644685, 'batch_time': 1.6551569778191353, 'data_time': 0.002572814309065762}
2025-11-05 03:32:54 Train Epoch 026: 100%|█████████▉| 2500/2502 [1:08:59<00:03,  1.66s/it, Loss=3.5082, Top1=N/A, LR=0.086952]   2025-11-05 03:35:39,858 - INFO - Step 67552: {'train_loss_batch': 3.818161964416504, 'train_lr': 0.08695175081644685, 'batch_time': 1.6552812580297775, 'data_time': 0.00255888683802602}
2025-11-05 03:35:39 Train Epoch 026: 100%|██████████| 2502/2502 [1:09:01<00:00,  1.66s/it, Loss=3.5082, Top1=N/A, LR=0.086952]
2025-11-05 03:35:42 Val Epoch 026:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 03:35:46   with torch.cuda.amp.autocast():
2025-11-05 03:35:47 Val Epoch 026: 100%|██████████| 98/98 [01:52<00:00,  1.15s/it, Loss=2.5742, Top1=59.46%, Top5=83.24%]
2025-11-05 03:37:34 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 03:37:34   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 03:37:34 2025-11-05 03:37:34,756 - INFO - Step 26: {'epoch': 26, 'learning_rate': 0.08568193966981556, 'train_loss': 3.507935093651763, 'train_top1': 55.5452759502924, 'train_top5': 78.66677326998051, 'train_precision': 55.208901283962575, 'train_recall': 55.37647711737732, 'train_f1': 55.05021969390423, 'val_loss': 2.5741720961761474, 'val_top1': 59.45600001220703, 'val_top5': 83.24200000732422, 'val_precision': 63.41437384523672, 'val_recall': 59.45399999999999, 'val_f1': 58.947659146997744}
2025-11-05 03:37:34 2025-11-05 03:37:34,757 - INFO - Epoch 026 Summary - LR: 0.085682, Train Loss: 3.5079, Val Loss: 2.5742, Val F1: 58.95%, Val Precision: 63.41%, Val Recall: 59.45%
2025-11-05 03:37:37 2025-11-05 03:37:37,973 - INFO - New best model saved with validation accuracy: 59.456%
2025-11-05 03:37:37 2025-11-05 03:37:37,973 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_027.pth
2025-11-05 03:37:38 Train Epoch 027:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 26 that is less than the current step 67552. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 03:37:41 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 03:37:41   with torch.cuda.amp.autocast():
2025-11-05 03:37:43 Train Epoch 027:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.4357, Top1=N/A, LR=0.085682]2025-11-05 03:37:43,231 - INFO - Step 67554: {'train_loss_batch': 3.435722827911377, 'train_lr': 0.08568193966981556, 'batch_time': 5.255894899368286, 'data_time': 3.6080503463745117}
2025-11-05 03:37:43 Train Epoch 027:   4%|▍         | 100/2502 [02:50<1:06:22,  1.66s/it, Loss=3.6543, Top1=N/A, LR=0.085682]2025-11-05 03:40:28,789 - INFO - Step 67654: {'train_loss_batch': 4.554903507232666, 'train_lr': 0.08568193966981556, 'batch_time': 1.6912362079809207, 'data_time': 0.0367086122531702}
2025-11-05 03:40:28 Train Epoch 027:   8%|▊         | 200/2502 [05:36<1:03:32,  1.66s/it, Loss=3.6059, Top1=N/A, LR=0.085682]2025-11-05 03:43:14,608 - INFO - Step 67754: {'train_loss_batch': 2.8160195350646973, 'train_lr': 0.08568193966981556, 'batch_time': 1.6747942777415412, 'data_time': 0.018962621688842773}
2025-11-05 03:43:14 Train Epoch 027:  12%|█▏        | 300/2502 [08:21<1:00:57,  1.66s/it, Loss=3.5632, Top1=N/A, LR=0.085682]2025-11-05 03:45:59,240 - INFO - Step 67854: {'train_loss_batch': 3.222996234893799, 'train_lr': 0.08568193966981556, 'batch_time': 1.6653343070781113, 'data_time': 0.012998743310719233}
2025-11-05 03:45:59 Train Epoch 027:  16%|█▌        | 400/2502 [11:06<58:11,  1.66s/it, Loss=3.5259, Top1=56.04%, LR=0.085682]2025-11-05 03:48:44,741 - INFO - Step 67954: {'train_loss_batch': 2.7486960887908936, 'train_lr': 0.08568193966981556, 'batch_time': 1.662757520366488, 'data_time': 0.010011782372681577}
2025-11-05 03:48:44 Train Epoch 027:  20%|█▉        | 500/2502 [13:51<54:44,  1.64s/it, Loss=3.5242, Top1=N/A, LR=0.085682]   2025-11-05 03:51:29,823 - INFO - Step 68054: {'train_loss_batch': 4.612336158752441, 'train_lr': 0.08568193966981556, 'batch_time': 1.6603754699349165, 'data_time': 0.008222214000191755}
2025-11-05 03:51:29 Train Epoch 027:  24%|██▍       | 600/2502 [16:36<52:17,  1.65s/it, Loss=3.5287, Top1=N/A, LR=0.085682]2025-11-05 03:54:14,677 - INFO - Step 68154: {'train_loss_batch': 2.8987820148468018, 'train_lr': 0.08568193966981556, 'batch_time': 1.6584042296830113, 'data_time': 0.007018637934857716}
2025-11-05 03:54:14 Train Epoch 027:  28%|██▊       | 700/2502 [19:22<49:52,  1.66s/it, Loss=3.5063, Top1=N/A, LR=0.085682]2025-11-05 03:57:00,491 - INFO - Step 68254: {'train_loss_batch': 3.6484880447387695, 'train_lr': 0.08568193966981556, 'batch_time': 1.658366730481854, 'data_time': 0.006163450178508241}
2025-11-05 03:57:00 Train Epoch 027:  32%|███▏      | 800/2502 [22:07<46:28,  1.64s/it, Loss=3.4998, Top1=55.81%, LR=0.085682]2025-11-05 03:59:45,546 - INFO - Step 68354: {'train_loss_batch': 2.903952121734619, 'train_lr': 0.08568193966981556, 'batch_time': 1.6573913305737404, 'data_time': 0.005511256490605005}
2025-11-05 03:59:45 Train Epoch 027:  36%|███▌      | 900/2502 [24:52<44:14,  1.66s/it, Loss=3.4920, Top1=N/A, LR=0.085682]   2025-11-05 04:02:30,137 - INFO - Step 68454: {'train_loss_batch': 4.141749382019043, 'train_lr': 0.08568193966981556, 'batch_time': 1.656116182875554, 'data_time': 0.005010607769169104}
2025-11-05 04:02:30 Train Epoch 027:  40%|███▉      | 1000/2502 [27:38<41:35,  1.66s/it, Loss=3.4910, Top1=55.81%, LR=0.085682]2025-11-05 04:05:16,110 - INFO - Step 68554: {'train_loss_batch': 2.730107307434082, 'train_lr': 0.08568193966981556, 'batch_time': 1.6564768737369961, 'data_time': 0.004609934218994506}
2025-11-05 04:05:16 Train Epoch 027:  44%|████▍     | 1100/2502 [30:24<38:53,  1.66s/it, Loss=3.4930, Top1=N/A, LR=0.085682]   2025-11-05 04:08:02,086 - INFO - Step 68654: {'train_loss_batch': 4.690322399139404, 'train_lr': 0.08568193966981556, 'batch_time': 1.656775143664063, 'data_time': 0.004278844751952238}
2025-11-05 04:08:02 Train Epoch 027:  48%|████▊     | 1200/2502 [33:09<35:50,  1.65s/it, Loss=3.5151, Top1=N/A, LR=0.085682]2025-11-05 04:10:47,646 - INFO - Step 68754: {'train_loss_batch': 3.8393306732177734, 'train_lr': 0.08568193966981556, 'batch_time': 1.656677222073227, 'data_time': 0.004005313812941933}
2025-11-05 04:10:47 Train Epoch 027:  52%|█████▏    | 1300/2502 [35:55<33:08,  1.65s/it, Loss=3.5174, Top1=N/A, LR=0.085682]2025-11-05 04:13:33,527 - INFO - Step 68854: {'train_loss_batch': 3.6455087661743164, 'train_lr': 0.08568193966981556, 'batch_time': 1.6568404261100118, 'data_time': 0.0037795683679719963}
2025-11-05 04:13:33 Train Epoch 027:  56%|█████▌    | 1400/2502 [38:41<30:34,  1.66s/it, Loss=3.5000, Top1=N/A, LR=0.085682]2025-11-05 04:16:19,108 - INFO - Step 68954: {'train_loss_batch': 3.114009141921997, 'train_lr': 0.08568193966981556, 'batch_time': 1.6567670567217765, 'data_time': 0.0035790581944837984}
2025-11-05 04:16:19 Train Epoch 027:  60%|█████▉    | 1500/2502 [41:27<27:44,  1.66s/it, Loss=3.5047, Top1=N/A, LR=0.085682]2025-11-05 04:19:05,150 - INFO - Step 69054: {'train_loss_batch': 4.554142951965332, 'train_lr': 0.08568193966981556, 'batch_time': 1.6570103222493089, 'data_time': 0.0034035765910291576}
2025-11-05 04:19:05 Train Epoch 027:  64%|██████▍   | 1600/2502 [44:12<24:45,  1.65s/it, Loss=3.5074, Top1=N/A, LR=0.085682]2025-11-05 04:21:50,516 - INFO - Step 69154: {'train_loss_batch': 2.735870122909546, 'train_lr': 0.08568193966981556, 'batch_time': 1.656800705518371, 'data_time': 0.0032516946500722203}
2025-11-05 04:21:50 Train Epoch 027:  68%|██████▊   | 1700/2502 [46:57<21:48,  1.63s/it, Loss=3.5168, Top1=55.52%, LR=0.085682]2025-11-05 04:24:35,668 - INFO - Step 69254: {'train_loss_batch': 2.7965221405029297, 'train_lr': 0.08568193966981556, 'batch_time': 1.6564901629172655, 'data_time': 0.0031188839257008743}
2025-11-05 04:24:35 Train Epoch 027:  72%|███████▏  | 1800/2502 [49:42<19:19,  1.65s/it, Loss=3.5122, Top1=N/A, LR=0.085682]   2025-11-05 04:27:20,395 - INFO - Step 69354: {'train_loss_batch': 4.280830383300781, 'train_lr': 0.08568193966981556, 'batch_time': 1.655978388947821, 'data_time': 0.0029973410553961314}
2025-11-05 04:27:20 Train Epoch 027:  76%|███████▌  | 1900/2502 [52:27<16:27,  1.64s/it, Loss=3.5101, Top1=N/A, LR=0.085682]2025-11-05 04:30:05,920 - INFO - Step 69454: {'train_loss_batch': 2.886927604675293, 'train_lr': 0.08568193966981556, 'batch_time': 1.655940017971097, 'data_time': 0.0028917216301466027}
2025-11-05 04:30:05 Train Epoch 027:  80%|███████▉  | 2000/2502 [55:12<13:49,  1.65s/it, Loss=3.5084, Top1=55.46%, LR=0.085682]2025-11-05 04:32:50,946 - INFO - Step 69554: {'train_loss_batch': 2.8715431690216064, 'train_lr': 0.08568193966981556, 'batch_time': 1.6556556309419295, 'data_time': 0.0027958903534301576}
2025-11-05 04:32:50 Train Epoch 027:  84%|████████▍ | 2100/2502 [57:58<11:06,  1.66s/it, Loss=3.5084, Top1=N/A, LR=0.085682]   2025-11-05 04:35:36,840 - INFO - Step 69654: {'train_loss_batch': 3.5214500427246094, 'train_lr': 0.08568193966981556, 'batch_time': 1.6558121993733497, 'data_time': 0.0027106906731091244}
2025-11-05 04:35:36 Train Epoch 027:  88%|████████▊ | 2200/2502 [1:00:43<08:18,  1.65s/it, Loss=3.5061, Top1=55.46%, LR=0.085682]2025-11-05 04:38:21,799 - INFO - Step 69754: {'train_loss_batch': 2.7843551635742188, 'train_lr': 0.08568193966981556, 'batch_time': 1.6555290383569006, 'data_time': 0.0026345467469953286}
2025-11-05 04:38:21 Train Epoch 027:  92%|█████████▏| 2300/2502 [1:03:29<05:35,  1.66s/it, Loss=3.5063, Top1=N/A, LR=0.085682]   2025-11-05 04:41:07,752 - INFO - Step 69854: {'train_loss_batch': 2.8610711097717285, 'train_lr': 0.08568193966981556, 'batch_time': 1.6557027772425155, 'data_time': 0.002563183538709605}
2025-11-05 04:41:07 Train Epoch 027:  96%|█████████▌| 2400/2502 [1:06:15<02:47,  1.64s/it, Loss=3.5075, Top1=N/A, LR=0.085682]2025-11-05 04:43:53,351 - INFO - Step 69954: {'train_loss_batch': 3.416282892227173, 'train_lr': 0.08568193966981556, 'batch_time': 1.655714714244126, 'data_time': 0.002497292518218524}
2025-11-05 04:43:53 Train Epoch 027: 100%|█████████▉| 2500/2502 [1:09:00<00:03,  1.67s/it, Loss=3.5072, Top1=N/A, LR=0.085682]2025-11-05 04:46:38,882 - INFO - Step 70054: {'train_loss_batch': 4.528814315795898, 'train_lr': 0.08568193966981556, 'batch_time': 1.655698535252647, 'data_time': 0.0024751465304381176}
2025-11-05 04:46:38 Train Epoch 027: 100%|██████████| 2502/2502 [1:09:02<00:00,  1.66s/it, Loss=3.5072, Top1=N/A, LR=0.085682]
2025-11-05 04:46:41 Val Epoch 027:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 04:46:45   with torch.cuda.amp.autocast():
2025-11-05 04:46:45 Val Epoch 027: 100%|██████████| 98/98 [01:48<00:00,  1.11s/it, Loss=2.5669, Top1=59.72%, Top5=83.61%]
2025-11-05 04:48:30 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 04:48:30   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 04:48:30 2025-11-05 04:48:30,201 - INFO - Step 27: {'epoch': 27, 'learning_rate': 0.0843633981150333, 'train_loss': 3.5075261651945535, 'train_top1': 55.42884332023576, 'train_top5': 78.87401768172889, 'train_precision': 55.13413863095414, 'train_recall': 55.34842685894308, 'train_f1': 55.00377130459893, 'val_loss': 2.5669148334503173, 'val_top1': 59.71999999267578, 'val_top5': 83.61400000488281, 'val_precision': 63.688652487671504, 'val_recall': 59.714, 'val_f1': 59.23355379359623}
2025-11-05 04:48:30 2025-11-05 04:48:30,202 - INFO - Epoch 027 Summary - LR: 0.084363, Train Loss: 3.5075, Val Loss: 2.5669, Val F1: 59.23%, Val Precision: 63.69%, Val Recall: 59.71%
2025-11-05 04:48:33 2025-11-05 04:48:33,182 - INFO - New best model saved with validation accuracy: 59.720%
2025-11-05 04:48:33 2025-11-05 04:48:33,182 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_028.pth
2025-11-05 04:48:33 Train Epoch 028:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 04:48:37   with torch.cuda.amp.autocast():
2025-11-05 04:48:38 wandb: WARNING Tried to log to step 27 that is less than the current step 70054. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 04:48:38 Train Epoch 028:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.8420, Top1=N/A, LR=0.084363]2025-11-05 04:48:38,868 - INFO - Step 70056: {'train_loss_batch': 2.8419933319091797, 'train_lr': 0.0843633981150333, 'batch_time': 5.683685302734375, 'data_time': 4.04521918296814}
2025-11-05 04:48:38 Train Epoch 028:   4%|▍         | 100/2502 [02:51<1:06:36,  1.66s/it, Loss=3.3407, Top1=N/A, LR=0.084363]2025-11-05 04:51:24,434 - INFO - Step 70156: {'train_loss_batch': 2.791182041168213, 'train_lr': 0.0843633981150333, 'batch_time': 1.6955441248298873, 'data_time': 0.041084235257441454}
2025-11-05 04:51:24 Train Epoch 028:   8%|▊         | 200/2502 [05:36<1:02:59,  1.64s/it, Loss=3.4637, Top1=N/A, LR=0.084363]2025-11-05 04:54:10,102 - INFO - Step 70256: {'train_loss_batch': 3.070636034011841, 'train_lr': 0.0843633981150333, 'batch_time': 1.676209499586874, 'data_time': 0.02114427979312726}
2025-11-05 04:54:10 Train Epoch 028:  12%|█▏        | 300/2502 [08:21<59:59,  1.63s/it, Loss=3.4603, Top1=N/A, LR=0.084363]2025-11-05 04:56:54,703 - INFO - Step 70356: {'train_loss_batch': 2.992626667022705, 'train_lr': 0.0843633981150333, 'batch_time': 1.666178008646664, 'data_time': 0.014427186642770356}
2025-11-05 04:56:54 Train Epoch 028:  16%|█▌        | 400/2502 [11:05<58:15,  1.66s/it, Loss=3.4657, Top1=55.81%, LR=0.084363]2025-11-05 04:59:38,945 - INFO - Step 70456: {'train_loss_batch': 2.8319497108459473, 'train_lr': 0.0843633981150333, 'batch_time': 1.660252323174417, 'data_time': 0.011074862278013157}
2025-11-05 04:59:38 Train Epoch 028:  20%|█▉        | 500/2502 [13:51<55:03,  1.65s/it, Loss=3.4527, Top1=N/A, LR=0.084363]   2025-11-05 05:02:24,452 - INFO - Step 70556: {'train_loss_batch': 2.683694839477539, 'train_lr': 0.0843633981150333, 'batch_time': 1.6592174347289308, 'data_time': 0.009053812769358743}
2025-11-05 05:02:24 Train Epoch 028:  24%|██▍       | 600/2502 [16:36<52:03,  1.64s/it, Loss=3.4530, Top1=N/A, LR=0.084363]2025-11-05 05:05:10,067 - INFO - Step 70656: {'train_loss_batch': 5.161252021789551, 'train_lr': 0.0843633981150333, 'batch_time': 1.6587058470372154, 'data_time': 0.007705674591953068}
2025-11-05 05:05:10 Train Epoch 028:  28%|██▊       | 700/2502 [19:22<49:32,  1.65s/it, Loss=3.4567, Top1=N/A, LR=0.084363]2025-11-05 05:07:55,212 - INFO - Step 70756: {'train_loss_batch': 4.3325347900390625, 'train_lr': 0.0843633981150333, 'batch_time': 1.6576712665476234, 'data_time': 0.0067560574127502005}
2025-11-05 05:07:55 Train Epoch 028:  32%|███▏      | 800/2502 [22:07<47:06,  1.66s/it, Loss=3.4776, Top1=N/A, LR=0.084363]2025-11-05 05:10:40,852 - INFO - Step 70856: {'train_loss_batch': 2.8010740280151367, 'train_lr': 0.0843633981150333, 'batch_time': 1.6575117757109072, 'data_time': 0.0060352290911918575}
2025-11-05 05:10:40 Train Epoch 028:  36%|███▌      | 900/2502 [24:53<44:17,  1.66s/it, Loss=3.4804, Top1=N/A, LR=0.084363]2025-11-05 05:13:26,911 - INFO - Step 70956: {'train_loss_batch': 4.536862373352051, 'train_lr': 0.0843633981150333, 'batch_time': 1.6578532450736296, 'data_time': 0.005469201804530475}
2025-11-05 05:13:27 Train Epoch 028:  40%|███▉      | 1000/2502 [27:39<41:38,  1.66s/it, Loss=3.4884, Top1=56.11%, LR=0.084363]2025-11-05 05:16:12,813 - INFO - Step 71056: {'train_loss_batch': 2.9211068153381348, 'train_lr': 0.0843633981150333, 'batch_time': 1.6579697796633908, 'data_time': 0.005019916282905327}
2025-11-05 05:16:12 Train Epoch 028:  44%|████▍     | 1100/2502 [30:24<38:29,  1.65s/it, Loss=3.4863, Top1=N/A, LR=0.084363]   2025-11-05 05:18:58,162 - INFO - Step 71156: {'train_loss_batch': 4.430702209472656, 'train_lr': 0.0843633981150333, 'batch_time': 1.6575621049259057, 'data_time': 0.004657479441241716}
2025-11-05 05:18:58 Train Epoch 028:  48%|████▊     | 1200/2502 [33:09<35:46,  1.65s/it, Loss=3.4933, Top1=N/A, LR=0.084363]2025-11-05 05:21:42,903 - INFO - Step 71256: {'train_loss_batch': 2.7586522102355957, 'train_lr': 0.0843633981150333, 'batch_time': 1.656717299025422, 'data_time': 0.0043549285542458715}
2025-11-05 05:21:42 Train Epoch 028:  52%|█████▏    | 1300/2502 [35:54<32:54,  1.64s/it, Loss=3.5030, Top1=N/A, LR=0.084363]2025-11-05 05:24:28,045 - INFO - Step 71356: {'train_loss_batch': 4.352156162261963, 'train_lr': 0.0843633981150333, 'batch_time': 1.6563096249864067, 'data_time': 0.0040961875079871144}
2025-11-05 05:24:28 Train Epoch 028:  56%|█████▌    | 1400/2502 [38:38<30:09,  1.64s/it, Loss=3.5007, Top1=N/A, LR=0.084363]2025-11-05 05:27:11,542 - INFO - Step 71456: {'train_loss_batch': 3.8040339946746826, 'train_lr': 0.0843633981150333, 'batch_time': 1.654786498609566, 'data_time': 0.0038749102947118023}
2025-11-05 05:27:11 Train Epoch 028:  60%|█████▉    | 1500/2502 [41:22<27:21,  1.64s/it, Loss=3.4883, Top1=55.91%, LR=0.084363]2025-11-05 05:29:55,779 - INFO - Step 71556: {'train_loss_batch': 3.0854685306549072, 'train_lr': 0.0843633981150333, 'batch_time': 1.6539588203277689, 'data_time': 0.003680974939995651}
2025-11-05 05:29:55 Train Epoch 028:  64%|██████▍   | 1600/2502 [44:07<24:40,  1.64s/it, Loss=3.4851, Top1=N/A, LR=0.084363]   2025-11-05 05:32:40,446 - INFO - Step 71656: {'train_loss_batch': 4.667278289794922, 'train_lr': 0.0843633981150333, 'batch_time': 1.6535038610014001, 'data_time': 0.003513342436219215}
2025-11-05 05:32:40 Train Epoch 028:  68%|██████▊   | 1700/2502 [46:52<22:02,  1.65s/it, Loss=3.4832, Top1=N/A, LR=0.084363]2025-11-05 05:35:25,817 - INFO - Step 71756: {'train_loss_batch': 2.691132068634033, 'train_lr': 0.0843633981150333, 'batch_time': 1.6535158818921645, 'data_time': 0.0033631314956602248}
2025-11-05 05:35:25 Train Epoch 028:  72%|███████▏  | 1800/2502 [49:38<19:20,  1.65s/it, Loss=3.4815, Top1=N/A, LR=0.084363]2025-11-05 05:38:11,363 - INFO - Step 71856: {'train_loss_batch': 3.3097891807556152, 'train_lr': 0.0843633981150333, 'batch_time': 1.6536236289870003, 'data_time': 0.0032285460493817453}
2025-11-05 05:38:11 Train Epoch 028:  76%|███████▌  | 1900/2502 [52:23<16:42,  1.67s/it, Loss=3.4832, Top1=55.86%, LR=0.084363]2025-11-05 05:40:56,546 - INFO - Step 71956: {'train_loss_batch': 2.893981695175171, 'train_lr': 0.0843633981150333, 'batch_time': 1.6535290840235213, 'data_time': 0.003110951840532635}
2025-11-05 05:40:56 Train Epoch 028:  80%|███████▉  | 2000/2502 [55:08<13:52,  1.66s/it, Loss=3.4810, Top1=55.83%, LR=0.084363]2025-11-05 05:43:41,997 - INFO - Step 72056: {'train_loss_batch': 2.8044533729553223, 'train_lr': 0.0843633981150333, 'batch_time': 1.6535778243442822, 'data_time': 0.0030055491701475924}
2025-11-05 05:43:42 Train Epoch 028:  84%|████████▍ | 2100/2502 [57:54<10:59,  1.64s/it, Loss=3.4878, Top1=N/A, LR=0.084363]   2025-11-05 05:46:27,543 - INFO - Step 72156: {'train_loss_batch': 4.362071990966797, 'train_lr': 0.0843633981150333, 'batch_time': 1.6536672175469596, 'data_time': 0.0029100432162169104}
2025-11-05 05:46:27 Train Epoch 028:  88%|████████▊ | 2200/2502 [1:00:38<08:20,  1.66s/it, Loss=3.4904, Top1=N/A, LR=0.084363]2025-11-05 05:49:12,162 - INFO - Step 72256: {'train_loss_batch': 2.9349217414855957, 'train_lr': 0.0843633981150333, 'batch_time': 1.6533276415152856, 'data_time': 0.0028224278449578916}
2025-11-05 05:49:12 Train Epoch 028:  92%|█████████▏| 2300/2502 [1:03:24<05:36,  1.66s/it, Loss=3.4878, Top1=55.80%, LR=0.084363]2025-11-05 05:51:57,747 - INFO - Step 72356: {'train_loss_batch': 2.8339335918426514, 'train_lr': 0.0843633981150333, 'batch_time': 1.6534372150043155, 'data_time': 0.0027407380302591666}
2025-11-05 05:51:57 Train Epoch 028:  96%|█████████▌| 2400/2502 [1:06:09<02:49,  1.66s/it, Loss=3.4834, Top1=N/A, LR=0.084363]   2025-11-05 05:54:42,804 - INFO - Step 72456: {'train_loss_batch': 2.8382017612457275, 'train_lr': 0.0843633981150333, 'batch_time': 1.6533178286173105, 'data_time': 0.0026656326181140853}
2025-11-05 05:54:42 Train Epoch 028: 100%|█████████▉| 2500/2502 [1:08:55<00:03,  1.66s/it, Loss=3.4864, Top1=55.72%, LR=0.084363]2025-11-05 05:57:28,525 - INFO - Step 72556: {'train_loss_batch': 2.6290855407714844, 'train_lr': 0.0843633981150333, 'batch_time': 1.6534733319463657, 'data_time': 0.0026445741512355018}
2025-11-05 05:57:28 Train Epoch 028: 100%|██████████| 2502/2502 [1:08:57<00:00,  1.65s/it, Loss=3.4864, Top1=55.72%, LR=0.084363]
2025-11-05 05:57:30 Val Epoch 028:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 05:57:35   with torch.cuda.amp.autocast():
2025-11-05 05:57:35 Val Epoch 028: 100%|██████████| 98/98 [01:52<00:00,  1.14s/it, Loss=2.5617, Top1=60.06%, Top5=83.74%]
2025-11-05 05:59:23 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 05:59:23   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 05:59:23 2025-11-05 05:59:23,108 - INFO - Step 28: {'epoch': 28, 'learning_rate': 0.08299792712126175, 'train_loss': 3.4861325314290803, 'train_top1': 55.72083938953488, 'train_top5': 78.8116218507752, 'train_precision': 55.44108279943134, 'train_recall': 55.55785922066696, 'train_f1': 55.270408858381856, 'val_loss': 2.5616916929626465, 'val_top1': 60.0560000012207, 'val_top5': 83.73600000976562, 'val_precision': 63.5718869358191, 'val_recall': 60.062000000000005, 'val_f1': 59.516870650909006}
2025-11-05 05:59:23 2025-11-05 05:59:23,110 - INFO - Epoch 028 Summary - LR: 0.082998, Train Loss: 3.4861, Val Loss: 2.5617, Val F1: 59.52%, Val Precision: 63.57%, Val Recall: 60.06%
2025-11-05 05:59:26 2025-11-05 05:59:26,681 - INFO - New best model saved with validation accuracy: 60.056%
2025-11-05 05:59:26 2025-11-05 05:59:26,682 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_029.pth
2025-11-05 05:59:26 Train Epoch 029:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 28 that is less than the current step 72556. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 05:59:30 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 05:59:30   with torch.cuda.amp.autocast():
2025-11-05 05:59:32 Train Epoch 029:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.1219, Top1=N/A, LR=0.082998]2025-11-05 05:59:32,008 - INFO - Step 72558: {'train_loss_batch': 4.121936798095703, 'train_lr': 0.08299792712126175, 'batch_time': 5.324838638305664, 'data_time': 3.680877208709717}
2025-11-05 05:59:32 Train Epoch 029:   4%|▍         | 100/2502 [02:50<1:06:04,  1.65s/it, Loss=3.3169, Top1=N/A, LR=0.082998]2025-11-05 06:02:17,550 - INFO - Step 72658: {'train_loss_batch': 3.300504684448242, 'train_lr': 0.08299792712126175, 'batch_time': 1.691756267358761, 'data_time': 0.03748313035115157}
2025-11-05 06:02:17 Train Epoch 029:   8%|▊         | 200/2502 [05:35<1:03:40,  1.66s/it, Loss=3.3946, Top1=N/A, LR=0.082998]2025-11-05 06:05:02,677 - INFO - Step 72758: {'train_loss_batch': 2.788323402404785, 'train_lr': 0.08299792712126175, 'batch_time': 1.6716112414402748, 'data_time': 0.019331022281551837}
2025-11-05 06:05:02 Train Epoch 029:  12%|█▏        | 300/2502 [08:21<1:00:17,  1.64s/it, Loss=3.4556, Top1=56.65%, LR=0.082998]2025-11-05 06:07:48,015 - INFO - Step 72858: {'train_loss_batch': 2.7325491905212402, 'train_lr': 0.08299792712126175, 'batch_time': 1.6655532339482608, 'data_time': 0.013241365502443028}
2025-11-05 06:07:48 Train Epoch 029:  16%|█▌        | 400/2502 [11:07<57:58,  1.65s/it, Loss=3.4738, Top1=N/A, LR=0.082998]   2025-11-05 06:10:33,718 - INFO - Step 72958: {'train_loss_batch': 4.943807601928711, 'train_lr': 0.08299792712126175, 'batch_time': 1.6634276436451367, 'data_time': 0.01017282847454423}
2025-11-05 06:10:33 Train Epoch 029:  20%|█▉        | 500/2502 [13:52<54:42,  1.64s/it, Loss=3.4785, Top1=N/A, LR=0.082998]2025-11-05 06:13:19,158 - INFO - Step 73058: {'train_loss_batch': 2.81847882270813, 'train_lr': 0.08299792712126175, 'batch_time': 1.661625133541054, 'data_time': 0.008347323316775871}
2025-11-05 06:13:19 Train Epoch 029:  24%|██▍       | 600/2502 [16:38<52:43,  1.66s/it, Loss=3.4657, Top1=56.30%, LR=0.082998]2025-11-05 06:16:04,778 - INFO - Step 73158: {'train_loss_batch': 2.807154417037964, 'train_lr': 0.08299792712126175, 'batch_time': 1.6607223481386155, 'data_time': 0.007127394493725058}
2025-11-05 06:16:04 Train Epoch 029:  28%|██▊       | 700/2502 [19:23<49:01,  1.63s/it, Loss=3.4828, Top1=N/A, LR=0.082998]   2025-11-05 06:18:50,072 - INFO - Step 73258: {'train_loss_batch': 3.2104053497314453, 'train_lr': 0.08299792712126175, 'batch_time': 1.6596108761731636, 'data_time': 0.0062500165294478525}
2025-11-05 06:18:50 Train Epoch 029:  32%|███▏      | 800/2502 [22:07<46:33,  1.64s/it, Loss=3.4894, Top1=N/A, LR=0.082998]2025-11-05 06:21:34,380 - INFO - Step 73358: {'train_loss_batch': 3.234569549560547, 'train_lr': 0.08299792712126175, 'batch_time': 1.6575472497761472, 'data_time': 0.005593485004744131}
2025-11-05 06:21:34 Train Epoch 029:  36%|███▌      | 900/2502 [24:52<43:54,  1.64s/it, Loss=3.4975, Top1=N/A, LR=0.082998]2025-11-05 06:24:18,928 - INFO - Step 73458: {'train_loss_batch': 3.913892984390259, 'train_lr': 0.08299792712126175, 'batch_time': 1.6562078226684334, 'data_time': 0.005082813934004399}
2025-11-05 06:24:18 Train Epoch 029:  40%|███▉      | 1000/2502 [27:37<41:18,  1.65s/it, Loss=3.4999, Top1=N/A, LR=0.082998]2025-11-05 06:27:04,397 - INFO - Step 73558: {'train_loss_batch': 2.8106305599212646, 'train_lr': 0.08299792712126175, 'batch_time': 1.6560563436159483, 'data_time': 0.004670966040718924}
2025-11-05 06:27:04 Train Epoch 029:  44%|████▍     | 1100/2502 [30:22<38:44,  1.66s/it, Loss=3.4972, Top1=N/A, LR=0.082998]2025-11-05 06:29:49,546 - INFO - Step 73658: {'train_loss_batch': 2.682741641998291, 'train_lr': 0.08299792712126175, 'batch_time': 1.655640769719428, 'data_time': 0.004338648403697833}
2025-11-05 06:29:49 Train Epoch 029:  48%|████▊     | 1200/2502 [33:07<36:04,  1.66s/it, Loss=3.4899, Top1=56.14%, LR=0.082998]2025-11-05 06:32:34,074 - INFO - Step 73758: {'train_loss_batch': 2.8742165565490723, 'train_lr': 0.08299792712126175, 'batch_time': 1.654778026919083, 'data_time': 0.00405969826208364}
2025-11-05 06:32:34 Train Epoch 029:  52%|█████▏    | 1300/2502 [35:53<33:20,  1.66s/it, Loss=3.4966, Top1=56.13%, LR=0.082998]2025-11-05 06:35:19,812 - INFO - Step 73858: {'train_loss_batch': 2.7939562797546387, 'train_lr': 0.08299792712126175, 'batch_time': 1.654977904751519, 'data_time': 0.0038215646736444097}
2025-11-05 06:35:19 Train Epoch 029:  56%|█████▌    | 1400/2502 [38:38<30:18,  1.65s/it, Loss=3.5024, Top1=N/A, LR=0.082998]   2025-11-05 06:38:05,220 - INFO - Step 73958: {'train_loss_batch': 4.008152008056641, 'train_lr': 0.08299792712126175, 'batch_time': 1.6549138762455682, 'data_time': 0.003617469793043334}
2025-11-05 06:38:05 Train Epoch 029:  60%|█████▉    | 1500/2502 [41:23<27:41,  1.66s/it, Loss=3.5048, Top1=56.08%, LR=0.082998]2025-11-05 06:40:50,401 - INFO - Step 74058: {'train_loss_batch': 2.748232364654541, 'train_lr': 0.08299792712126175, 'batch_time': 1.6547069433607473, 'data_time': 0.0034402034665488306}
2025-11-05 06:40:50 Train Epoch 029:  64%|██████▍   | 1600/2502 [44:08<24:44,  1.65s/it, Loss=3.4979, Top1=N/A, LR=0.082998]   2025-11-05 06:43:35,474 - INFO - Step 74158: {'train_loss_batch': 4.3487677574157715, 'train_lr': 0.08299792712126175, 'batch_time': 1.6544581469262414, 'data_time': 0.0032831358209689807}
2025-11-05 06:43:35 Train Epoch 029:  68%|██████▊   | 1700/2502 [46:53<22:07,  1.65s/it, Loss=3.4951, Top1=N/A, LR=0.082998]2025-11-05 06:46:20,631 - INFO - Step 74258: {'train_loss_batch': 2.7868425846099854, 'train_lr': 0.08299792712126175, 'batch_time': 1.6542882019460097, 'data_time': 0.003146401578576897}
2025-11-05 06:46:20 Train Epoch 029:  72%|███████▏  | 1800/2502 [49:39<19:15,  1.65s/it, Loss=3.4970, Top1=N/A, LR=0.082998]2025-11-05 06:49:06,281 - INFO - Step 74358: {'train_loss_batch': 3.1846096515655518, 'train_lr': 0.08299792712126175, 'batch_time': 1.6544108008226377, 'data_time': 0.0030271723692712354}
2025-11-05 06:49:06 Train Epoch 029:  76%|███████▌  | 1900/2502 [52:24<16:33,  1.65s/it, Loss=3.4952, Top1=N/A, LR=0.082998]2025-11-05 06:51:51,682 - INFO - Step 74458: {'train_loss_batch': 2.752441644668579, 'train_lr': 0.08299792712126175, 'batch_time': 1.6543898090822078, 'data_time': 0.002919994361271424}
2025-11-05 06:51:51 Train Epoch 029:  80%|███████▉  | 2000/2502 [55:10<13:48,  1.65s/it, Loss=3.4971, Top1=N/A, LR=0.082998]2025-11-05 06:54:37,571 - INFO - Step 74558: {'train_loss_batch': 3.2625505924224854, 'train_lr': 0.08299792712126175, 'batch_time': 1.6546146163101616, 'data_time': 0.0028235690704528716}
2025-11-05 06:54:37 Train Epoch 029:  84%|████████▍ | 2100/2502 [57:55<11:06,  1.66s/it, Loss=3.4963, Top1=N/A, LR=0.082998]2025-11-05 06:57:22,638 - INFO - Step 74658: {'train_loss_batch': 2.744570016860962, 'train_lr': 0.08299792712126175, 'batch_time': 1.654426742882345, 'data_time': 0.0027369489447381713}
2025-11-05 06:57:22 Train Epoch 029:  88%|████████▊ | 2200/2502 [1:00:41<08:15,  1.64s/it, Loss=3.4919, Top1=N/A, LR=0.082998]2025-11-05 07:00:07,750 - INFO - Step 74758: {'train_loss_batch': 2.9014124870300293, 'train_lr': 0.08299792712126175, 'batch_time': 1.6542765911142159, 'data_time': 0.002657463744465517}
2025-11-05 07:00:07 Train Epoch 029:  92%|█████████▏| 2300/2502 [1:03:25<05:34,  1.65s/it, Loss=3.4899, Top1=55.92%, LR=0.082998]2025-11-05 07:02:51,992 - INFO - Step 74858: {'train_loss_batch': 2.7598683834075928, 'train_lr': 0.08299792712126175, 'batch_time': 1.6537611361225497, 'data_time': 0.002580693575259345}
2025-11-05 07:02:51 Train Epoch 029:  96%|█████████▌| 2400/2502 [1:06:10<02:49,  1.66s/it, Loss=3.4824, Top1=55.91%, LR=0.082998]2025-11-05 07:05:37,603 - INFO - Step 74958: {'train_loss_batch': 2.797281265258789, 'train_lr': 0.08299792712126175, 'batch_time': 1.6538586301736065, 'data_time': 0.0025148462226419637}
2025-11-05 07:05:37 Train Epoch 029: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.66s/it, Loss=3.4840, Top1=N/A, LR=0.082998]   2025-11-05 07:08:23,187 - INFO - Step 75058: {'train_loss_batch': 4.263126850128174, 'train_lr': 0.08299792712126175, 'batch_time': 1.6539381255821342, 'data_time': 0.0024763964882186774}
2025-11-05 07:08:23 Train Epoch 029: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=3.4840, Top1=N/A, LR=0.082998]
2025-11-05 07:08:25 Val Epoch 029:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 07:08:29   with torch.cuda.amp.autocast():
2025-11-05 07:08:30 Val Epoch 029: 100%|██████████| 98/98 [01:53<00:00,  1.15s/it, Loss=2.6184, Top1=58.06%, Top5=82.43%]
2025-11-05 07:10:18 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 07:10:18   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 07:10:18 2025-11-05 07:10:18,547 - INFO - Step 29: {'epoch': 29, 'learning_rate': 0.08158739175763129, 'train_loss': 3.4836775313178414, 'train_top1': 55.92769724334601, 'train_top5': 79.09710670152091, 'train_precision': 55.634881436431236, 'train_recall': 55.794837855648716, 'train_f1': 55.477121106446205, 'val_loss': 2.618398388824463, 'val_top1': 58.05999998535156, 'val_top5': 82.4300000024414, 'val_precision': 62.38519797095482, 'val_recall': 58.052000000000014, 'val_f1': 57.408299033844834}
2025-11-05 07:10:18 2025-11-05 07:10:18,549 - INFO - Epoch 029 Summary - LR: 0.081587, Train Loss: 3.4837, Val Loss: 2.6184, Val F1: 57.41%, Val Precision: 62.39%, Val Recall: 58.05%
2025-11-05 07:10:19 2025-11-05 07:10:19,984 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_030.pth
2025-11-05 07:10:19 Train Epoch 030:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 07:10:23   with torch.cuda.amp.autocast():
2025-11-05 07:10:25 Train Epoch 030:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.7085, Top1=57.42%, LR=0.081587]2025-11-05 07:10:25,483 - INFO - Step 75060: {'train_loss_batch': 2.7085094451904297, 'train_lr': 0.08158739175763129, 'batch_time': 5.497268915176392, 'data_time': 3.839536666870117}
2025-11-05 07:10:25 Train Epoch 030:   0%|          | 2/2502 [00:07<2:14:34,  3.23s/it, Loss=2.7085, Top1=57.42%, LR=0.081587]wandb: WARNING Tried to log to step 29 that is less than the current step 75058. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 07:10:28 Train Epoch 030:   4%|▍         | 100/2502 [02:51<1:06:14,  1.65s/it, Loss=3.2533, Top1=56.18%, LR=0.081587]2025-11-05 07:13:11,184 - INFO - Step 75160: {'train_loss_batch': 2.813413143157959, 'train_lr': 0.08158739175763129, 'batch_time': 1.6950318270390576, 'data_time': 0.03901850351012579}
2025-11-05 07:13:11 Train Epoch 030:   8%|▊         | 200/2502 [05:36<1:03:51,  1.66s/it, Loss=3.3444, Top1=56.28%, LR=0.081587]2025-11-05 07:15:56,803 - INFO - Step 75260: {'train_loss_batch': 2.792452573776245, 'train_lr': 0.08158739175763129, 'batch_time': 1.6757070140459052, 'data_time': 0.02015315003656036}
2025-11-05 07:15:56 Train Epoch 030:  12%|█▏        | 300/2502 [08:22<1:00:53,  1.66s/it, Loss=3.3979, Top1=56.43%, LR=0.081587]2025-11-05 07:18:42,045 - INFO - Step 75360: {'train_loss_batch': 2.662620782852173, 'train_lr': 0.08158739175763129, 'batch_time': 1.6679700624903175, 'data_time': 0.013790152001618547}
2025-11-05 07:18:42 Train Epoch 030:  16%|█▌        | 400/2502 [11:07<58:05,  1.66s/it, Loss=3.4251, Top1=N/A, LR=0.081587]   2025-11-05 07:21:27,930 - INFO - Step 75460: {'train_loss_batch': 5.304274559020996, 'train_lr': 0.08158739175763129, 'batch_time': 1.66569497222615, 'data_time': 0.010602267900309955}
2025-11-05 07:21:27 Train Epoch 030:  20%|█▉        | 500/2502 [13:52<55:00,  1.65s/it, Loss=3.4292, Top1=N/A, LR=0.081587]2025-11-05 07:24:12,823 - INFO - Step 75560: {'train_loss_batch': 2.6362829208374023, 'train_lr': 0.08158739175763129, 'batch_time': 1.6623484974135896, 'data_time': 0.008681643270922754}
2025-11-05 07:24:12 Train Epoch 030:  24%|██▍       | 600/2502 [16:38<52:02,  1.64s/it, Loss=3.4322, Top1=N/A, LR=0.081587]2025-11-05 07:26:58,330 - INFO - Step 75660: {'train_loss_batch': 4.983476638793945, 'train_lr': 0.08158739175763129, 'batch_time': 1.6611370865794863, 'data_time': 0.007401696854145475}
2025-11-05 07:26:58 Train Epoch 030:  28%|██▊       | 700/2502 [19:22<49:13,  1.64s/it, Loss=3.4333, Top1=N/A, LR=0.081587]2025-11-05 07:29:42,954 - INFO - Step 75760: {'train_loss_batch': 3.3641586303710938, 'train_lr': 0.08158739175763129, 'batch_time': 1.6590109589096482, 'data_time': 0.006481005699930449}
2025-11-05 07:29:42 Train Epoch 030:  32%|███▏      | 800/2502 [22:08<47:06,  1.66s/it, Loss=3.4418, Top1=N/A, LR=0.081587]2025-11-05 07:32:28,528 - INFO - Step 75860: {'train_loss_batch': 3.657442331314087, 'train_lr': 0.08158739175763129, 'batch_time': 1.6586025377337852, 'data_time': 0.005795010913177376}
2025-11-05 07:32:28 Train Epoch 030:  36%|███▌      | 900/2502 [24:54<44:07,  1.65s/it, Loss=3.4450, Top1=N/A, LR=0.081587]2025-11-05 07:35:14,286 - INFO - Step 75960: {'train_loss_batch': 2.728776693344116, 'train_lr': 0.08158739175763129, 'batch_time': 1.6584887705685427, 'data_time': 0.005260086482955137}
2025-11-05 07:35:14 Train Epoch 030:  40%|███▉      | 1000/2502 [27:39<41:29,  1.66s/it, Loss=3.4405, Top1=N/A, LR=0.081587]2025-11-05 07:37:59,372 - INFO - Step 76060: {'train_loss_batch': 3.350731611251831, 'train_lr': 0.08158739175763129, 'batch_time': 1.657726657735956, 'data_time': 0.004828415669642247}
2025-11-05 07:37:59 Train Epoch 030:  44%|████▍     | 1100/2502 [30:24<38:47,  1.66s/it, Loss=3.4422, Top1=N/A, LR=0.081587]2025-11-05 07:40:44,849 - INFO - Step 76160: {'train_loss_batch': 3.3328068256378174, 'train_lr': 0.08158739175763129, 'batch_time': 1.6574580292610772, 'data_time': 0.0044733267497410025}
2025-11-05 07:40:44 Train Epoch 030:  48%|████▊     | 1200/2502 [33:10<36:01,  1.66s/it, Loss=3.4460, Top1=N/A, LR=0.081587]2025-11-05 07:43:30,100 - INFO - Step 76260: {'train_loss_batch': 4.908497333526611, 'train_lr': 0.08158739175763129, 'batch_time': 1.657045555551483, 'data_time': 0.0041827714810462715}
2025-11-05 07:43:30 Train Epoch 030:  52%|█████▏    | 1300/2502 [35:55<33:04,  1.65s/it, Loss=3.4457, Top1=N/A, LR=0.081587]2025-11-05 07:46:15,869 - INFO - Step 76360: {'train_loss_batch': 2.8314077854156494, 'train_lr': 0.08158739175763129, 'batch_time': 1.657094955444336, 'data_time': 0.003934354620837506}
2025-11-05 07:46:15 Train Epoch 030:  56%|█████▌    | 1400/2502 [38:41<30:18,  1.65s/it, Loss=3.4504, Top1=N/A, LR=0.081587]2025-11-05 07:49:01,237 - INFO - Step 76460: {'train_loss_batch': 3.6214418411254883, 'train_lr': 0.08158739175763129, 'batch_time': 1.6568514303510995, 'data_time': 0.003730550312638538}
2025-11-05 07:49:01 Train Epoch 030:  60%|█████▉    | 1500/2502 [41:26<27:35,  1.65s/it, Loss=3.4525, Top1=56.49%, LR=0.081587]2025-11-05 07:51:46,605 - INFO - Step 76560: {'train_loss_batch': 2.764263153076172, 'train_lr': 0.08158739175763129, 'batch_time': 1.656639729556364, 'data_time': 0.0035531285760563107}
2025-11-05 07:51:46 Train Epoch 030:  64%|██████▍   | 1600/2502 [44:11<24:55,  1.66s/it, Loss=3.4547, Top1=N/A, LR=0.081587]   2025-11-05 07:54:31,760 - INFO - Step 76660: {'train_loss_batch': 3.9601316452026367, 'train_lr': 0.08158739175763129, 'batch_time': 1.6563215954165247, 'data_time': 0.003394800152799474}
2025-11-05 07:54:31 Train Epoch 030:  68%|██████▊   | 1700/2502 [46:57<22:09,  1.66s/it, Loss=3.4572, Top1=N/A, LR=0.081587]2025-11-05 07:57:17,789 - INFO - Step 76760: {'train_loss_batch': 4.454273700714111, 'train_lr': 0.08158739175763129, 'batch_time': 1.6565546465228405, 'data_time': 0.0032580763644712383}
2025-11-05 07:57:17 Train Epoch 030:  72%|███████▏  | 1800/2502 [49:43<19:22,  1.66s/it, Loss=3.4563, Top1=N/A, LR=0.081587]2025-11-05 08:00:03,638 - INFO - Step 76860: {'train_loss_batch': 4.3961687088012695, 'train_lr': 0.08158739175763129, 'batch_time': 1.6566620116628321, 'data_time': 0.003134338277237472}
2025-11-05 08:00:03 Train Epoch 030:  76%|███████▌  | 1900/2502 [52:29<16:32,  1.65s/it, Loss=3.4562, Top1=56.38%, LR=0.081587]2025-11-05 08:02:49,165 - INFO - Step 76960: {'train_loss_batch': 2.753448963165283, 'train_lr': 0.08158739175763129, 'batch_time': 1.656589021436921, 'data_time': 0.003021113061077152}
2025-11-05 08:02:49 Train Epoch 030:  80%|███████▉  | 2000/2502 [55:14<13:54,  1.66s/it, Loss=3.4554, Top1=N/A, LR=0.081587]   2025-11-05 08:05:34,541 - INFO - Step 77060: {'train_loss_batch': 4.262657165527344, 'train_lr': 0.08158739175763129, 'batch_time': 1.6564472079098314, 'data_time': 0.0029228453276337295}
2025-11-05 08:05:34 Train Epoch 030:  84%|████████▍ | 2100/2502 [57:59<11:01,  1.65s/it, Loss=3.4553, Top1=N/A, LR=0.081587]2025-11-05 08:08:19,896 - INFO - Step 77160: {'train_loss_batch': 3.227548837661743, 'train_lr': 0.08158739175763129, 'batch_time': 1.6563091716103642, 'data_time': 0.0028328049018347848}
2025-11-05 08:08:19 Train Epoch 030:  88%|████████▊ | 2200/2502 [1:00:45<08:17,  1.65s/it, Loss=3.4584, Top1=N/A, LR=0.081587]2025-11-05 08:11:05,398 - INFO - Step 77260: {'train_loss_batch': 2.9112746715545654, 'train_lr': 0.08158739175763129, 'batch_time': 1.6562504562558178, 'data_time': 0.0027519124900682685}
2025-11-05 08:11:05 Train Epoch 030:  92%|█████████▏| 2300/2502 [1:03:30<05:31,  1.64s/it, Loss=3.4616, Top1=N/A, LR=0.081587]2025-11-05 08:13:50,582 - INFO - Step 77360: {'train_loss_batch': 2.8795559406280518, 'train_lr': 0.08158739175763129, 'batch_time': 1.6560586303899931, 'data_time': 0.002677513070336532}
2025-11-05 08:13:50 Train Epoch 030:  96%|█████████▌| 2400/2502 [1:06:14<02:46,  1.63s/it, Loss=3.4649, Top1=56.37%, LR=0.081587]2025-11-05 08:16:34,939 - INFO - Step 77460: {'train_loss_batch': 2.6414966583251953, 'train_lr': 0.08158739175763129, 'batch_time': 1.6555388116776968, 'data_time': 0.002605639910112069}
2025-11-05 08:16:34 Train Epoch 030: 100%|█████████▉| 2500/2502 [1:08:59<00:03,  1.67s/it, Loss=3.4652, Top1=N/A, LR=0.081587]   2025-11-05 08:19:19,217 - INFO - Step 77560: {'train_loss_batch': 4.434330463409424, 'train_lr': 0.08158739175763129, 'batch_time': 1.65502829057891, 'data_time': 0.00257970847305609}
2025-11-05 08:19:19 Train Epoch 030: 100%|██████████| 2502/2502 [1:09:01<00:00,  1.66s/it, Loss=3.4652, Top1=N/A, LR=0.081587]
2025-11-05 08:19:21 Val Epoch 030:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 08:19:25   with torch.cuda.amp.autocast():
2025-11-05 08:19:26 Val Epoch 030: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.5282, Top1=60.68%, Top5=83.91%]
2025-11-05 08:21:13 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 08:21:13   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 08:21:13 2025-11-05 08:21:13,467 - INFO - Step 30: {'epoch': 30, 'learning_rate': 0.08013371864578094, 'train_loss': 3.4649532168126886, 'train_top1': 56.33103788240918, 'train_top5': 79.40853549235182, 'train_precision': 56.032667479979025, 'train_recall': 56.193986393744076, 'train_f1': 55.88651078164906, 'val_loss': 2.5282179096984865, 'val_top1': 60.67799999511719, 'val_top5': 83.91199997314453, 'val_precision': 63.72200227232546, 'val_recall': 60.675999999999995, 'val_f1': 59.96437233161617}
2025-11-05 08:21:13 2025-11-05 08:21:13,468 - INFO - Epoch 030 Summary - LR: 0.080134, Train Loss: 3.4650, Val Loss: 2.5282, Val F1: 59.96%, Val Precision: 63.72%, Val Recall: 60.68%
2025-11-05 08:21:17 2025-11-05 08:21:17,004 - INFO - New best model saved with validation accuracy: 60.678%
2025-11-05 08:21:17 2025-11-05 08:21:17,004 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_031.pth
2025-11-05 08:21:17 Train Epoch 031:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 30 that is less than the current step 77560. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 08:21:20 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 08:21:20   with torch.cuda.amp.autocast():
2025-11-05 08:21:22 Train Epoch 031:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.7777, Top1=N/A, LR=0.080134]2025-11-05 08:21:22,196 - INFO - Step 77562: {'train_loss_batch': 3.777689218521118, 'train_lr': 0.08013371864578094, 'batch_time': 5.189721345901489, 'data_time': 3.5570836067199707}
2025-11-05 08:21:22 Train Epoch 031:   4%|▍         | 100/2502 [02:49<1:05:51,  1.65s/it, Loss=3.4123, Top1=55.85%, LR=0.080134]2025-11-05 08:24:06,881 - INFO - Step 77662: {'train_loss_batch': 2.899346351623535, 'train_lr': 0.08013371864578094, 'batch_time': 1.6819295694332312, 'data_time': 0.03625676419475291}
2025-11-05 08:24:06 Train Epoch 031:   8%|▊         | 200/2502 [05:33<1:03:09,  1.65s/it, Loss=3.4018, Top1=56.24%, LR=0.080134]2025-11-05 08:26:50,524 - INFO - Step 77762: {'train_loss_batch': 2.6800365447998047, 'train_lr': 0.08013371864578094, 'batch_time': 1.6592946028828028, 'data_time': 0.01869924151482274}
2025-11-05 08:26:50 Train Epoch 031:  12%|█▏        | 300/2502 [08:18<1:00:44,  1.66s/it, Loss=3.4314, Top1=N/A, LR=0.080134]   2025-11-05 08:29:35,117 - INFO - Step 77862: {'train_loss_batch': 2.816039800643921, 'train_lr': 0.08013371864578094, 'batch_time': 1.6548524718744018, 'data_time': 0.012814348322212498}
2025-11-05 08:29:35 Train Epoch 031:  16%|█▌        | 400/2502 [11:02<57:37,  1.64s/it, Loss=3.4564, Top1=N/A, LR=0.080134]2025-11-05 08:32:19,347 - INFO - Step 77962: {'train_loss_batch': 2.635240316390991, 'train_lr': 0.08013371864578094, 'batch_time': 1.6517213068698113, 'data_time': 0.009859856821949642}
2025-11-05 08:32:19 Train Epoch 031:  20%|█▉        | 500/2502 [13:48<55:13,  1.66s/it, Loss=3.4729, Top1=N/A, LR=0.080134]2025-11-05 08:35:05,045 - INFO - Step 78062: {'train_loss_batch': 2.8196005821228027, 'train_lr': 0.08013371864578094, 'batch_time': 1.6527722295886742, 'data_time': 0.008090161991690447}
2025-11-05 08:35:05 Train Epoch 031:  24%|██▍       | 600/2502 [16:33<52:19,  1.65s/it, Loss=3.4590, Top1=N/A, LR=0.080134]2025-11-05 08:37:50,366 - INFO - Step 78162: {'train_loss_batch': 4.431728363037109, 'train_lr': 0.08013371864578094, 'batch_time': 1.652843638783485, 'data_time': 0.00691584342728042}
2025-11-05 08:37:50 Train Epoch 031:  28%|██▊       | 700/2502 [19:17<49:22,  1.64s/it, Loss=3.4540, Top1=56.74%, LR=0.080134]2025-11-05 08:40:34,809 - INFO - Step 78262: {'train_loss_batch': 2.6681437492370605, 'train_lr': 0.08013371864578094, 'batch_time': 1.651642826246297, 'data_time': 0.006072902475376102}
2025-11-05 08:40:34 Train Epoch 031:  32%|███▏      | 800/2502 [22:02<47:01,  1.66s/it, Loss=3.4509, Top1=N/A, LR=0.080134]   2025-11-05 08:43:19,273 - INFO - Step 78362: {'train_loss_batch': 2.7650299072265625, 'train_lr': 0.08013371864578094, 'batch_time': 1.6507689851648948, 'data_time': 0.005437273211247019}
2025-11-05 08:43:19 Train Epoch 031:  36%|███▌      | 900/2502 [24:47<44:22,  1.66s/it, Loss=3.4481, Top1=N/A, LR=0.080134]2025-11-05 08:46:04,776 - INFO - Step 78462: {'train_loss_batch': 2.8224706649780273, 'train_lr': 0.08013371864578094, 'batch_time': 1.6512409865393094, 'data_time': 0.004934689842503555}
2025-11-05 08:46:04 Train Epoch 031:  40%|███▉      | 997/2502 [27:26<41:15,  1.64s/it, Loss=3.4481, Top1=N/A, LR=0.080134]
2025-11-05 08:48:49 Train Epoch 031:  44%|████▍     | 1100/2502 [30:18<38:51,  1.66s/it, Loss=3.4559, Top1=N/A, LR=0.080134]2025-11-05 08:51:35,730 - INFO - Step 78662: {'train_loss_batch': 2.7051804065704346, 'train_lr': 0.08013371864578094, 'batch_time': 1.6518814156642294, 'data_time': 0.0042165278522238525}
2025-11-05 08:51:35 Train Epoch 031:  48%|████▊     | 1200/2502 [33:04<36:00,  1.66s/it, Loss=3.4610, Top1=N/A, LR=0.080134]2025-11-05 08:54:21,697 - INFO - Step 78762: {'train_loss_batch': 3.949575424194336, 'train_lr': 0.08013371864578094, 'batch_time': 1.6525300408680175, 'data_time': 0.003949975490967102}
2025-11-05 08:54:21 Train Epoch 031:  52%|█████▏    | 1300/2502 [35:50<33:10,  1.66s/it, Loss=3.4508, Top1=N/A, LR=0.080134]2025-11-05 08:57:07,303 - INFO - Step 78862: {'train_loss_batch': 5.070660591125488, 'train_lr': 0.08013371864578094, 'batch_time': 1.6528014689569377, 'data_time': 0.003723851173864155}
2025-11-05 08:57:07 Train Epoch 031:  56%|█████▌    | 1400/2502 [38:35<30:11,  1.64s/it, Loss=3.4571, Top1=N/A, LR=0.080134]2025-11-05 08:59:52,310 - INFO - Step 78962: {'train_loss_batch': 4.462663650512695, 'train_lr': 0.08013371864578094, 'batch_time': 1.652606356237549, 'data_time': 0.003528498480100448}
2025-11-05 08:59:52 Train Epoch 031:  60%|█████▉    | 1500/2502 [41:20<27:23,  1.64s/it, Loss=3.4598, Top1=56.58%, LR=0.080134]2025-11-05 09:02:37,134 - INFO - Step 79062: {'train_loss_batch': 2.60951828956604, 'train_lr': 0.08013371864578094, 'batch_time': 1.6523149510687942, 'data_time': 0.003360185044991978}
2025-11-05 09:02:37 Train Epoch 031:  64%|██████▍   | 1600/2502 [44:03<24:43,  1.64s/it, Loss=3.4635, Top1=N/A, LR=0.080134]   2025-11-05 09:05:20,978 - INFO - Step 79162: {'train_loss_batch': 3.7292141914367676, 'train_lr': 0.08013371864578094, 'batch_time': 1.651448342444225, 'data_time': 0.003211548148208227}
2025-11-05 09:05:20 Train Epoch 031:  68%|██████▊   | 1700/2502 [46:48<21:59,  1.65s/it, Loss=3.4614, Top1=56.50%, LR=0.080134]2025-11-05 09:08:05,323 - INFO - Step 79262: {'train_loss_batch': 2.8252062797546387, 'train_lr': 0.08013371864578094, 'batch_time': 1.6509779302741694, 'data_time': 0.0030815172447729365}
2025-11-05 09:08:05 Train Epoch 031:  72%|███████▏  | 1800/2502 [49:32<19:11,  1.64s/it, Loss=3.4694, Top1=N/A, LR=0.080134]   2025-11-05 09:10:49,492 - INFO - Step 79362: {'train_loss_batch': 2.68877911567688, 'train_lr': 0.08013371864578094, 'batch_time': 1.6504623081073835, 'data_time': 0.0029650682346612995}
2025-11-05 09:10:49 Train Epoch 031:  76%|███████▌  | 1900/2502 [52:16<16:28,  1.64s/it, Loss=3.4781, Top1=N/A, LR=0.080134]2025-11-05 09:13:33,961 - INFO - Step 79462: {'train_loss_batch': 4.515216827392578, 'train_lr': 0.08013371864578094, 'batch_time': 1.6501586170086167, 'data_time': 0.0028610130412399237}
2025-11-05 09:13:33 Train Epoch 031:  80%|███████▉  | 2000/2502 [55:01<13:43,  1.64s/it, Loss=3.4773, Top1=N/A, LR=0.080134]2025-11-05 09:16:18,368 - INFO - Step 79562: {'train_loss_batch': 3.9778640270233154, 'train_lr': 0.08013371864578094, 'batch_time': 1.6498539927957774, 'data_time': 0.0027684533911785563}
2025-11-05 09:16:18 Train Epoch 031:  84%|████████▍ | 2100/2502 [57:45<10:59,  1.64s/it, Loss=3.4745, Top1=56.48%, LR=0.080134]2025-11-05 09:19:02,595 - INFO - Step 79662: {'train_loss_batch': 2.696755886077881, 'train_lr': 0.08013371864578094, 'batch_time': 1.6494928348183575, 'data_time': 0.0026821961918086}
2025-11-05 09:19:02 Train Epoch 031:  88%|████████▊ | 2200/2502 [1:00:30<08:20,  1.66s/it, Loss=3.4744, Top1=N/A, LR=0.080134]   2025-11-05 09:21:47,858 - INFO - Step 79762: {'train_loss_batch': 4.670254707336426, 'train_lr': 0.08013371864578094, 'batch_time': 1.6496353406139201, 'data_time': 0.002605958182071892}
2025-11-05 09:21:47 Train Epoch 031:  92%|█████████▏| 2300/2502 [1:03:16<05:35,  1.66s/it, Loss=3.4760, Top1=N/A, LR=0.080134]2025-11-05 09:24:33,978 - INFO - Step 79862: {'train_loss_batch': 4.761913299560547, 'train_lr': 0.08013371864578094, 'batch_time': 1.650137930422024, 'data_time': 0.0025379908494149433}
2025-11-05 09:24:33 Train Epoch 031:  96%|█████████▌| 2400/2502 [1:06:02<02:48,  1.65s/it, Loss=3.4761, Top1=N/A, LR=0.080134]2025-11-05 09:27:19,066 - INFO - Step 79962: {'train_loss_batch': 4.362293243408203, 'train_lr': 0.08013371864578094, 'batch_time': 1.6501685067645115, 'data_time': 0.002476024210626808}
2025-11-05 09:27:19 Train Epoch 031: 100%|█████████▉| 2500/2502 [1:08:47<00:03,  1.65s/it, Loss=3.4719, Top1=N/A, LR=0.080134]2025-11-05 09:30:04,076 - INFO - Step 80062: {'train_loss_batch': 2.7581968307495117, 'train_lr': 0.08013371864578094, 'batch_time': 1.6501657664418363, 'data_time': 0.00245146263317793}
2025-11-05 09:30:04 Train Epoch 031: 100%|██████████| 2502/2502 [1:08:48<00:00,  1.65s/it, Loss=3.4719, Top1=N/A, LR=0.080134]
2025-11-05 09:30:06 Val Epoch 031:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 09:30:10   with torch.cuda.amp.autocast():
2025-11-05 09:30:11 Val Epoch 031: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.5314, Top1=60.17%, Top5=84.27%]
2025-11-05 09:31:58 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 09:31:58   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 09:31:58 2025-11-05 09:31:58,092 - INFO - Step 31: {'epoch': 31, 'learning_rate': 0.07863889332832485, 'train_loss': 3.4726252080344087, 'train_top1': 56.45710198692153, 'train_top5': 79.45406815895372, 'train_precision': 56.1654065490082, 'train_recall': 56.32247142793025, 'train_f1': 56.003834816223275, 'val_loss': 2.531434672393799, 'val_top1': 60.171999978027344, 'val_top5': 84.26999997802734, 'val_precision': 64.22663051160215, 'val_recall': 60.175999999999995, 'val_f1': 59.81324203423504}
2025-11-05 09:31:58 2025-11-05 09:31:58,093 - INFO - Epoch 031 Summary - LR: 0.078639, Train Loss: 3.4726, Val Loss: 2.5314, Val F1: 59.81%, Val Precision: 64.23%, Val Recall: 60.18%
2025-11-05 09:31:58 wandb: WARNING Tried to log to step 31 that is less than the current step 80062. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 09:31:58 Train Epoch 032:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 09:32:03   with torch.cuda.amp.autocast():
2025-11-05 09:32:05 Train Epoch 032:   0%|          | 0/2502 [00:06<?, ?it/s, Loss=2.8796, Top1=55.66%, LR=0.078639]2025-11-05 09:32:05,479 - INFO - Step 80064: {'train_loss_batch': 2.8796212673187256, 'train_lr': 0.07863889332832485, 'batch_time': 6.552062273025513, 'data_time': 4.9254560470581055}
2025-11-05 09:32:05 Train Epoch 032:   4%|▍         | 100/2502 [02:50<1:05:47,  1.64s/it, Loss=3.3463, Top1=N/A, LR=0.078639]   2025-11-05 09:34:49,584 - INFO - Step 80164: {'train_loss_batch': 3.110110282897949, 'train_lr': 0.07863889332832485, 'batch_time': 1.689681636224879, 'data_time': 0.04977344286323774}
2025-11-05 09:34:49 Train Epoch 032:   8%|▊         | 200/2502 [05:34<1:02:35,  1.63s/it, Loss=3.4047, Top1=N/A, LR=0.078639]2025-11-05 09:37:33,678 - INFO - Step 80264: {'train_loss_batch': 4.205178260803223, 'train_lr': 0.07863889332832485, 'batch_time': 1.6654315160874704, 'data_time': 0.025517081739890635}
2025-11-05 09:37:33 Train Epoch 032:  12%|█▏        | 300/2502 [08:18<1:00:00,  1.63s/it, Loss=3.4387, Top1=N/A, LR=0.078639]2025-11-05 09:40:17,816 - INFO - Step 80364: {'train_loss_batch': 2.6571366786956787, 'train_lr': 0.07863889332832485, 'batch_time': 1.6574394005873672, 'data_time': 0.01736735188683798}
2025-11-05 09:40:17 Train Epoch 032:  16%|█▌        | 400/2502 [11:02<57:25,  1.64s/it, Loss=3.4539, Top1=N/A, LR=0.078639]2025-11-05 09:43:01,532 - INFO - Step 80464: {'train_loss_batch': 2.7960641384124756, 'train_lr': 0.07863889332832485, 'batch_time': 1.6523803124701293, 'data_time': 0.013280535576646761}
2025-11-05 09:43:01 Train Epoch 032:  20%|█▉        | 500/2502 [13:47<54:57,  1.65s/it, Loss=3.4454, Top1=N/A, LR=0.078639]2025-11-05 09:45:46,047 - INFO - Step 80564: {'train_loss_batch': 4.4354448318481445, 'train_lr': 0.07863889332832485, 'batch_time': 1.650937234094281, 'data_time': 0.010828254703514114}
2025-11-05 09:45:46 Train Epoch 032:  24%|██▍       | 600/2502 [16:31<51:58,  1.64s/it, Loss=3.4401, Top1=56.77%, LR=0.078639]2025-11-05 09:48:30,161 - INFO - Step 80664: {'train_loss_batch': 2.731735944747925, 'train_lr': 0.07863889332832485, 'batch_time': 1.6493061572660423, 'data_time': 0.00918687361845756}
2025-11-05 09:48:30 Train Epoch 032:  28%|██▊       | 700/2502 [19:15<49:41,  1.65s/it, Loss=3.4494, Top1=N/A, LR=0.078639]   2025-11-05 09:51:14,715 - INFO - Step 80764: {'train_loss_batch': 3.6645731925964355, 'train_lr': 0.07863889332832485, 'batch_time': 1.6487698565196038, 'data_time': 0.008022646421032523}
2025-11-05 09:51:14 Train Epoch 032:  32%|███▏      | 800/2502 [22:00<46:52,  1.65s/it, Loss=3.4528, Top1=N/A, LR=0.078639]2025-11-05 09:53:59,443 - INFO - Step 80864: {'train_loss_batch': 5.063437461853027, 'train_lr': 0.07863889332832485, 'batch_time': 1.6485834142539684, 'data_time': 0.007143908820944035}
2025-11-05 09:53:59 Train Epoch 032:  36%|███▌      | 900/2502 [24:45<44:07,  1.65s/it, Loss=3.4634, Top1=N/A, LR=0.078639]2025-11-05 09:56:43,974 - INFO - Step 80964: {'train_loss_batch': 2.7017197608947754, 'train_lr': 0.07863889332832485, 'batch_time': 1.6482195549879168, 'data_time': 0.006460478779478422}
2025-11-05 09:56:43 Train Epoch 032:  40%|███▉      | 1000/2502 [27:28<41:03,  1.64s/it, Loss=3.4625, Top1=N/A, LR=0.078639]2025-11-05 09:59:27,834 - INFO - Step 81064: {'train_loss_batch': 2.542245864868164, 'train_lr': 0.07863889332832485, 'batch_time': 1.6472586335002126, 'data_time': 0.00591214172370903}
2025-11-05 09:59:27 Train Epoch 032:  44%|████▍     | 1100/2502 [30:14<38:25,  1.64s/it, Loss=3.4466, Top1=56.68%, LR=0.078639]2025-11-05 10:02:13,102 - INFO - Step 81164: {'train_loss_batch': 2.5558221340179443, 'train_lr': 0.07863889332832485, 'batch_time': 1.6477506743248325, 'data_time': 0.005471388065847454}
2025-11-05 10:02:13 Train Epoch 032:  48%|████▊     | 1200/2502 [32:59<35:59,  1.66s/it, Loss=3.4382, Top1=N/A, LR=0.078639]   2025-11-05 10:04:58,319 - INFO - Step 81264: {'train_loss_batch': 4.282587051391602, 'train_lr': 0.07863889332832485, 'batch_time': 1.6481180099722348, 'data_time': 0.005098345476225155}
2025-11-05 10:04:58 Train Epoch 032:  52%|█████▏    | 1300/2502 [35:44<33:11,  1.66s/it, Loss=3.4403, Top1=N/A, LR=0.078639]2025-11-05 10:07:42,987 - INFO - Step 81364: {'train_loss_batch': 4.104085922241211, 'train_lr': 0.07863889332832485, 'batch_time': 1.6480073467023, 'data_time': 0.004783008943421396}
2025-11-05 10:07:42 Train Epoch 032:  56%|█████▌    | 1400/2502 [38:28<30:06,  1.64s/it, Loss=3.4464, Top1=56.70%, LR=0.078639]2025-11-05 10:10:27,468 - INFO - Step 81464: {'train_loss_batch': 2.76468563079834, 'train_lr': 0.07863889332832485, 'batch_time': 1.6477791415547405, 'data_time': 0.004511069094258321}
2025-11-05 10:10:27 Train Epoch 032:  60%|█████▉    | 1500/2502 [41:13<27:41,  1.66s/it, Loss=3.4432, Top1=56.68%, LR=0.078639]2025-11-05 10:13:12,604 - INFO - Step 81564: {'train_loss_batch': 2.703007459640503, 'train_lr': 0.07863889332832485, 'batch_time': 1.648017254771589, 'data_time': 0.004279460690960258}
2025-11-05 10:13:12 Train Epoch 032:  64%|██████▍   | 1600/2502 [43:59<24:53,  1.66s/it, Loss=3.4429, Top1=N/A, LR=0.078639]   2025-11-05 10:15:58,396 - INFO - Step 81664: {'train_loss_batch': 3.3456006050109863, 'train_lr': 0.07863889332832485, 'batch_time': 1.6486359379724886, 'data_time': 0.004076410575928649}
2025-11-05 10:15:58 Train Epoch 032:  68%|██████▊   | 1700/2502 [46:45<22:07,  1.65s/it, Loss=3.4383, Top1=N/A, LR=0.078639]2025-11-05 10:18:44,279 - INFO - Step 81764: {'train_loss_batch': 3.9871582984924316, 'train_lr': 0.07863889332832485, 'batch_time': 1.6492348877841203, 'data_time': 0.0038963866752431647}
2025-11-05 10:18:44 Train Epoch 032:  72%|███████▏  | 1800/2502 [49:30<19:10,  1.64s/it, Loss=3.4410, Top1=N/A, LR=0.078639]2025-11-05 10:21:29,448 - INFO - Step 81864: {'train_loss_batch': 2.5912437438964844, 'train_lr': 0.07863889332832485, 'batch_time': 1.6493709266086474, 'data_time': 0.0037366973235168966}
2025-11-05 10:21:29 Train Epoch 032:  76%|███████▌  | 1900/2502 [52:14<16:18,  1.63s/it, Loss=3.4401, Top1=N/A, LR=0.078639]2025-11-05 10:24:13,632 - INFO - Step 81964: {'train_loss_batch': 5.186938762664795, 'train_lr': 0.07863889332832485, 'batch_time': 1.648975058645402, 'data_time': 0.00359259712765055}
2025-11-05 10:24:13 Train Epoch 032:  80%|███████▉  | 2000/2502 [54:57<13:44,  1.64s/it, Loss=3.4383, Top1=N/A, LR=0.078639]2025-11-05 10:26:56,684 - INFO - Step 82064: {'train_loss_batch': 4.574719429016113, 'train_lr': 0.07863889332832485, 'batch_time': 1.6480523895347554, 'data_time': 0.003459280339078508}
2025-11-05 10:26:56 Train Epoch 032:  84%|████████▍ | 2100/2502 [57:42<10:58,  1.64s/it, Loss=3.4401, Top1=N/A, LR=0.078639]2025-11-05 10:29:41,080 - INFO - Step 82164: {'train_loss_batch': 3.089435338973999, 'train_lr': 0.07863889332832485, 'batch_time': 1.6478574617541104, 'data_time': 0.0033423695207947155}
2025-11-05 10:29:41 Train Epoch 032:  88%|████████▊ | 2200/2502 [1:00:26<08:17,  1.65s/it, Loss=3.4381, Top1=N/A, LR=0.078639]2025-11-05 10:32:25,338 - INFO - Step 82264: {'train_loss_batch': 5.237832069396973, 'train_lr': 0.07863889332832485, 'batch_time': 1.6476175463345852, 'data_time': 0.0032367557896532183}
2025-11-05 10:32:25 Train Epoch 032:  92%|█████████▏| 2300/2502 [1:03:10<05:31,  1.64s/it, Loss=3.4368, Top1=56.64%, LR=0.078639]2025-11-05 10:35:09,907 - INFO - Step 82364: {'train_loss_batch': 2.7289226055145264, 'train_lr': 0.07863889332832485, 'batch_time': 1.6475334787099374, 'data_time': 0.0031375078883496433}
2025-11-05 10:35:09 Train Epoch 032:  96%|█████████▌| 2400/2502 [1:05:55<02:47,  1.64s/it, Loss=3.4402, Top1=N/A, LR=0.078639]   2025-11-05 10:37:53,996 - INFO - Step 82464: {'train_loss_batch': 4.395738124847412, 'train_lr': 0.07863889332832485, 'batch_time': 1.6472570659815395, 'data_time': 0.0030515296217899727}
2025-11-05 10:37:53 Train Epoch 032: 100%|█████████▉| 2500/2502 [1:08:39<00:03,  1.65s/it, Loss=3.4416, Top1=N/A, LR=0.078639]2025-11-05 10:40:38,584 - INFO - Step 82564: {'train_loss_batch': 4.425047874450684, 'train_lr': 0.07863889332832485, 'batch_time': 1.6472016496212185, 'data_time': 0.00299134744448168}
2025-11-05 10:40:38 Train Epoch 032: 100%|██████████| 2502/2502 [1:08:41<00:00,  1.65s/it, Loss=3.4416, Top1=N/A, LR=0.078639]
2025-11-05 10:40:40 Val Epoch 032:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 10:40:45   with torch.cuda.amp.autocast():
2025-11-05 10:40:45 Val Epoch 032: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=2.4922, Top1=61.33%, Top5=84.46%]
2025-11-05 10:42:31 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 10:42:31   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 10:42:31 2025-11-05 10:42:31,473 - INFO - Step 32: {'epoch': 32, 'learning_rate': 0.07710495755684012, 'train_loss': 3.4418738752627354, 'train_top1': 56.636679292929294, 'train_top5': 79.59043560606061, 'train_precision': 56.33493387290084, 'train_recall': 56.49402940957899, 'train_f1': 56.18973300669947, 'val_loss': 2.49221400680542, 'val_top1': 61.32600000366211, 'val_top5': 84.4580000048828, 'val_precision': 64.45277169000174, 'val_recall': 61.324, 'val_f1': 60.85946577387277}
2025-11-05 10:42:31 2025-11-05 10:42:31,475 - INFO - Epoch 032 Summary - LR: 0.077105, Train Loss: 3.4419, Val Loss: 2.4922, Val F1: 60.86%, Val Precision: 64.45%, Val Recall: 61.32%
2025-11-05 10:42:34 2025-11-05 10:42:34,969 - INFO - New best model saved with validation accuracy: 61.326%
2025-11-05 10:42:34 2025-11-05 10:42:34,970 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_033.pth
2025-11-05 10:42:34 Train Epoch 033:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 32 that is less than the current step 82564. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 10:42:39 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 10:42:39   with torch.cuda.amp.autocast():
2025-11-05 10:42:40 Train Epoch 033:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.7778, Top1=N/A, LR=0.077105]2025-11-05 10:42:40,832 - INFO - Step 82566: {'train_loss_batch': 2.777766466140747, 'train_lr': 0.07710495755684012, 'batch_time': 5.858994245529175, 'data_time': 4.219869136810303}
2025-11-05 10:42:40 Train Epoch 033:   4%|▍         | 100/2502 [02:49<1:06:16,  1.66s/it, Loss=3.4511, Top1=N/A, LR=0.077105]2025-11-05 10:45:24,877 - INFO - Step 82666: {'train_loss_batch': 2.7404232025146484, 'train_lr': 0.07710495755684012, 'batch_time': 1.6822223852176477, 'data_time': 0.04277004581866878}
2025-11-05 10:45:24 Train Epoch 033:   8%|▊         | 200/2502 [05:35<1:03:50,  1.66s/it, Loss=3.4343, Top1=57.63%, LR=0.077105]2025-11-05 10:48:10,780 - INFO - Step 82766: {'train_loss_batch': 2.688420295715332, 'train_lr': 0.07710495755684012, 'batch_time': 1.6706853826247638, 'data_time': 0.02195347600908422}
2025-11-05 10:48:10 Train Epoch 033:  12%|█▏        | 300/2502 [08:21<1:00:47,  1.66s/it, Loss=3.4277, Top1=57.16%, LR=0.077105]2025-11-05 10:50:56,586 - INFO - Step 82866: {'train_loss_batch': 2.8182501792907715, 'train_lr': 0.07710495755684012, 'batch_time': 1.66648957024381, 'data_time': 0.014988329719467416}
2025-11-05 10:50:56 Train Epoch 033:  16%|█▌        | 400/2502 [11:07<57:43,  1.65s/it, Loss=3.4279, Top1=N/A, LR=0.077105]   2025-11-05 10:53:42,063 - INFO - Step 82966: {'train_loss_batch': 4.673815727233887, 'train_lr': 0.07710495755684012, 'batch_time': 1.663567467520659, 'data_time': 0.011492496714033093}
2025-11-05 10:53:42 Train Epoch 033:  20%|█▉        | 500/2502 [13:51<55:28,  1.66s/it, Loss=3.4287, Top1=57.12%, LR=0.077105]2025-11-05 10:56:26,375 - INFO - Step 83066: {'train_loss_batch': 2.704315662384033, 'train_lr': 0.07710495755684012, 'batch_time': 1.659484028102395, 'data_time': 0.009389665550338532}
2025-11-05 10:56:26 Train Epoch 033:  24%|██▍       | 600/2502 [16:37<52:23,  1.65s/it, Loss=3.4226, Top1=N/A, LR=0.077105]   2025-11-05 10:59:12,192 - INFO - Step 83166: {'train_loss_batch': 4.030023574829102, 'train_lr': 0.07710495755684012, 'batch_time': 1.6592649699447555, 'data_time': 0.007992693667800574}
2025-11-05 10:59:12 Train Epoch 033:  28%|██▊       | 700/2502 [19:22<49:24,  1.65s/it, Loss=3.4356, Top1=N/A, LR=0.077105]2025-11-05 11:01:57,174 - INFO - Step 83266: {'train_loss_batch': 3.815431594848633, 'train_lr': 0.07710495755684012, 'batch_time': 1.657917775373146, 'data_time': 0.006990334106750053}
2025-11-05 11:01:57 Train Epoch 033:  32%|███▏      | 800/2502 [22:07<46:43,  1.65s/it, Loss=3.4418, Top1=N/A, LR=0.077105]2025-11-05 11:04:42,433 - INFO - Step 83366: {'train_loss_batch': 3.0394949913024902, 'train_lr': 0.07710495755684012, 'batch_time': 1.6572528656353516, 'data_time': 0.006232543533363295}
2025-11-05 11:04:42 Train Epoch 033:  36%|███▌      | 900/2502 [24:53<44:14,  1.66s/it, Loss=3.4355, Top1=N/A, LR=0.077105]2025-11-05 11:07:28,088 - INFO - Step 83466: {'train_loss_batch': 4.265652656555176, 'train_lr': 0.07710495755684012, 'batch_time': 1.6571743676188784, 'data_time': 0.005649386976455875}
2025-11-05 11:07:28 Train Epoch 033:  40%|███▉      | 1000/2502 [27:38<41:32,  1.66s/it, Loss=3.4413, Top1=N/A, LR=0.077105]2025-11-05 11:10:13,966 - INFO - Step 83566: {'train_loss_batch': 2.646603584289551, 'train_lr': 0.07710495755684012, 'batch_time': 1.6573337341522003, 'data_time': 0.005184461067725609}
2025-11-05 11:10:13 Train Epoch 033:  44%|████▍     | 1100/2502 [30:24<38:48,  1.66s/it, Loss=3.4468, Top1=N/A, LR=0.077105]2025-11-05 11:12:59,963 - INFO - Step 83666: {'train_loss_batch': 3.2614872455596924, 'train_lr': 0.07710495755684012, 'batch_time': 1.6575732737860822, 'data_time': 0.004801879894939149}
2025-11-05 11:12:59 Train Epoch 033:  48%|████▊     | 1200/2502 [33:09<35:42,  1.65s/it, Loss=3.4447, Top1=N/A, LR=0.077105]2025-11-05 11:15:44,331 - INFO - Step 83766: {'train_loss_batch': 2.865377426147461, 'train_lr': 0.07710495755684012, 'batch_time': 1.656416193829488, 'data_time': 0.004481329905996712}
2025-11-05 11:15:44 Train Epoch 033:  52%|█████▏    | 1300/2502 [35:53<32:44,  1.63s/it, Loss=3.4494, Top1=N/A, LR=0.077105]2025-11-05 11:18:28,892 - INFO - Step 83866: {'train_loss_batch': 4.602596759796143, 'train_lr': 0.07710495755684012, 'batch_time': 1.6555860757277985, 'data_time': 0.00421150256632659}
2025-11-05 11:18:28 Train Epoch 033:  56%|█████▌    | 1400/2502 [38:38<30:14,  1.65s/it, Loss=3.4492, Top1=57.11%, LR=0.077105]2025-11-05 11:21:13,020 - INFO - Step 83966: {'train_loss_batch': 2.862699031829834, 'train_lr': 0.07710495755684012, 'batch_time': 1.6545642453546954, 'data_time': 0.003975936466246993}
2025-11-05 11:21:13 Train Epoch 033:  60%|█████▉    | 1500/2502 [41:22<27:28,  1.65s/it, Loss=3.4471, Top1=57.08%, LR=0.077105]2025-11-05 11:23:57,962 - INFO - Step 84066: {'train_loss_batch': 2.8298707008361816, 'train_lr': 0.07710495755684012, 'batch_time': 1.654221420364329, 'data_time': 0.003774576072769114}
2025-11-05 11:23:57 Train Epoch 033:  64%|██████▍   | 1600/2502 [44:08<25:00,  1.66s/it, Loss=3.4500, Top1=57.15%, LR=0.077105]2025-11-05 11:26:43,071 - INFO - Step 84166: {'train_loss_batch': 2.6586453914642334, 'train_lr': 0.07710495755684012, 'batch_time': 1.6540260982096455, 'data_time': 0.003598094656644651}
2025-11-05 11:26:43 Train Epoch 033:  68%|██████▊   | 1700/2502 [46:54<22:09,  1.66s/it, Loss=3.4453, Top1=57.19%, LR=0.077105]2025-11-05 11:29:28,993 - INFO - Step 84266: {'train_loss_batch': 2.7087182998657227, 'train_lr': 0.07710495755684012, 'batch_time': 1.6543313484203108, 'data_time': 0.0034423432302503006}
2025-11-05 11:29:28 Train Epoch 033:  72%|███████▏  | 1800/2502 [49:39<19:21,  1.66s/it, Loss=3.4437, Top1=57.21%, LR=0.077105]2025-11-05 11:32:14,522 - INFO - Step 84366: {'train_loss_batch': 2.648575782775879, 'train_lr': 0.07710495755684012, 'batch_time': 1.6543840485635828, 'data_time': 0.00330235824394332}
2025-11-05 11:32:14 Train Epoch 033:  76%|███████▌  | 1900/2502 [52:24<16:35,  1.65s/it, Loss=3.4389, Top1=N/A, LR=0.077105]   2025-11-05 11:34:59,503 - INFO - Step 84466: {'train_loss_batch': 4.569808006286621, 'train_lr': 0.07710495755684012, 'batch_time': 1.6541437797957756, 'data_time': 0.003176878026634188}
2025-11-05 11:34:59 Train Epoch 033:  80%|███████▉  | 2000/2502 [55:10<13:42,  1.64s/it, Loss=3.4334, Top1=N/A, LR=0.077105]2025-11-05 11:37:45,037 - INFO - Step 84566: {'train_loss_batch': 4.747891426086426, 'train_lr': 0.07710495755684012, 'batch_time': 1.6542033347292342, 'data_time': 0.0030678100195126435}
2025-11-05 11:37:45 Train Epoch 033:  84%|████████▍ | 2100/2502 [57:54<11:02,  1.65s/it, Loss=3.4287, Top1=N/A, LR=0.077105]2025-11-05 11:40:29,543 - INFO - Step 84666: {'train_loss_batch': 2.84252667427063, 'train_lr': 0.07710495755684012, 'batch_time': 1.6537680114126954, 'data_time': 0.0029662641555226912}
2025-11-05 11:40:29 Train Epoch 033:  88%|████████▊ | 2200/2502 [1:00:39<08:16,  1.64s/it, Loss=3.4369, Top1=57.11%, LR=0.077105]2025-11-05 11:43:14,454 - INFO - Step 84766: {'train_loss_batch': 2.7960398197174072, 'train_lr': 0.07710495755684012, 'batch_time': 1.6535560287491184, 'data_time': 0.0028753435541315005}
2025-11-05 11:43:14 Train Epoch 033:  92%|█████████▏| 2300/2502 [1:03:23<05:32,  1.64s/it, Loss=3.4389, Top1=N/A, LR=0.077105]   2025-11-05 11:45:58,829 - INFO - Step 84866: {'train_loss_batch': 2.75002121925354, 'train_lr': 0.07710495755684012, 'batch_time': 1.65312986589421, 'data_time': 0.002789929761724957}
2025-11-05 11:45:58 Train Epoch 033:  96%|█████████▌| 2400/2502 [1:06:08<02:48,  1.65s/it, Loss=3.4382, Top1=N/A, LR=0.077105]2025-11-05 11:48:43,398 - INFO - Step 84966: {'train_loss_batch': 4.698325157165527, 'train_lr': 0.07710495755684012, 'batch_time': 1.6528199318794845, 'data_time': 0.0027124869828420795}
2025-11-05 11:48:43 Train Epoch 033: 100%|█████████▉| 2500/2502 [1:08:53<00:03,  1.66s/it, Loss=3.4375, Top1=N/A, LR=0.077105]2025-11-05 11:51:28,286 - INFO - Step 85066: {'train_loss_batch': 3.358088731765747, 'train_lr': 0.07710495755684012, 'batch_time': 1.6526624066788689, 'data_time': 0.0026639256178021954}
2025-11-05 11:51:28 Train Epoch 033: 100%|██████████| 2502/2502 [1:08:55<00:00,  1.65s/it, Loss=3.4375, Top1=N/A, LR=0.077105]
2025-11-05 11:51:30 Val Epoch 033:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 11:51:34   with torch.cuda.amp.autocast():
2025-11-05 11:51:35 Val Epoch 033: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=2.5278, Top1=60.92%, Top5=84.15%]
2025-11-05 11:53:21 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 11:53:21   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 11:53:21 2025-11-05 11:53:21,161 - INFO - Step 33: {'epoch': 33, 'learning_rate': 0.07553400650307944, 'train_loss': 3.437239844450276, 'train_top1': 57.12403402928417, 'train_top5': 79.92136659436008, 'train_precision': 56.81921798688816, 'train_recall': 56.985520558819914, 'train_f1': 56.67344472400525, 'val_loss': 2.527809720001221, 'val_top1': 60.92399999023438, 'val_top5': 84.1519999951172, 'val_precision': 64.2656402252768, 'val_recall': 60.922, 'val_f1': 60.273319937480665}
2025-11-05 11:53:21 2025-11-05 11:53:21,163 - INFO - Epoch 033 Summary - LR: 0.075534, Train Loss: 3.4372, Val Loss: 2.5278, Val F1: 60.27%, Val Precision: 64.27%, Val Recall: 60.92%
2025-11-05 11:53:21 Train Epoch 034:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 11:53:25   with torch.cuda.amp.autocast():
2025-11-05 11:53:26 Train Epoch 034:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.7167, Top1=57.42%, LR=0.075534]2025-11-05 11:53:26,819 - INFO - Step 85068: {'train_loss_batch': 2.716702461242676, 'train_lr': 0.07553400650307944, 'batch_time': 5.206009864807129, 'data_time': 3.5817782878875732}
2025-11-05 11:53:26 Train Epoch 034:   0%|          | 2/2502 [00:06<2:09:32,  3.11s/it, Loss=2.7167, Top1=57.42%, LR=0.075534]wandb: WARNING Tried to log to step 33 that is less than the current step 85066. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 11:53:30 Train Epoch 034:   4%|▍         | 100/2502 [02:49<1:06:18,  1.66s/it, Loss=3.3927, Top1=N/A, LR=0.075534]   2025-11-05 11:56:11,457 - INFO - Step 85168: {'train_loss_batch': 4.604666233062744, 'train_lr': 0.07553400650307944, 'batch_time': 1.6816306940399774, 'data_time': 0.036464492873390125}
2025-11-05 11:56:11 Train Epoch 034:   8%|▊         | 200/2502 [05:34<1:03:12,  1.65s/it, Loss=3.4225, Top1=N/A, LR=0.075534]2025-11-05 11:58:56,607 - INFO - Step 85268: {'train_loss_batch': 4.045141220092773, 'train_lr': 0.07553400650307944, 'batch_time': 1.666642509289642, 'data_time': 0.018821087642688656}
2025-11-05 11:58:56 Train Epoch 034:  12%|█▏        | 300/2502 [08:19<1:00:26,  1.65s/it, Loss=3.4283, Top1=N/A, LR=0.075534]2025-11-05 12:01:41,162 - INFO - Step 85368: {'train_loss_batch': 4.251509666442871, 'train_lr': 0.07553400650307944, 'batch_time': 1.659633672118583, 'data_time': 0.012893003482755237}
2025-11-05 12:01:41 Train Epoch 034:  16%|█▌        | 400/2502 [11:04<58:05,  1.66s/it, Loss=3.3866, Top1=N/A, LR=0.075534]2025-11-05 12:04:26,440 - INFO - Step 85468: {'train_loss_batch': 4.650328159332275, 'train_lr': 0.07553400650307944, 'batch_time': 1.6579219551752333, 'data_time': 0.009917720000345511}
2025-11-05 12:04:26 Train Epoch 034:  20%|█▉        | 500/2502 [13:50<55:20,  1.66s/it, Loss=3.3856, Top1=N/A, LR=0.075534]2025-11-05 12:07:12,293 - INFO - Step 85568: {'train_loss_batch': 2.57808518409729, 'train_lr': 0.07553400650307944, 'batch_time': 1.6580447570054593, 'data_time': 0.00813951463756447}
2025-11-05 12:07:12 Train Epoch 034:  24%|██▍       | 600/2502 [16:35<52:17,  1.65s/it, Loss=3.3896, Top1=N/A, LR=0.075534]2025-11-05 12:09:57,495 - INFO - Step 85668: {'train_loss_batch': 4.430503845214844, 'train_lr': 0.07553400650307944, 'batch_time': 1.6570409736696774, 'data_time': 0.006944820210461608}
2025-11-05 12:09:57 Train Epoch 034:  28%|██▊       | 700/2502 [19:20<49:32,  1.65s/it, Loss=3.3786, Top1=N/A, LR=0.075534]2025-11-05 12:12:42,174 - INFO - Step 85768: {'train_loss_batch': 3.8321499824523926, 'train_lr': 0.07553400650307944, 'batch_time': 1.655578271808706, 'data_time': 0.006085801566718479}
2025-11-05 12:12:42 Train Epoch 034:  32%|███▏      | 800/2502 [22:05<46:30,  1.64s/it, Loss=3.3852, Top1=57.38%, LR=0.075534]2025-11-05 12:15:26,816 - INFO - Step 85868: {'train_loss_batch': 2.747375965118408, 'train_lr': 0.07553400650307944, 'batch_time': 1.6544354405444808, 'data_time': 0.005452368589822719}
2025-11-05 12:15:26 Train Epoch 034:  36%|███▌      | 900/2502 [24:49<43:49,  1.64s/it, Loss=3.3836, Top1=57.40%, LR=0.075534]2025-11-05 12:18:11,307 - INFO - Step 85968: {'train_loss_batch': 2.7788546085357666, 'train_lr': 0.07553400650307944, 'batch_time': 1.6533771566757218, 'data_time': 0.004951668632414179}
2025-11-05 12:18:11 Train Epoch 034:  40%|███▉      | 1000/2502 [27:34<41:14,  1.65s/it, Loss=3.3793, Top1=N/A, LR=0.075534]   2025-11-05 12:20:55,844 - INFO - Step 86068: {'train_loss_batch': 2.7152953147888184, 'train_lr': 0.07553400650307944, 'batch_time': 1.6525769152722278, 'data_time': 0.004550344341403835}
2025-11-05 12:20:55 Train Epoch 034:  44%|████▍     | 1100/2502 [30:19<38:47,  1.66s/it, Loss=3.3804, Top1=57.44%, LR=0.075534]2025-11-05 12:23:40,857 - INFO - Step 86168: {'train_loss_batch': 2.620365619659424, 'train_lr': 0.07553400650307944, 'batch_time': 1.6523546453609346, 'data_time': 0.0042320690189676866}
2025-11-05 12:23:40 Train Epoch 034:  48%|████▊     | 1200/2502 [33:04<35:40,  1.64s/it, Loss=3.3753, Top1=N/A, LR=0.075534]   2025-11-05 12:26:26,399 - INFO - Step 86268: {'train_loss_batch': 2.702261447906494, 'train_lr': 0.07553400650307944, 'batch_time': 1.6526100087622422, 'data_time': 0.003958982194492362}
2025-11-05 12:26:26 Train Epoch 034:  52%|█████▏    | 1300/2502 [35:49<33:02,  1.65s/it, Loss=3.3724, Top1=57.41%, LR=0.075534]2025-11-05 12:29:11,311 - INFO - Step 86368: {'train_loss_batch': 2.677018165588379, 'train_lr': 0.07553400650307944, 'batch_time': 1.6523413434567404, 'data_time': 0.0037245037553862}
2025-11-05 12:29:11 Train Epoch 034:  56%|█████▌    | 1400/2502 [38:35<30:29,  1.66s/it, Loss=3.3729, Top1=N/A, LR=0.075534]   2025-11-05 12:31:57,174 - INFO - Step 86468: {'train_loss_batch': 2.8901357650756836, 'train_lr': 0.07553400650307944, 'batch_time': 1.6527902513295731, 'data_time': 0.0035257135264623344}
2025-11-05 12:31:57 Train Epoch 034:  60%|█████▉    | 1500/2502 [41:21<27:42,  1.66s/it, Loss=3.3788, Top1=57.44%, LR=0.075534]2025-11-05 12:34:43,081 - INFO - Step 86568: {'train_loss_batch': 2.7188267707824707, 'train_lr': 0.07553400650307944, 'batch_time': 1.6532083478949215, 'data_time': 0.003354899332096067}
2025-11-05 12:34:43 Train Epoch 034:  64%|██████▍   | 1600/2502 [44:07<24:54,  1.66s/it, Loss=3.3888, Top1=N/A, LR=0.075534]   2025-11-05 12:37:28,651 - INFO - Step 86668: {'train_loss_batch': 3.6218652725219727, 'train_lr': 0.07553400650307944, 'batch_time': 1.6533640278346236, 'data_time': 0.0032064217764611993}
2025-11-05 12:37:28 Train Epoch 034:  68%|██████▊   | 1700/2502 [46:52<21:59,  1.64s/it, Loss=3.3917, Top1=N/A, LR=0.075534]2025-11-05 12:40:13,732 - INFO - Step 86768: {'train_loss_batch': 2.791823387145996, 'train_lr': 0.07553400650307944, 'batch_time': 1.6532139182441168, 'data_time': 0.0030744807430045313}
2025-11-05 12:40:13 Train Epoch 034:  72%|███████▏  | 1800/2502 [49:36<19:06,  1.63s/it, Loss=3.3988, Top1=N/A, LR=0.075534]2025-11-05 12:42:58,185 - INFO - Step 86868: {'train_loss_batch': 4.487071990966797, 'train_lr': 0.07553400650307944, 'batch_time': 1.6527311768815096, 'data_time': 0.002952161992277985}
2025-11-05 12:42:58 Train Epoch 034:  76%|███████▌  | 1900/2502 [52:20<16:27,  1.64s/it, Loss=3.4008, Top1=57.46%, LR=0.075534]2025-11-05 12:45:41,637 - INFO - Step 86968: {'train_loss_batch': 2.70112943649292, 'train_lr': 0.07553400650307944, 'batch_time': 1.6517732779268588, 'data_time': 0.002849082455140926}
2025-11-05 12:45:41 Train Epoch 034:  80%|███████▉  | 2000/2502 [55:04<13:44,  1.64s/it, Loss=3.4022, Top1=57.41%, LR=0.075534]2025-11-05 12:48:26,388 - INFO - Step 87068: {'train_loss_batch': 2.851457118988037, 'train_lr': 0.07553400650307944, 'batch_time': 1.6515597782392373, 'data_time': 0.0027581490855524385}
2025-11-05 12:48:26 Train Epoch 034:  84%|████████▍ | 2100/2502 [57:49<11:05,  1.65s/it, Loss=3.4041, Top1=N/A, LR=0.075534]   2025-11-05 12:51:11,142 - INFO - Step 87168: {'train_loss_batch': 4.1441192626953125, 'train_lr': 0.07553400650307944, 'batch_time': 1.6513685604778146, 'data_time': 0.002674891913521579}
2025-11-05 12:51:11 Train Epoch 034:  88%|████████▊ | 2200/2502 [1:00:35<08:20,  1.66s/it, Loss=3.4106, Top1=N/A, LR=0.075534]2025-11-05 12:53:56,930 - INFO - Step 87268: {'train_loss_batch': 2.8275794982910156, 'train_lr': 0.07553400650307944, 'batch_time': 1.651664474453508, 'data_time': 0.0025967485738959653}
2025-11-05 12:53:56 Train Epoch 034:  92%|█████████▏| 2300/2502 [1:03:20<05:35,  1.66s/it, Loss=3.4156, Top1=N/A, LR=0.075534]2025-11-05 12:56:42,160 - INFO - Step 87368: {'train_loss_batch': 4.397212505340576, 'train_lr': 0.07553400650307944, 'batch_time': 1.651692072757686, 'data_time': 0.002526276840017651}
2025-11-05 12:56:42 Train Epoch 034:  96%|█████████▌| 2400/2502 [1:06:06<02:47,  1.65s/it, Loss=3.4187, Top1=N/A, LR=0.075534]2025-11-05 12:59:27,822 - INFO - Step 87468: {'train_loss_batch': 3.6262197494506836, 'train_lr': 0.07553400650307944, 'batch_time': 1.6518970898219518, 'data_time': 0.002462819634453052}
2025-11-05 12:59:27 Train Epoch 034: 100%|█████████▉| 2500/2502 [1:08:51<00:03,  1.66s/it, Loss=3.4190, Top1=N/A, LR=0.075534]2025-11-05 13:02:13,540 - INFO - Step 87568: {'train_loss_batch': 2.794875383377075, 'train_lr': 0.07553400650307944, 'batch_time': 1.652108152214883, 'data_time': 0.0024261683380541826}
2025-11-05 13:02:13 Train Epoch 034: 100%|██████████| 2502/2502 [1:08:53<00:00,  1.65s/it, Loss=3.4190, Top1=N/A, LR=0.075534]
2025-11-05 13:02:15 Val Epoch 034:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 13:02:20   with torch.cuda.amp.autocast():
2025-11-05 13:02:20 Val Epoch 034: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.4861, Top1=61.57%, Top5=84.62%]
2025-11-05 13:04:07 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 13:04:07   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 13:04:07 2025-11-05 13:04:07,631 - INFO - Step 34: {'epoch': 34, 'learning_rate': 0.07392818589721882, 'train_loss': 3.418749912465505, 'train_top1': 57.34469506048387, 'train_top5': 80.00724546370968, 'train_precision': 57.03326957736128, 'train_recall': 57.16949086414598, 'train_f1': 56.88339388157617, 'val_loss': 2.4861229232788085, 'val_top1': 61.56999999023437, 'val_top5': 84.61799997802734, 'val_precision': 64.83585280891565, 'val_recall': 61.576, 'val_f1': 61.073454377178834}
2025-11-05 13:04:07 2025-11-05 13:04:07,633 - INFO - Epoch 034 Summary - LR: 0.073928, Train Loss: 3.4187, Val Loss: 2.4861, Val F1: 61.07%, Val Precision: 64.84%, Val Recall: 61.58%
2025-11-05 13:04:08 wandb: WARNING Tried to log to step 34 that is less than the current step 87568. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 13:04:11 2025-11-05 13:04:11,190 - INFO - New best model saved with validation accuracy: 61.570%
2025-11-05 13:04:11 2025-11-05 13:04:11,190 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_035.pth
2025-11-05 13:04:11 Train Epoch 035:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 13:04:15   with torch.cuda.amp.autocast():
2025-11-05 13:04:17 Train Epoch 035:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.6520, Top1=N/A, LR=0.073928]2025-11-05 13:04:17,026 - INFO - Step 87570: {'train_loss_batch': 4.652019500732422, 'train_lr': 0.07392818589721882, 'batch_time': 5.833799123764038, 'data_time': 4.192736625671387}
2025-11-05 13:04:17 Train Epoch 035:   4%|▍         | 100/2502 [02:50<1:05:40,  1.64s/it, Loss=3.4104, Top1=N/A, LR=0.073928]2025-11-05 13:07:01,470 - INFO - Step 87670: {'train_loss_batch': 4.968417167663574, 'train_lr': 0.07392818589721882, 'batch_time': 1.68592367549934, 'data_time': 0.042580623437862584}
2025-11-05 13:07:01 Train Epoch 035:   8%|▊         | 200/2502 [05:34<1:03:06,  1.65s/it, Loss=3.4046, Top1=N/A, LR=0.073928]2025-11-05 13:09:46,112 - INFO - Step 87770: {'train_loss_batch': 2.8732030391693115, 'train_lr': 0.07392818589721882, 'batch_time': 1.6662683475076856, 'data_time': 0.021930974514330206}
2025-11-05 13:09:46 Train Epoch 035:  12%|█▏        | 300/2502 [08:19<1:00:26,  1.65s/it, Loss=3.4346, Top1=N/A, LR=0.073928]2025-11-05 13:12:30,406 - INFO - Step 87870: {'train_loss_batch': 2.982926607131958, 'train_lr': 0.07392818589721882, 'batch_time': 1.6585157559163546, 'data_time': 0.014957520652847037}
2025-11-05 13:12:30 Train Epoch 035:  16%|█▌        | 400/2502 [11:03<57:19,  1.64s/it, Loss=3.4423, Top1=N/A, LR=0.073928]2025-11-05 13:15:15,113 - INFO - Step 87970: {'train_loss_batch': 2.628000020980835, 'train_lr': 0.07392818589721882, 'batch_time': 1.6556632738755528, 'data_time': 0.011462630179160254}
2025-11-05 13:15:15 Train Epoch 035:  20%|█▉        | 500/2502 [13:48<55:05,  1.65s/it, Loss=3.4093, Top1=N/A, LR=0.073928]2025-11-05 13:17:59,459 - INFO - Step 88070: {'train_loss_batch': 2.7423722743988037, 'train_lr': 0.07392818589721882, 'batch_time': 1.6532254951918672, 'data_time': 0.009370991331850459}
2025-11-05 13:17:59 Train Epoch 035:  24%|██▍       | 600/2502 [16:33<52:06,  1.64s/it, Loss=3.3840, Top1=N/A, LR=0.073928]2025-11-05 13:20:44,695 - INFO - Step 88170: {'train_loss_batch': 2.863130569458008, 'train_lr': 0.07392818589721882, 'batch_time': 1.6530816610561632, 'data_time': 0.007975458900464355}
2025-11-05 13:20:44 Train Epoch 035:  28%|██▊       | 700/2502 [19:18<49:43,  1.66s/it, Loss=3.4043, Top1=N/A, LR=0.073928]2025-11-05 13:23:29,686 - INFO - Step 88270: {'train_loss_batch': 3.9673216342926025, 'train_lr': 0.07392818589721882, 'batch_time': 1.65262965400957, 'data_time': 0.006977002392821918}
2025-11-05 13:23:29 Train Epoch 035:  32%|███▏      | 800/2502 [22:04<47:08,  1.66s/it, Loss=3.4094, Top1=N/A, LR=0.073928]2025-11-05 13:26:15,335 - INFO - Step 88370: {'train_loss_batch': 2.857355833053589, 'train_lr': 0.07392818589721882, 'batch_time': 1.653111431631405, 'data_time': 0.006240657503982906}
2025-11-05 13:26:15 Train Epoch 035:  36%|███▌      | 900/2502 [24:49<44:17,  1.66s/it, Loss=3.4086, Top1=57.89%, LR=0.073928]2025-11-05 13:29:00,578 - INFO - Step 88470: {'train_loss_batch': 2.6840124130249023, 'train_lr': 0.07392818589721882, 'batch_time': 1.6530351236578362, 'data_time': 0.005661129554553778}
2025-11-05 13:29:00 Train Epoch 035:  40%|███▉      | 1000/2502 [27:35<41:21,  1.65s/it, Loss=3.4238, Top1=57.87%, LR=0.073928]2025-11-05 13:31:46,204 - INFO - Step 88570: {'train_loss_batch': 2.5643835067749023, 'train_lr': 0.07392818589721882, 'batch_time': 1.6533573042977225, 'data_time': 0.005220490616637391}
2025-11-05 13:31:46 Train Epoch 035:  44%|████▍     | 1100/2502 [30:20<38:52,  1.66s/it, Loss=3.4254, Top1=N/A, LR=0.073928]   2025-11-05 13:34:31,992 - INFO - Step 88670: {'train_loss_batch': 2.7685017585754395, 'train_lr': 0.07392818589721882, 'batch_time': 1.653767545260049, 'data_time': 0.004836720839941318}
2025-11-05 13:34:31 Train Epoch 035:  48%|████▊     | 1200/2502 [33:06<35:58,  1.66s/it, Loss=3.4294, Top1=N/A, LR=0.073928]2025-11-05 13:37:17,470 - INFO - Step 88770: {'train_loss_batch': 4.066324710845947, 'train_lr': 0.07392818589721882, 'batch_time': 1.653851866225815, 'data_time': 0.004513895978141486}
2025-11-05 13:37:17 Train Epoch 035:  52%|█████▏    | 1300/2502 [35:50<33:09,  1.66s/it, Loss=3.4246, Top1=N/A, LR=0.073928]2025-11-05 13:40:01,882 - INFO - Step 88870: {'train_loss_batch': 4.396121978759766, 'train_lr': 0.07392818589721882, 'batch_time': 1.653103734602111, 'data_time': 0.0042447479388788975}
2025-11-05 13:40:01 Train Epoch 035:  56%|█████▌    | 1400/2502 [38:36<30:25,  1.66s/it, Loss=3.4230, Top1=N/A, LR=0.073928]2025-11-05 13:42:47,985 - INFO - Step 88970: {'train_loss_batch': 2.833770751953125, 'train_lr': 0.07392818589721882, 'batch_time': 1.6536690931844338, 'data_time': 0.004259555872468588}
2025-11-05 13:42:47 Train Epoch 035:  60%|█████▉    | 1500/2502 [41:21<27:19,  1.64s/it, Loss=3.4223, Top1=N/A, LR=0.073928]2025-11-05 13:45:32,886 - INFO - Step 89070: {'train_loss_batch': 2.8162105083465576, 'train_lr': 0.07392818589721882, 'batch_time': 1.6533587709575555, 'data_time': 0.004045007071599891}
2025-11-05 13:45:32 Train Epoch 035:  64%|██████▍   | 1600/2502 [44:06<24:49,  1.65s/it, Loss=3.4219, Top1=57.76%, LR=0.073928]2025-11-05 13:48:17,734 - INFO - Step 89170: {'train_loss_batch': 2.7371110916137695, 'train_lr': 0.07392818589721882, 'batch_time': 1.6530537402756433, 'data_time': 0.0038759145492467933}
2025-11-05 13:48:17 Train Epoch 035:  68%|██████▊   | 1700/2502 [46:51<22:05,  1.65s/it, Loss=3.4215, Top1=57.75%, LR=0.073928]2025-11-05 13:51:02,264 - INFO - Step 89270: {'train_loss_batch': 2.5918240547180176, 'train_lr': 0.07392818589721882, 'batch_time': 1.6525979196233374, 'data_time': 0.003704376461784816}
2025-11-05 13:51:02 Train Epoch 035:  72%|███████▏  | 1800/2502 [49:36<19:13,  1.64s/it, Loss=3.4235, Top1=N/A, LR=0.073928]   2025-11-05 13:53:47,839 - INFO - Step 89370: {'train_loss_batch': 3.4119873046875, 'train_lr': 0.07392818589721882, 'batch_time': 1.6527726263684872, 'data_time': 0.0035832819178791457}
2025-11-05 13:53:47 Train Epoch 035:  76%|███████▌  | 1900/2502 [52:21<16:40,  1.66s/it, Loss=3.4188, Top1=57.69%, LR=0.073928]2025-11-05 13:56:32,446 - INFO - Step 89470: {'train_loss_batch': 2.7033956050872803, 'train_lr': 0.07392818589721882, 'batch_time': 1.6524201929412472, 'data_time': 0.003447442729494435}
2025-11-05 13:56:32 Train Epoch 035:  80%|███████▉  | 2000/2502 [55:06<13:52,  1.66s/it, Loss=3.4164, Top1=57.71%, LR=0.073928]2025-11-05 13:59:18,098 - INFO - Step 89570: {'train_loss_batch': 2.812150001525879, 'train_lr': 0.07392818589721882, 'batch_time': 1.652624583613688, 'data_time': 0.00333964794889085}
2025-11-05 13:59:18 Train Epoch 035:  84%|████████▍ | 2100/2502 [57:52<11:04,  1.65s/it, Loss=3.4208, Top1=N/A, LR=0.073928]   2025-11-05 14:02:03,301 - INFO - Step 89670: {'train_loss_batch': 2.6103930473327637, 'train_lr': 0.07392818589721882, 'batch_time': 1.652596562093692, 'data_time': 0.0033120019614044}
2025-11-05 14:02:03 Train Epoch 035:  88%|████████▊ | 2200/2502 [1:00:37<08:21,  1.66s/it, Loss=3.4291, Top1=N/A, LR=0.073928]2025-11-05 14:04:49,039 - INFO - Step 89770: {'train_loss_batch': 4.126523017883301, 'train_lr': 0.07392818589721882, 'batch_time': 1.6528133795728255, 'data_time': 0.0033942921927494116}
2025-11-05 14:04:49 Train Epoch 035:  92%|█████████▏| 2300/2502 [1:03:24<05:31,  1.64s/it, Loss=3.4210, Top1=N/A, LR=0.073928]2025-11-05 14:07:35,470 - INFO - Step 89870: {'train_loss_batch': 2.939673900604248, 'train_lr': 0.07392818589721882, 'batch_time': 1.6533131282363753, 'data_time': 0.003899963458482725}
2025-11-05 14:07:35 Train Epoch 035:  96%|█████████▌| 2400/2502 [1:06:10<02:49,  1.66s/it, Loss=3.4217, Top1=N/A, LR=0.073928]2025-11-05 14:10:21,432 - INFO - Step 89970: {'train_loss_batch': 4.625975608825684, 'train_lr': 0.07392818589721882, 'batch_time': 1.6535755022025913, 'data_time': 0.0040324905026907325}
2025-11-05 14:10:21 Train Epoch 035: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.66s/it, Loss=3.4189, Top1=N/A, LR=0.073928]2025-11-05 14:13:07,488 - INFO - Step 90070: {'train_loss_batch': 2.8002936840057373, 'train_lr': 0.07392818589721882, 'batch_time': 1.6538550363736646, 'data_time': 0.003932011122705459}
2025-11-05 14:13:07 Train Epoch 035: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=3.4189, Top1=N/A, LR=0.073928]
2025-11-05 14:13:09 Val Epoch 035:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 14:13:14   with torch.cuda.amp.autocast():
2025-11-05 14:13:14 Val Epoch 035: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.4726, Top1=61.85%, Top5=85.13%]
2025-11-05 14:15:01 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 14:15:01   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 14:15:01 2025-11-05 14:15:01,264 - INFO - Step 35: {'epoch': 35, 'learning_rate': 0.07228968909704804, 'train_loss': 3.4186795150442757, 'train_top1': 57.644793507067135, 'train_top5': 80.26446554770318, 'train_precision': 57.35960752282967, 'train_recall': 57.49938182707331, 'train_f1': 57.21033810435476, 'val_loss': 2.4726169955444335, 'val_top1': 61.84600001220703, 'val_top5': 85.12800001220702, 'val_precision': 65.01052581763722, 'val_recall': 61.85000000000001, 'val_f1': 61.32082265882004}
2025-11-05 14:15:01 2025-11-05 14:15:01,265 - INFO - Epoch 035 Summary - LR: 0.072290, Train Loss: 3.4187, Val Loss: 2.4726, Val F1: 61.32%, Val Precision: 65.01%, Val Recall: 61.85%
2025-11-05 14:15:04 2025-11-05 14:15:04,278 - INFO - New best model saved with validation accuracy: 61.846%
2025-11-05 14:15:04 2025-11-05 14:15:04,278 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_036.pth
2025-11-05 14:15:04 Train Epoch 036:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 14:15:07   with torch.cuda.amp.autocast():
2025-11-05 14:15:08 wandb: WARNING Tried to log to step 35 that is less than the current step 90070. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 14:15:09 Train Epoch 036:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.5757, Top1=N/A, LR=0.072290]2025-11-05 14:15:09,573 - INFO - Step 90072: {'train_loss_batch': 3.5756592750549316, 'train_lr': 0.07228968909704804, 'batch_time': 5.293351411819458, 'data_time': 3.6380717754364014}
2025-11-05 14:15:09 Train Epoch 036:   4%|▍         | 100/2502 [02:50<1:06:15,  1.65s/it, Loss=3.3381, Top1=N/A, LR=0.072290]2025-11-05 14:17:55,172 - INFO - Step 90172: {'train_loss_batch': 4.622241973876953, 'train_lr': 0.07228968909704804, 'batch_time': 1.6920125696918753, 'data_time': 0.03703829321530786}
2025-11-05 14:17:55 Train Epoch 036:   8%|▊         | 200/2502 [05:36<1:03:03,  1.64s/it, Loss=3.3802, Top1=N/A, LR=0.072290]2025-11-05 14:20:40,539 - INFO - Step 90272: {'train_loss_batch': 5.064872741699219, 'train_lr': 0.07228968909704804, 'batch_time': 1.6729324457064196, 'data_time': 0.019102823674975344}
2025-11-05 14:20:40 Train Epoch 036:  12%|█▏        | 300/2502 [08:20<1:00:35,  1.65s/it, Loss=3.3571, Top1=58.32%, LR=0.072290]2025-11-05 14:23:25,277 - INFO - Step 90372: {'train_loss_batch': 2.533137798309326, 'train_lr': 0.07228968909704804, 'batch_time': 1.6644421946566763, 'data_time': 0.013084868656044387}
2025-11-05 14:23:25 Train Epoch 036:  16%|█▌        | 400/2502 [11:05<57:11,  1.63s/it, Loss=3.3848, Top1=N/A, LR=0.072290]   2025-11-05 14:26:10,069 - INFO - Step 90472: {'train_loss_batch': 3.1746859550476074, 'train_lr': 0.07228968909704804, 'batch_time': 1.6603218783761498, 'data_time': 0.01006284972973298}
2025-11-05 14:26:10 Train Epoch 036:  20%|█▉        | 500/2502 [13:50<55:26,  1.66s/it, Loss=3.3908, Top1=N/A, LR=0.072290]2025-11-05 14:28:54,916 - INFO - Step 90572: {'train_loss_batch': 4.395105361938477, 'train_lr': 0.07228968909704804, 'batch_time': 1.6579571536439146, 'data_time': 0.008250117539883611}
2025-11-05 14:28:54 Train Epoch 036:  24%|██▍       | 600/2502 [16:36<52:38,  1.66s/it, Loss=3.3811, Top1=N/A, LR=0.072290]2025-11-05 14:31:40,802 - INFO - Step 90672: {'train_loss_batch': 3.1296210289001465, 'train_lr': 0.07228968909704804, 'batch_time': 1.6581064678864947, 'data_time': 0.007054140087768758}
2025-11-05 14:31:40 Train Epoch 036:  28%|██▊       | 700/2502 [19:22<49:48,  1.66s/it, Loss=3.3850, Top1=57.92%, LR=0.072290]2025-11-05 14:34:26,418 - INFO - Step 90772: {'train_loss_batch': 2.7046804428100586, 'train_lr': 0.07228968909704804, 'batch_time': 1.657827388202923, 'data_time': 0.006193514047097548}
2025-11-05 14:34:26 Train Epoch 036:  32%|███▏      | 800/2502 [22:07<47:10,  1.66s/it, Loss=3.3800, Top1=57.92%, LR=0.072290]2025-11-05 14:37:12,253 - INFO - Step 90872: {'train_loss_batch': 2.762066125869751, 'train_lr': 0.07228968909704804, 'batch_time': 1.6578939133666726, 'data_time': 0.005550932497269801}
2025-11-05 14:37:12 Train Epoch 036:  36%|███▌      | 900/2502 [24:53<44:26,  1.66s/it, Loss=3.3758, Top1=N/A, LR=0.072290]   2025-11-05 14:39:57,997 - INFO - Step 90972: {'train_loss_batch': 3.611783027648926, 'train_lr': 0.07228968909704804, 'batch_time': 1.6578428155177176, 'data_time': 0.005052471531350393}
2025-11-05 14:39:57 Train Epoch 036:  40%|███▉      | 1000/2502 [27:38<41:32,  1.66s/it, Loss=3.3804, Top1=N/A, LR=0.072290]2025-11-05 14:42:42,438 - INFO - Step 91072: {'train_loss_batch': 2.653055191040039, 'train_lr': 0.07228968909704804, 'batch_time': 1.656500411438537, 'data_time': 0.004646049989210618}
2025-11-05 14:42:42 Train Epoch 036:  44%|████▍     | 1100/2502 [30:23<38:37,  1.65s/it, Loss=3.3817, Top1=N/A, LR=0.072290]2025-11-05 14:45:28,249 - INFO - Step 91172: {'train_loss_batch': 2.6482841968536377, 'train_lr': 0.07228968909704804, 'batch_time': 1.6566468792325468, 'data_time': 0.004319698998973545}
2025-11-05 14:45:28 Train Epoch 036:  48%|████▊     | 1200/2502 [33:09<36:08,  1.67s/it, Loss=3.3942, Top1=58.07%, LR=0.072290]2025-11-05 14:48:13,796 - INFO - Step 91272: {'train_loss_batch': 2.7516162395477295, 'train_lr': 0.07228968909704804, 'batch_time': 1.6565478538891159, 'data_time': 0.004042197028167242}
2025-11-05 14:48:13 Train Epoch 036:  52%|█████▏    | 1300/2502 [35:55<33:14,  1.66s/it, Loss=3.3968, Top1=N/A, LR=0.072290]   2025-11-05 14:50:59,814 - INFO - Step 91372: {'train_loss_batch': 4.1566243171691895, 'train_lr': 0.07228968909704804, 'batch_time': 1.656827338011241, 'data_time': 0.0038171457749527292}
2025-11-05 14:50:59 Train Epoch 036:  56%|█████▌    | 1400/2502 [38:41<30:31,  1.66s/it, Loss=3.4056, Top1=N/A, LR=0.072290]2025-11-05 14:53:45,758 - INFO - Step 91472: {'train_loss_batch': 4.788805961608887, 'train_lr': 0.07228968909704804, 'batch_time': 1.6570137079743978, 'data_time': 0.0036201766352411853}
2025-11-05 14:53:45 Train Epoch 036:  60%|█████▉    | 1500/2502 [41:26<27:33,  1.65s/it, Loss=3.4012, Top1=N/A, LR=0.072290]2025-11-05 14:56:31,084 - INFO - Step 91572: {'train_loss_batch': 2.7927284240722656, 'train_lr': 0.07228968909704804, 'batch_time': 1.6567634105047013, 'data_time': 0.0034441581017013233}
2025-11-05 14:56:31 Train Epoch 036:  64%|██████▍   | 1600/2502 [44:12<24:43,  1.65s/it, Loss=3.4039, Top1=N/A, LR=0.072290]2025-11-05 14:59:16,770 - INFO - Step 91672: {'train_loss_batch': 4.415339469909668, 'train_lr': 0.07228968909704804, 'batch_time': 1.6567695280822048, 'data_time': 0.0032923546230547284}
2025-11-05 14:59:16 Train Epoch 036:  68%|██████▊   | 1700/2502 [46:57<22:00,  1.65s/it, Loss=3.4106, Top1=N/A, LR=0.072290]2025-11-05 15:02:01,806 - INFO - Step 91772: {'train_loss_batch': 2.6459219455718994, 'train_lr': 0.07228968909704804, 'batch_time': 1.6563927092880169, 'data_time': 0.003158111000397428}
2025-11-05 15:02:01 Train Epoch 036:  72%|███████▏  | 1800/2502 [49:43<19:24,  1.66s/it, Loss=3.4190, Top1=N/A, LR=0.072290]2025-11-05 15:04:47,527 - INFO - Step 91872: {'train_loss_batch': 2.8469934463500977, 'train_lr': 0.07228968909704804, 'batch_time': 1.6564378106944897, 'data_time': 0.003039035580013938}
2025-11-05 15:04:47 Train Epoch 036:  76%|███████▌  | 1900/2502 [52:29<16:39,  1.66s/it, Loss=3.4133, Top1=N/A, LR=0.072290]2025-11-05 15:07:33,510 - INFO - Step 91972: {'train_loss_batch': 2.686023235321045, 'train_lr': 0.07228968909704804, 'batch_time': 1.6566157915415356, 'data_time': 0.0029351699985622546}
2025-11-05 15:07:33 Train Epoch 036:  80%|███████▉  | 2000/2502 [55:13<13:45,  1.65s/it, Loss=3.4116, Top1=N/A, LR=0.072290]2025-11-05 15:10:17,699 - INFO - Step 92072: {'train_loss_batch': 2.857712507247925, 'train_lr': 0.07228968909704804, 'batch_time': 1.6558799934291888, 'data_time': 0.0028380052498851284}
2025-11-05 15:10:17 Train Epoch 036:  84%|████████▍ | 2100/2502 [57:59<11:06,  1.66s/it, Loss=3.4176, Top1=N/A, LR=0.072290]2025-11-05 15:13:03,623 - INFO - Step 92172: {'train_loss_batch': 2.948287010192871, 'train_lr': 0.07228968909704804, 'batch_time': 1.6560395549218578, 'data_time': 0.002754058910516941}
2025-11-05 15:13:03 Train Epoch 036:  88%|████████▊ | 2200/2502 [1:00:45<08:20,  1.66s/it, Loss=3.4172, Top1=N/A, LR=0.072290]2025-11-05 15:15:49,371 - INFO - Step 92272: {'train_loss_batch': 3.6065244674682617, 'train_lr': 0.07228968909704804, 'batch_time': 1.6561052225113781, 'data_time': 0.00267516867998566}
2025-11-05 15:15:49 Train Epoch 036:  92%|█████████▏| 2300/2502 [1:03:30<05:34,  1.66s/it, Loss=3.4207, Top1=N/A, LR=0.072290]2025-11-05 15:18:35,275 - INFO - Step 92372: {'train_loss_batch': 3.5313377380371094, 'train_lr': 0.07228968909704804, 'batch_time': 1.6562324819850798, 'data_time': 0.002606601727521715}
2025-11-05 15:18:35 Train Epoch 036:  96%|█████████▌| 2400/2502 [1:06:16<02:49,  1.66s/it, Loss=3.4197, Top1=N/A, LR=0.072290]2025-11-05 15:21:20,838 - INFO - Step 92472: {'train_loss_batch': 2.865328073501587, 'train_lr': 0.07228968909704804, 'batch_time': 1.6562074828475577, 'data_time': 0.0025440686347831146}
2025-11-05 15:21:20 Train Epoch 036: 100%|█████████▉| 2500/2502 [1:09:02<00:03,  1.66s/it, Loss=3.4194, Top1=N/A, LR=0.072290]2025-11-05 15:24:06,674 - INFO - Step 92572: {'train_loss_batch': 2.7090158462524414, 'train_lr': 0.07228968909704804, 'batch_time': 1.6562934966623093, 'data_time': 0.0025386295524514805}
2025-11-05 15:24:06 Train Epoch 036: 100%|██████████| 2502/2502 [1:09:04<00:00,  1.66s/it, Loss=3.4194, Top1=N/A, LR=0.072290]
2025-11-05 15:24:08 Val Epoch 036:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 15:24:13   with torch.cuda.amp.autocast():
2025-11-05 15:24:13 Val Epoch 036: 100%|██████████| 98/98 [01:52<00:00,  1.15s/it, Loss=2.4506, Top1=62.14%, Top5=85.30%]
2025-11-05 15:26:01 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 15:26:01   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 15:26:01 2025-11-05 15:26:01,540 - INFO - Step 36: {'epoch': 36, 'learning_rate': 0.07062075409210782, 'train_loss': 3.4195047144314272, 'train_top1': 57.92153159340659, 'train_top5': 80.46402815934066, 'train_precision': 57.568842949804086, 'train_recall': 57.736685182622836, 'train_f1': 57.433788080650494, 'val_loss': 2.4505623139190673, 'val_top1': 62.13799997070313, 'val_top5': 85.30400000244141, 'val_precision': 65.42236832287061, 'val_recall': 62.138000000000005, 'val_f1': 61.69487398314206}
2025-11-05 15:26:01 2025-11-05 15:26:01,542 - INFO - Epoch 036 Summary - LR: 0.070621, Train Loss: 3.4195, Val Loss: 2.4506, Val F1: 61.69%, Val Precision: 65.42%, Val Recall: 62.14%
2025-11-05 15:26:05 2025-11-05 15:26:05,341 - INFO - New best model saved with validation accuracy: 62.138%
2025-11-05 15:26:05 2025-11-05 15:26:05,342 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_037.pth
2025-11-05 15:26:05 Train Epoch 037:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 15:26:08   with torch.cuda.amp.autocast():
2025-11-05 15:26:09 wandb: WARNING Tried to log to step 36 that is less than the current step 92572. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 15:26:10 Train Epoch 037:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.3803, Top1=N/A, LR=0.070621]2025-11-05 15:26:10,560 - INFO - Step 92574: {'train_loss_batch': 4.380274772644043, 'train_lr': 0.07062075409210782, 'batch_time': 5.2161431312561035, 'data_time': 3.572699546813965}
2025-11-05 15:26:10 Train Epoch 037:   4%|▍         | 100/2502 [02:50<1:06:30,  1.66s/it, Loss=3.3438, Top1=N/A, LR=0.070621]2025-11-05 15:28:56,144 - INFO - Step 92674: {'train_loss_batch': 3.344491720199585, 'train_lr': 0.07062075409210782, 'batch_time': 1.6910971249684248, 'data_time': 0.03643492896958153}
2025-11-05 15:28:56 Train Epoch 037:   8%|▊         | 200/2502 [05:36<1:02:59,  1.64s/it, Loss=3.3962, Top1=N/A, LR=0.070621]2025-11-05 15:31:41,927 - INFO - Step 92774: {'train_loss_batch': 2.710239887237549, 'train_lr': 0.07062075409210782, 'batch_time': 1.6745424804402822, 'data_time': 0.018766105471558832}
2025-11-05 15:31:41 Train Epoch 037:  12%|█▏        | 300/2502 [08:21<1:00:11,  1.64s/it, Loss=3.4021, Top1=N/A, LR=0.070621]2025-11-05 15:34:26,757 - INFO - Step 92874: {'train_loss_batch': 2.6485090255737305, 'train_lr': 0.07062075409210782, 'batch_time': 1.6658250287521716, 'data_time': 0.012892893382481166}
2025-11-05 15:34:26 Train Epoch 037:  16%|█▌        | 400/2502 [11:07<58:17,  1.66s/it, Loss=3.3769, Top1=58.68%, LR=0.070621]2025-11-05 15:37:12,564 - INFO - Step 92974: {'train_loss_batch': 2.8009347915649414, 'train_lr': 0.07062075409210782, 'batch_time': 1.6638911477942717, 'data_time': 0.009939287071513416}
2025-11-05 15:37:12 Train Epoch 037:  20%|█▉        | 500/2502 [13:52<54:34,  1.64s/it, Loss=3.4024, Top1=N/A, LR=0.070621]   2025-11-05 15:39:57,638 - INFO - Step 93074: {'train_loss_batch': 4.641977310180664, 'train_lr': 0.07062075409210782, 'batch_time': 1.6612649254218308, 'data_time': 0.008160051471458938}
2025-11-05 15:39:57 Train Epoch 037:  24%|██▍       | 600/2502 [16:36<52:08,  1.65s/it, Loss=3.4111, Top1=N/A, LR=0.070621]2025-11-05 15:42:42,332 - INFO - Step 93174: {'train_loss_batch': 3.030210494995117, 'train_lr': 0.07062075409210782, 'batch_time': 1.6588810362950737, 'data_time': 0.006968612480480937}
2025-11-05 15:42:42 Train Epoch 037:  28%|██▊       | 700/2502 [19:22<49:47,  1.66s/it, Loss=3.4151, Top1=N/A, LR=0.070621]2025-11-05 15:45:27,847 - INFO - Step 93274: {'train_loss_batch': 2.858306646347046, 'train_lr': 0.07062075409210782, 'batch_time': 1.6583482960661535, 'data_time': 0.006119908007677544}
2025-11-05 15:45:27 Train Epoch 037:  32%|███▏      | 800/2502 [22:08<47:07,  1.66s/it, Loss=3.4062, Top1=N/A, LR=0.070621]2025-11-05 15:48:13,563 - INFO - Step 93374: {'train_loss_batch': 3.180800199508667, 'train_lr': 0.07062075409210782, 'batch_time': 1.6582006634249074, 'data_time': 0.005488185846850221}
2025-11-05 15:48:13 Train Epoch 037:  36%|███▌      | 900/2502 [24:53<43:45,  1.64s/it, Loss=3.4090, Top1=N/A, LR=0.070621]2025-11-05 15:50:58,357 - INFO - Step 93474: {'train_loss_batch': 2.955934524536133, 'train_lr': 0.07062075409210782, 'batch_time': 1.6570603207134116, 'data_time': 0.004988227911979853}
2025-11-05 15:50:58 Train Epoch 037:  40%|███▉      | 1000/2502 [27:38<41:13,  1.65s/it, Loss=3.4130, Top1=58.27%, LR=0.070621]2025-11-05 15:53:43,702 - INFO - Step 93574: {'train_loss_batch': 2.7901861667633057, 'train_lr': 0.07062075409210782, 'batch_time': 1.656700121891963, 'data_time': 0.004597188709499119}
2025-11-05 15:53:43 Train Epoch 037:  44%|████▍     | 1100/2502 [30:23<38:32,  1.65s/it, Loss=3.4098, Top1=N/A, LR=0.070621]   2025-11-05 15:56:28,736 - INFO - Step 93674: {'train_loss_batch': 3.5771570205688477, 'train_lr': 0.07062075409210782, 'batch_time': 1.6561216210149614, 'data_time': 0.0042763312874221455}
2025-11-05 15:56:28 Train Epoch 037:  48%|████▊     | 1200/2502 [33:09<35:52,  1.65s/it, Loss=3.4050, Top1=N/A, LR=0.070621]2025-11-05 15:59:14,463 - INFO - Step 93774: {'train_loss_batch': 2.6789755821228027, 'train_lr': 0.07062075409210782, 'batch_time': 1.656217217544632, 'data_time': 0.004010558029098574}
2025-11-05 15:59:14 Train Epoch 037:  52%|█████▏    | 1300/2502 [35:54<33:13,  1.66s/it, Loss=3.4103, Top1=N/A, LR=0.070621]2025-11-05 16:01:59,865 - INFO - Step 93874: {'train_loss_batch': 3.5345282554626465, 'train_lr': 0.07062075409210782, 'batch_time': 1.6560487631739882, 'data_time': 0.0037819668478822816}
2025-11-05 16:01:59 Train Epoch 037:  56%|█████▌    | 1400/2502 [38:39<30:26,  1.66s/it, Loss=3.4082, Top1=N/A, LR=0.070621]2025-11-05 16:04:45,295 - INFO - Step 93974: {'train_loss_batch': 4.130395889282227, 'train_lr': 0.07062075409210782, 'batch_time': 1.6559235020078649, 'data_time': 0.003590581078430655}
2025-11-05 16:04:45 Train Epoch 037:  60%|█████▉    | 1500/2502 [41:25<27:45,  1.66s/it, Loss=3.4093, Top1=N/A, LR=0.070621]2025-11-05 16:07:30,624 - INFO - Step 94074: {'train_loss_batch': 4.298043251037598, 'train_lr': 0.07062075409210782, 'batch_time': 1.6557479924475804, 'data_time': 0.0034205716582316704}
2025-11-05 16:07:30 Train Epoch 037:  64%|██████▍   | 1600/2502 [44:10<24:37,  1.64s/it, Loss=3.4022, Top1=58.25%, LR=0.070621]2025-11-05 16:10:16,128 - INFO - Step 94174: {'train_loss_batch': 2.598618507385254, 'train_lr': 0.07062075409210782, 'batch_time': 1.655703374253892, 'data_time': 0.003272770197223232}
2025-11-05 16:10:16 Train Epoch 037:  68%|██████▊   | 1700/2502 [46:55<22:08,  1.66s/it, Loss=3.4012, Top1=N/A, LR=0.070621]   2025-11-05 16:13:01,299 - INFO - Step 94274: {'train_loss_batch': 2.975001335144043, 'train_lr': 0.07062075409210782, 'batch_time': 1.655468330181465, 'data_time': 0.003145810087451789}
2025-11-05 16:13:01 Train Epoch 037:  72%|███████▏  | 1800/2502 [49:41<19:07,  1.64s/it, Loss=3.3989, Top1=N/A, LR=0.070621]2025-11-05 16:15:46,384 - INFO - Step 94374: {'train_loss_batch': 3.1428823471069336, 'train_lr': 0.07062075409210782, 'batch_time': 1.6552119415247195, 'data_time': 0.0030335083992728785}
2025-11-05 16:15:46 Train Epoch 037:  76%|███████▌  | 1900/2502 [52:25<16:37,  1.66s/it, Loss=3.3894, Top1=58.22%, LR=0.070621]2025-11-05 16:18:30,435 - INFO - Step 94474: {'train_loss_batch': 2.6994333267211914, 'train_lr': 0.07062075409210782, 'batch_time': 1.6544384430860732, 'data_time': 0.002929673202410301}
2025-11-05 16:18:30 Train Epoch 037:  80%|███████▉  | 2000/2502 [55:09<13:50,  1.65s/it, Loss=3.3918, Top1=N/A, LR=0.070621]   2025-11-05 16:21:14,941 - INFO - Step 94574: {'train_loss_batch': 5.153693199157715, 'train_lr': 0.07062075409210782, 'batch_time': 1.6539698277396717, 'data_time': 0.0028346529726622285}
2025-11-05 16:21:14 Train Epoch 037:  84%|████████▍ | 2100/2502 [57:53<10:56,  1.63s/it, Loss=3.3986, Top1=N/A, LR=0.070621]2025-11-05 16:23:59,287 - INFO - Step 94674: {'train_loss_batch': 3.226597309112549, 'train_lr': 0.07062075409210782, 'batch_time': 1.6534689858321745, 'data_time': 0.00274829192708527}
2025-11-05 16:23:59 Train Epoch 037:  88%|████████▊ | 2200/2502 [1:00:38<08:21,  1.66s/it, Loss=3.4055, Top1=N/A, LR=0.070621]2025-11-05 16:26:44,303 - INFO - Step 94774: {'train_loss_batch': 5.0488715171813965, 'train_lr': 0.07062075409210782, 'batch_time': 1.653318963988918, 'data_time': 0.002671725746719797}
2025-11-05 16:26:44 Train Epoch 037:  92%|█████████▏| 2300/2502 [1:03:24<05:35,  1.66s/it, Loss=3.4056, Top1=N/A, LR=0.070621]2025-11-05 16:29:30,089 - INFO - Step 94874: {'train_loss_batch': 3.545037269592285, 'train_lr': 0.07062075409210782, 'batch_time': 1.6535161047176608, 'data_time': 0.0026030108390917525}
2025-11-05 16:29:30 Train Epoch 037:  96%|█████████▌| 2400/2502 [1:06:09<02:48,  1.65s/it, Loss=3.4063, Top1=58.14%, LR=0.070621]2025-11-05 16:32:15,111 - INFO - Step 94974: {'train_loss_batch': 2.5960946083068848, 'train_lr': 0.07062075409210782, 'batch_time': 1.6533786372709454, 'data_time': 0.0025379102660039723}
2025-11-05 16:32:15 Train Epoch 037: 100%|█████████▉| 2500/2502 [1:08:55<00:03,  1.66s/it, Loss=3.4035, Top1=N/A, LR=0.070621]   2025-11-05 16:35:01,069 - INFO - Step 95074: {'train_loss_batch': 2.874676465988159, 'train_lr': 0.07062075409210782, 'batch_time': 1.6536266794208525, 'data_time': 0.0025263297848585175}
2025-11-05 16:35:01 Train Epoch 037: 100%|██████████| 2502/2502 [1:08:57<00:00,  1.65s/it, Loss=3.4035, Top1=N/A, LR=0.070621]
2025-11-05 16:35:03 Val Epoch 037:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 16:35:07   with torch.cuda.amp.autocast():
2025-11-05 16:35:08 Val Epoch 037: 100%|██████████| 98/98 [01:52<00:00,  1.15s/it, Loss=2.5072, Top1=61.03%, Top5=84.75%]
2025-11-05 16:36:56 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 16:36:56   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 16:36:56 2025-11-05 16:36:56,325 - INFO - Step 37: {'epoch': 37, 'learning_rate': 0.0689236604468653, 'train_loss': 3.4037118007620273, 'train_top1': 58.12944423652694, 'train_top5': 80.65002806886227, 'train_precision': 57.890559902629114, 'train_recall': 57.9706605215398, 'train_f1': 57.71399596238373, 'val_loss': 2.507221957550049, 'val_top1': 61.03400000488281, 'val_top5': 84.75200000732421, 'val_precision': 64.8296947373317, 'val_recall': 61.034, 'val_f1': 60.55337295923461}
2025-11-05 16:36:56 2025-11-05 16:36:56,327 - INFO - Epoch 037 Summary - LR: 0.068924, Train Loss: 3.4037, Val Loss: 2.5072, Val F1: 60.55%, Val Precision: 64.83%, Val Recall: 61.03%
2025-11-05 16:36:57 Train Epoch 038:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 37 that is less than the current step 95074. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 16:37:01 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 16:37:01   with torch.cuda.amp.autocast():
2025-11-05 16:37:02 Train Epoch 038:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.6700, Top1=58.01%, LR=0.068924]2025-11-05 16:37:02,853 - INFO - Step 95076: {'train_loss_batch': 2.6699914932250977, 'train_lr': 0.0689236604468653, 'batch_time': 5.786350250244141, 'data_time': 4.149993896484375}
2025-11-05 16:37:02 Train Epoch 038:   4%|▍         | 100/2502 [02:50<1:06:28,  1.66s/it, Loss=3.3330, Top1=N/A, LR=0.068924]   2025-11-05 16:39:47,544 - INFO - Step 95176: {'train_loss_batch': 3.7852210998535156, 'train_lr': 0.0689236604468653, 'batch_time': 1.6879116639052287, 'data_time': 0.04215953845788937}
2025-11-05 16:39:47 Train Epoch 038:   8%|▊         | 200/2502 [05:36<1:03:33,  1.66s/it, Loss=3.3317, Top1=N/A, LR=0.068924]2025-11-05 16:42:33,426 - INFO - Step 95276: {'train_loss_batch': 4.8833417892456055, 'train_lr': 0.0689236604468653, 'batch_time': 1.6734397838364785, 'data_time': 0.02167942749327095}
2025-11-05 16:42:33 Train Epoch 038:  12%|█▏        | 300/2502 [08:22<1:00:47,  1.66s/it, Loss=3.3425, Top1=N/A, LR=0.068924]2025-11-05 16:45:19,261 - INFO - Step 95376: {'train_loss_batch': 3.89918851852417, 'train_lr': 0.0689236604468653, 'batch_time': 1.6684253223710679, 'data_time': 0.01479801228672167}
2025-11-05 16:45:19 Train Epoch 038:  16%|█▌        | 400/2502 [11:07<57:42,  1.65s/it, Loss=3.3191, Top1=58.44%, LR=0.068924]2025-11-05 16:48:04,833 - INFO - Step 95476: {'train_loss_batch': 2.6804957389831543, 'train_lr': 0.0689236604468653, 'batch_time': 1.6652567255823987, 'data_time': 0.011369772385480694}
2025-11-05 16:48:04 Train Epoch 038:  20%|█▉        | 500/2502 [13:53<55:21,  1.66s/it, Loss=3.3394, Top1=N/A, LR=0.068924]   2025-11-05 16:50:50,084 - INFO - Step 95576: {'train_loss_batch': 2.652444839477539, 'train_lr': 0.0689236604468653, 'batch_time': 1.6627116260414352, 'data_time': 0.009299077911529237}
2025-11-05 16:50:50 Train Epoch 038:  24%|██▍       | 600/2502 [16:38<52:05,  1.64s/it, Loss=3.3578, Top1=N/A, LR=0.068924]2025-11-05 16:53:35,131 - INFO - Step 95676: {'train_loss_batch': 4.3722147941589355, 'train_lr': 0.0689236604468653, 'batch_time': 1.6606744371119038, 'data_time': 0.007918543506184355}
2025-11-05 16:53:35 Train Epoch 038:  28%|██▊       | 700/2502 [19:22<49:50,  1.66s/it, Loss=3.3775, Top1=58.54%, LR=0.068924]2025-11-05 16:56:20,054 - INFO - Step 95776: {'train_loss_batch': 2.4320998191833496, 'train_lr': 0.0689236604468653, 'batch_time': 1.6590416509652783, 'data_time': 0.006944529170146512}
2025-11-05 16:56:20 Train Epoch 038:  32%|███▏      | 800/2502 [22:08<46:45,  1.65s/it, Loss=3.3746, Top1=N/A, LR=0.068924]   2025-11-05 16:59:06,029 - INFO - Step 95876: {'train_loss_batch': 4.233363151550293, 'train_lr': 0.0689236604468653, 'batch_time': 1.6591289957215574, 'data_time': 0.006201246109199286}
2025-11-05 16:59:06 Train Epoch 038:  36%|███▌      | 900/2502 [24:54<44:17,  1.66s/it, Loss=3.3741, Top1=N/A, LR=0.068924]2025-11-05 17:01:51,892 - INFO - Step 95976: {'train_loss_batch': 2.928858757019043, 'train_lr': 0.0689236604468653, 'batch_time': 1.6590737325369849, 'data_time': 0.0056293809850525515}
2025-11-05 17:01:51 Train Epoch 038:  40%|███▉      | 1000/2502 [27:40<41:32,  1.66s/it, Loss=3.3847, Top1=58.47%, LR=0.068924]2025-11-05 17:04:37,843 - INFO - Step 96076: {'train_loss_batch': 2.5824623107910156, 'train_lr': 0.0689236604468653, 'batch_time': 1.6591173173426152, 'data_time': 0.005164535371930926}
2025-11-05 17:04:37 Train Epoch 038:  44%|████▍     | 1100/2502 [30:26<38:47,  1.66s/it, Loss=3.3916, Top1=N/A, LR=0.068924]   2025-11-05 17:07:23,800 - INFO - Step 96176: {'train_loss_batch': 4.004153251647949, 'train_lr': 0.0689236604468653, 'batch_time': 1.659157539258536, 'data_time': 0.004782938069370419}
2025-11-05 17:07:23 Train Epoch 038:  48%|████▊     | 1200/2502 [33:12<35:56,  1.66s/it, Loss=3.3837, Top1=58.43%, LR=0.068924]2025-11-05 17:10:09,699 - INFO - Step 96276: {'train_loss_batch': 2.731931209564209, 'train_lr': 0.0689236604468653, 'batch_time': 1.6591438605922346, 'data_time': 0.004467112535640263}
2025-11-05 17:10:09 Train Epoch 038:  52%|█████▏    | 1300/2502 [35:58<32:56,  1.64s/it, Loss=3.3764, Top1=N/A, LR=0.068924]   2025-11-05 17:12:55,393 - INFO - Step 96376: {'train_loss_batch': 2.6996984481811523, 'train_lr': 0.0689236604468653, 'batch_time': 1.6589744345762472, 'data_time': 0.0041994110242299714}
2025-11-05 17:12:55 Train Epoch 038:  56%|█████▌    | 1400/2502 [38:43<30:25,  1.66s/it, Loss=3.3823, Top1=N/A, LR=0.068924]2025-11-05 17:15:41,027 - INFO - Step 96476: {'train_loss_batch': 4.344245910644531, 'train_lr': 0.0689236604468653, 'batch_time': 1.6587857556802557, 'data_time': 0.003973497483323592}
2025-11-05 17:15:41 Train Epoch 038:  60%|█████▉    | 1500/2502 [41:29<27:27,  1.64s/it, Loss=3.3771, Top1=58.19%, LR=0.068924]2025-11-05 17:18:26,302 - INFO - Step 96576: {'train_loss_batch': 2.714909315109253, 'train_lr': 0.0689236604468653, 'batch_time': 1.6583840623051227, 'data_time': 0.0037725170321022965}
2025-11-05 17:18:26 Train Epoch 038:  64%|██████▍   | 1600/2502 [44:14<24:44,  1.65s/it, Loss=3.3752, Top1=N/A, LR=0.068924]   2025-11-05 17:21:11,570 - INFO - Step 96676: {'train_loss_batch': 2.814332962036133, 'train_lr': 0.0689236604468653, 'batch_time': 1.658027352577891, 'data_time': 0.0036015010192794847}
2025-11-05 17:21:11 Train Epoch 038:  68%|██████▊   | 1700/2502 [46:59<22:04,  1.65s/it, Loss=3.3824, Top1=N/A, LR=0.068924]2025-11-05 17:23:56,545 - INFO - Step 96776: {'train_loss_batch': 3.0727949142456055, 'train_lr': 0.0689236604468653, 'batch_time': 1.6575408794822446, 'data_time': 0.0034499636543000325}
2025-11-05 17:23:56 Train Epoch 038:  72%|███████▏  | 1800/2502 [49:44<19:23,  1.66s/it, Loss=3.3844, Top1=N/A, LR=0.068924]2025-11-05 17:26:42,033 - INFO - Step 96876: {'train_loss_batch': 3.8624415397644043, 'train_lr': 0.0689236604468653, 'batch_time': 1.6573927651373033, 'data_time': 0.0033150682973570457}
2025-11-05 17:26:42 Train Epoch 038:  76%|███████▌  | 1900/2502 [52:30<16:40,  1.66s/it, Loss=3.3945, Top1=N/A, LR=0.068924]2025-11-05 17:29:27,774 - INFO - Step 96976: {'train_loss_batch': 2.7637858390808105, 'train_lr': 0.0689236604468653, 'batch_time': 1.657393732050856, 'data_time': 0.0031937553279340926}
2025-11-05 17:29:27 Train Epoch 038:  80%|███████▉  | 2000/2502 [55:16<13:52,  1.66s/it, Loss=3.3985, Top1=58.15%, LR=0.068924]2025-11-05 17:32:13,681 - INFO - Step 97076: {'train_loss_batch': 2.5040206909179688, 'train_lr': 0.0689236604468653, 'batch_time': 1.6574770729878971, 'data_time': 0.003082349382597825}
2025-11-05 17:32:13 Train Epoch 038:  84%|████████▍ | 2100/2502 [58:02<11:06,  1.66s/it, Loss=3.4043, Top1=N/A, LR=0.068924]   2025-11-05 17:34:59,651 - INFO - Step 97176: {'train_loss_batch': 4.325996398925781, 'train_lr': 0.0689236604468653, 'batch_time': 1.6575829177059371, 'data_time': 0.002985321187451021}
2025-11-05 17:34:59 Train Epoch 038:  88%|████████▊ | 2200/2502 [1:00:48<08:20,  1.66s/it, Loss=3.4012, Top1=N/A, LR=0.068924]2025-11-05 17:37:45,376 - INFO - Step 97276: {'train_loss_batch': 4.768298149108887, 'train_lr': 0.0689236604468653, 'batch_time': 1.6575678920052583, 'data_time': 0.0028952825399812163}
2025-11-05 17:37:45 Train Epoch 038:  92%|█████████▏| 2300/2502 [1:03:33<05:36,  1.66s/it, Loss=3.3982, Top1=N/A, LR=0.068924]2025-11-05 17:40:30,883 - INFO - Step 97376: {'train_loss_batch': 2.766759157180786, 'train_lr': 0.0689236604468653, 'batch_time': 1.657459117183992, 'data_time': 0.0028163631394447215}
2025-11-05 17:40:30 Train Epoch 038:  96%|█████████▌| 2400/2502 [1:06:19<02:49,  1.66s/it, Loss=3.3981, Top1=N/A, LR=0.068924]2025-11-05 17:43:16,755 - INFO - Step 97476: {'train_loss_batch': 2.749202013015747, 'train_lr': 0.0689236604468653, 'batch_time': 1.6575117471663965, 'data_time': 0.0027413571789482146}
2025-11-05 17:43:16 Train Epoch 038: 100%|█████████▉| 2500/2502 [1:09:05<00:03,  1.66s/it, Loss=3.4101, Top1=N/A, LR=0.068924]2025-11-05 17:46:02,679 - INFO - Step 97576: {'train_loss_batch': 4.841307640075684, 'train_lr': 0.0689236604468653, 'batch_time': 1.6575806777699382, 'data_time': 0.0026918777891370305}
2025-11-05 17:46:02 Train Epoch 038: 100%|██████████| 2502/2502 [1:09:07<00:00,  1.66s/it, Loss=3.4101, Top1=N/A, LR=0.068924]
2025-11-05 17:46:04 Val Epoch 038:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 17:46:09   with torch.cuda.amp.autocast():
2025-11-05 17:46:09 Val Epoch 038: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.4725, Top1=62.91%, Top5=85.70%]
2025-11-05 17:47:57 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 17:47:57   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 17:47:57 2025-11-05 17:47:57,011 - INFO - Step 38: {'epoch': 38, 'learning_rate': 0.06720072618710324, 'train_loss': 3.4097658598737466, 'train_top1': 58.118476640159045, 'train_top5': 80.67493476640159, 'train_precision': 57.86540518535958, 'train_recall': 57.97777716788215, 'train_f1': 57.7142770169923, 'val_loss': 2.472473067703247, 'val_top1': 62.90799998779297, 'val_top5': 85.69999997558594, 'val_precision': 65.46279553998367, 'val_recall': 62.902, 'val_f1': 62.3035288813597}
2025-11-05 17:47:57 2025-11-05 17:47:57,012 - INFO - Epoch 038 Summary - LR: 0.067201, Train Loss: 3.4098, Val Loss: 2.4725, Val F1: 62.30%, Val Precision: 65.46%, Val Recall: 62.90%
2025-11-05 17:48:00 2025-11-05 17:48:00,015 - INFO - New best model saved with validation accuracy: 62.908%
2025-11-05 17:48:00 2025-11-05 17:48:00,015 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_039.pth
2025-11-05 17:48:00 Train Epoch 039:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 38 that is less than the current step 97576. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 17:48:03 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 17:48:03   with torch.cuda.amp.autocast():
2025-11-05 17:48:05 Train Epoch 039:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.6883, Top1=57.42%, LR=0.067201]2025-11-05 17:48:05,428 - INFO - Step 97578: {'train_loss_batch': 2.6883134841918945, 'train_lr': 0.06720072618710324, 'batch_time': 5.411345481872559, 'data_time': 3.7725918292999268}
2025-11-05 17:48:05 Train Epoch 039:   4%|▍         | 100/2502 [02:50<1:06:15,  1.66s/it, Loss=3.3386, Top1=59.75%, LR=0.067201]2025-11-05 17:50:50,917 - INFO - Step 97678: {'train_loss_batch': 2.583193302154541, 'train_lr': 0.06720072618710324, 'batch_time': 1.6920878060973517, 'data_time': 0.038474640043655244}
2025-11-05 17:50:50 Train Epoch 039:   8%|▊         | 200/2502 [05:36<1:03:39,  1.66s/it, Loss=3.3414, Top1=N/A, LR=0.067201]   2025-11-05 17:53:36,487 - INFO - Step 97778: {'train_loss_batch': 2.663667678833008, 'train_lr': 0.06720072618710324, 'batch_time': 1.6739816535171586, 'data_time': 0.01984347395635956}
2025-11-05 17:53:36 Train Epoch 039:  12%|█▏        | 300/2502 [08:22<1:00:39,  1.65s/it, Loss=3.3154, Top1=58.86%, LR=0.067201]2025-11-05 17:56:22,249 - INFO - Step 97878: {'train_loss_batch': 2.656195640563965, 'train_lr': 0.06720072618710324, 'batch_time': 1.6685466425759452, 'data_time': 0.01359563491669208}
2025-11-05 17:56:22 Train Epoch 039:  16%|█▌        | 400/2502 [11:08<57:58,  1.65s/it, Loss=3.3664, Top1=N/A, LR=0.067201]   2025-11-05 17:59:08,176 - INFO - Step 97978: {'train_loss_batch': 4.265912055969238, 'train_lr': 0.06720072618710324, 'batch_time': 1.6662318510307637, 'data_time': 0.010461866112421279}
2025-11-05 17:59:08 Train Epoch 039:  20%|█▉        | 500/2502 [13:53<55:27,  1.66s/it, Loss=3.3690, Top1=N/A, LR=0.067201]2025-11-05 18:01:53,754 - INFO - Step 98078: {'train_loss_batch': 3.6837158203125, 'train_lr': 0.06720072618710324, 'batch_time': 1.664145090860759, 'data_time': 0.008576166130111603}
2025-11-05 18:01:53 Train Epoch 039:  24%|██▍       | 600/2502 [16:38<52:40,  1.66s/it, Loss=3.3805, Top1=N/A, LR=0.067201]2025-11-05 18:04:38,578 - INFO - Step 98178: {'train_loss_batch': 4.434859275817871, 'train_lr': 0.06720072618710324, 'batch_time': 1.661498105466465, 'data_time': 0.007320623032860272}
2025-11-05 18:04:38 Train Epoch 039:  28%|██▊       | 700/2502 [19:24<49:35,  1.65s/it, Loss=3.3823, Top1=58.68%, LR=0.067201]2025-11-05 18:07:24,182 - INFO - Step 98278: {'train_loss_batch': 2.5876269340515137, 'train_lr': 0.06720072618710324, 'batch_time': 1.6607197779901697, 'data_time': 0.006415556228790065}
2025-11-05 18:07:24 Train Epoch 039:  32%|███▏      | 800/2502 [22:09<47:09,  1.66s/it, Loss=3.3696, Top1=N/A, LR=0.067201]   2025-11-05 18:10:09,972 - INFO - Step 98378: {'train_loss_batch': 2.705735206604004, 'train_lr': 0.06720072618710324, 'batch_time': 1.66036758530006, 'data_time': 0.005740851201070531}
2025-11-05 18:10:09 Train Epoch 039:  36%|███▌      | 900/2502 [24:55<44:23,  1.66s/it, Loss=3.3696, Top1=N/A, LR=0.067201]2025-11-05 18:12:55,774 - INFO - Step 98478: {'train_loss_batch': 3.3658456802368164, 'train_lr': 0.06720072618710324, 'batch_time': 1.66010594897212, 'data_time': 0.005215015051499853}
2025-11-05 18:12:55 Train Epoch 039:  40%|███▉      | 1000/2502 [27:41<41:08,  1.64s/it, Loss=3.3680, Top1=58.61%, LR=0.067201]2025-11-05 18:15:41,305 - INFO - Step 98578: {'train_loss_batch': 2.6989264488220215, 'train_lr': 0.06720072618710324, 'batch_time': 1.6596278622672036, 'data_time': 0.004796332769936972}
2025-11-05 18:15:41 Train Epoch 039:  44%|████▍     | 1100/2502 [30:26<38:49,  1.66s/it, Loss=3.3770, Top1=N/A, LR=0.067201]   2025-11-05 18:18:26,906 - INFO - Step 98678: {'train_loss_batch': 2.6715121269226074, 'train_lr': 0.06720072618710324, 'batch_time': 1.6592983285694314, 'data_time': 0.004451789171667558}
2025-11-05 18:18:26 Train Epoch 039:  48%|████▊     | 1200/2502 [33:12<35:50,  1.65s/it, Loss=3.3702, Top1=58.58%, LR=0.067201]2025-11-05 18:21:12,424 - INFO - Step 98778: {'train_loss_batch': 2.670358657836914, 'train_lr': 0.06720072618710324, 'batch_time': 1.6589554150237529, 'data_time': 0.004164077161650773}
2025-11-05 18:21:12 Train Epoch 039:  52%|█████▏    | 1300/2502 [35:57<33:15,  1.66s/it, Loss=3.3682, Top1=N/A, LR=0.067201]   2025-11-05 18:23:57,709 - INFO - Step 98878: {'train_loss_batch': 2.621295213699341, 'train_lr': 0.06720072618710324, 'batch_time': 1.658485730183665, 'data_time': 0.003920462386228779}
2025-11-05 18:23:57 Train Epoch 039:  56%|█████▌    | 1400/2502 [38:42<30:30,  1.66s/it, Loss=3.3666, Top1=N/A, LR=0.067201]2025-11-05 18:26:42,042 - INFO - Step 98978: {'train_loss_batch': 4.021278381347656, 'train_lr': 0.06720072618710324, 'batch_time': 1.6574042862777112, 'data_time': 0.0037132689648232743}
2025-11-05 18:26:42 Train Epoch 039:  60%|█████▉    | 1500/2502 [41:27<27:25,  1.64s/it, Loss=3.3733, Top1=N/A, LR=0.067201]2025-11-05 18:29:27,164 - INFO - Step 99078: {'train_loss_batch': 2.7093780040740967, 'train_lr': 0.06720072618710324, 'batch_time': 1.6569915613597588, 'data_time': 0.003532991021732582}
2025-11-05 18:29:27 Train Epoch 039:  64%|██████▍   | 1600/2502 [44:12<24:54,  1.66s/it, Loss=3.3721, Top1=N/A, LR=0.067201]2025-11-05 18:32:12,354 - INFO - Step 99178: {'train_loss_batch': 3.2297892570495605, 'train_lr': 0.06720072618710324, 'batch_time': 1.6566737149373805, 'data_time': 0.0033759001565679472}
2025-11-05 18:32:12 Train Epoch 039:  68%|██████▊   | 1700/2502 [46:56<21:54,  1.64s/it, Loss=3.3705, Top1=N/A, LR=0.067201]2025-11-05 18:34:56,157 - INFO - Step 99278: {'train_loss_batch': 4.832508087158203, 'train_lr': 0.06720072618710324, 'batch_time': 1.6555773183081446, 'data_time': 0.003236455121228164}
2025-11-05 18:34:56 Train Epoch 039:  72%|███████▏  | 1800/2502 [49:41<19:25,  1.66s/it, Loss=3.3703, Top1=58.53%, LR=0.067201]2025-11-05 18:37:41,840 - INFO - Step 99378: {'train_loss_batch': 2.691290855407715, 'train_lr': 0.06720072618710324, 'batch_time': 1.6556466360478717, 'data_time': 0.00311571369562991}
2025-11-05 18:37:41 Train Epoch 039:  76%|███████▌  | 1900/2502 [52:27<16:34,  1.65s/it, Loss=3.3704, Top1=N/A, LR=0.067201]   2025-11-05 18:40:27,050 - INFO - Step 99478: {'train_loss_batch': 4.252022743225098, 'train_lr': 0.06720072618710324, 'batch_time': 1.6554604228830914, 'data_time': 0.003009354422306902}
2025-11-05 18:40:27 Train Epoch 039:  80%|███████▉  | 2000/2502 [55:12<13:48,  1.65s/it, Loss=3.3736, Top1=58.50%, LR=0.067201]2025-11-05 18:43:12,182 - INFO - Step 99578: {'train_loss_batch': 2.634767532348633, 'train_lr': 0.06720072618710324, 'batch_time': 1.6552530452646297, 'data_time': 0.0029121398449182393}
2025-11-05 18:43:12 Train Epoch 039:  84%|████████▍ | 2100/2502 [57:57<10:57,  1.63s/it, Loss=3.3760, Top1=N/A, LR=0.067201]   2025-11-05 18:45:57,322 - INFO - Step 99678: {'train_loss_batch': 2.868952989578247, 'train_lr': 0.06720072618710324, 'batch_time': 1.6550698793257832, 'data_time': 0.0028224485026036826}
2025-11-05 18:45:57 Train Epoch 039:  88%|████████▊ | 2200/2502 [1:00:42<08:17,  1.65s/it, Loss=3.3751, Top1=58.46%, LR=0.067201]2025-11-05 18:48:42,183 - INFO - Step 99778: {'train_loss_batch': 2.654040813446045, 'train_lr': 0.06720072618710324, 'batch_time': 1.6547760664468893, 'data_time': 0.0027406737783831504}
2025-11-05 18:48:42 Train Epoch 039:  92%|█████████▏| 2300/2502 [1:03:27<05:33,  1.65s/it, Loss=3.3778, Top1=N/A, LR=0.067201]   2025-11-05 18:51:27,503 - INFO - Step 99878: {'train_loss_batch': 3.8527145385742188, 'train_lr': 0.06720072618710324, 'batch_time': 1.6547076343609322, 'data_time': 0.0026650419446604505}
2025-11-05 18:51:27 Train Epoch 039:  96%|█████████▌| 2400/2502 [1:06:13<02:49,  1.66s/it, Loss=3.3774, Top1=N/A, LR=0.067201]2025-11-05 18:54:13,238 - INFO - Step 99978: {'train_loss_batch': 4.058435440063477, 'train_lr': 0.06720072618710324, 'batch_time': 1.654817507992879, 'data_time': 0.0025956631699784105}
2025-11-05 18:54:13 Train Epoch 039: 100%|█████████▉| 2500/2502 [1:08:58<00:03,  1.67s/it, Loss=3.3710, Top1=N/A, LR=0.067201]2025-11-05 18:56:58,885 - INFO - Step 100078: {'train_loss_batch': 3.427398920059204, 'train_lr': 0.06720072618710324, 'batch_time': 1.6548835466690703, 'data_time': 0.00255488586730835}
2025-11-05 18:56:58 Train Epoch 039: 100%|██████████| 2502/2502 [1:09:00<00:00,  1.65s/it, Loss=3.3710, Top1=N/A, LR=0.067201]
2025-11-05 18:57:01 Val Epoch 039:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 18:57:05   with torch.cuda.amp.autocast():
2025-11-05 18:57:06 Val Epoch 039: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=2.5028, Top1=61.24%, Top5=84.43%]
2025-11-05 18:58:52 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 18:58:52   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 18:58:52 2025-11-05 18:58:52,191 - INFO - Step 39: {'epoch': 39, 'learning_rate': 0.0654543046337755, 'train_loss': 3.3713130137140896, 'train_top1': 58.44179764243615, 'train_top5': 80.89237475442043, 'train_precision': 58.23893577315879, 'train_recall': 58.34900932735154, 'train_f1': 58.079974183301786, 'val_loss': 2.50280450302124, 'val_top1': 61.24399999023437, 'val_top5': 84.426, 'val_precision': 64.38505375303441, 'val_recall': 61.242000000000004, 'val_f1': 60.68525274241476}
2025-11-05 18:58:52 2025-11-05 18:58:52,193 - INFO - Epoch 039 Summary - LR: 0.065454, Train Loss: 3.3713, Val Loss: 2.5028, Val F1: 60.69%, Val Precision: 64.39%, Val Recall: 61.24%
2025-11-05 18:58:53 2025-11-05 18:58:53,649 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_040.pth
2025-11-05 18:58:53 Train Epoch 040:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 18:58:57   with torch.cuda.amp.autocast():
2025-11-05 18:58:58 Train Epoch 040:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.3641, Top1=N/A, LR=0.065454]2025-11-05 18:58:58,983 - INFO - Step 100080: {'train_loss_batch': 4.364059925079346, 'train_lr': 0.0654543046337755, 'batch_time': 5.331465005874634, 'data_time': 3.6862618923187256}
2025-11-05 18:58:58 Train Epoch 040:   0%|          | 1/2502 [00:05<3:42:18,  5.33s/it, Loss=4.3641, Top1=N/A, LR=0.065454]wandb: WARNING Tried to log to step 39 that is less than the current step 100078. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 18:59:00 Train Epoch 040:   4%|▍         | 100/2502 [02:51<1:06:22,  1.66s/it, Loss=3.3030, Top1=58.18%, LR=0.065454]2025-11-05 19:01:44,766 - INFO - Step 100180: {'train_loss_batch': 2.823566436767578, 'train_lr': 0.0654543046337755, 'batch_time': 1.6942060088167097, 'data_time': 0.037480290573422274}
2025-11-05 19:01:44 Train Epoch 040:   8%|▊         | 200/2502 [05:35<1:02:58,  1.64s/it, Loss=3.2586, Top1=N/A, LR=0.065454]   2025-11-05 19:04:29,580 - INFO - Step 100280: {'train_loss_batch': 2.974895477294922, 'train_lr': 0.0654543046337755, 'batch_time': 1.6712875354349317, 'data_time': 0.019355639889465634}
2025-11-05 19:04:29 Train Epoch 040:  12%|█▏        | 300/2502 [08:21<1:00:20,  1.64s/it, Loss=3.2681, Top1=N/A, LR=0.065454]2025-11-05 19:07:14,726 - INFO - Step 100380: {'train_loss_batch': 3.3581669330596924, 'train_lr': 0.0654543046337755, 'batch_time': 1.6646991956273582, 'data_time': 0.013255501110292353}
2025-11-05 19:07:14 Train Epoch 040:  16%|█▌        | 400/2502 [11:06<58:03,  1.66s/it, Loss=3.2675, Top1=N/A, LR=0.065454]2025-11-05 19:09:59,967 - INFO - Step 100480: {'train_loss_batch': 2.6840929985046387, 'train_lr': 0.0654543046337755, 'batch_time': 1.6616335147039552, 'data_time': 0.010191321075705816}
2025-11-05 19:09:59 Train Epoch 040:  20%|█▉        | 500/2502 [13:51<55:21,  1.66s/it, Loss=3.2860, Top1=N/A, LR=0.065454]2025-11-05 19:12:44,665 - INFO - Step 100580: {'train_loss_batch': 3.9526896476745605, 'train_lr': 0.0654543046337755, 'batch_time': 1.6587087884396612, 'data_time': 0.008360032787817918}
2025-11-05 19:12:44 Train Epoch 040:  24%|██▍       | 600/2502 [16:36<52:45,  1.66s/it, Loss=3.3158, Top1=58.94%, LR=0.065454]2025-11-05 19:15:30,532 - INFO - Step 100680: {'train_loss_batch': 2.617838144302368, 'train_lr': 0.0654543046337755, 'batch_time': 1.6587016487280264, 'data_time': 0.0071419812676910555}
2025-11-05 19:15:30 Train Epoch 040:  28%|██▊       | 700/2502 [19:22<49:36,  1.65s/it, Loss=3.3296, Top1=N/A, LR=0.065454]   2025-11-05 19:18:16,158 - INFO - Step 100780: {'train_loss_batch': 3.2368595600128174, 'train_lr': 0.0654543046337755, 'batch_time': 1.6583536582734548, 'data_time': 0.006272075859864326}
2025-11-05 19:18:16 Train Epoch 040:  32%|███▏      | 800/2502 [22:07<46:50,  1.65s/it, Loss=3.3475, Top1=N/A, LR=0.065454]2025-11-05 19:21:01,617 - INFO - Step 100880: {'train_loss_batch': 4.11580228805542, 'train_lr': 0.0654543046337755, 'batch_time': 1.6578828088948492, 'data_time': 0.005613410368691967}
2025-11-05 19:21:01 Train Epoch 040:  36%|███▌      | 900/2502 [24:53<44:10,  1.65s/it, Loss=3.3576, Top1=N/A, LR=0.065454]2025-11-05 19:23:47,051 - INFO - Step 100980: {'train_loss_batch': 4.2995991706848145, 'train_lr': 0.0654543046337755, 'batch_time': 1.6574899292944802, 'data_time': 0.005097590064367364}
2025-11-05 19:23:47 Train Epoch 040:  40%|███▉      | 1000/2502 [27:39<41:27,  1.66s/it, Loss=3.3663, Top1=58.78%, LR=0.065454]2025-11-05 19:26:32,788 - INFO - Step 101080: {'train_loss_batch': 2.702075481414795, 'train_lr': 0.0654543046337755, 'batch_time': 1.657477248084176, 'data_time': 0.004698025239454759}
2025-11-05 19:26:32 Train Epoch 040:  44%|████▍     | 1100/2502 [30:24<38:46,  1.66s/it, Loss=3.3644, Top1=N/A, LR=0.065454]   2025-11-05 19:29:18,559 - INFO - Step 101180: {'train_loss_batch': 4.7985429763793945, 'train_lr': 0.0654543046337755, 'batch_time': 1.657497733858474, 'data_time': 0.004369085426226623}
2025-11-05 19:29:18 Train Epoch 040:  48%|████▊     | 1200/2502 [33:08<35:49,  1.65s/it, Loss=3.3677, Top1=N/A, LR=0.065454]2025-11-05 19:32:02,614 - INFO - Step 101280: {'train_loss_batch': 3.137946844100952, 'train_lr': 0.0654543046337755, 'batch_time': 1.6560869872024118, 'data_time': 0.004092481710035338}
2025-11-05 19:32:02 Train Epoch 040:  52%|█████▏    | 1300/2502 [35:53<33:09,  1.66s/it, Loss=3.3807, Top1=N/A, LR=0.065454]2025-11-05 19:34:47,644 - INFO - Step 101380: {'train_loss_batch': 4.749782562255859, 'train_lr': 0.0654543046337755, 'batch_time': 1.6556419232183013, 'data_time': 0.003858216078257579}
2025-11-05 19:34:47 Train Epoch 040:  56%|█████▌    | 1400/2502 [38:39<30:24,  1.66s/it, Loss=3.3787, Top1=58.66%, LR=0.065454]2025-11-05 19:37:33,350 - INFO - Step 101480: {'train_loss_batch': 2.6870129108428955, 'train_lr': 0.0654543046337755, 'batch_time': 1.6557432177405456, 'data_time': 0.003660695030381218}
2025-11-05 19:37:33 Train Epoch 040:  60%|█████▉    | 1500/2502 [41:25<27:29,  1.65s/it, Loss=3.3895, Top1=N/A, LR=0.065454]   2025-11-05 19:40:19,078 - INFO - Step 101580: {'train_loss_batch': 3.2337355613708496, 'train_lr': 0.0654543046337755, 'batch_time': 1.6558451198244952, 'data_time': 0.0034840500886880264}
2025-11-05 19:40:19 Train Epoch 040:  64%|██████▍   | 1600/2502 [44:11<24:57,  1.66s/it, Loss=3.3857, Top1=N/A, LR=0.065454]2025-11-05 19:43:04,908 - INFO - Step 101680: {'train_loss_batch': 2.665940999984741, 'train_lr': 0.0654543046337755, 'batch_time': 1.655998667205296, 'data_time': 0.0033300536487491186}
2025-11-05 19:43:04 Train Epoch 040:  68%|██████▊   | 1700/2502 [46:56<21:58,  1.64s/it, Loss=3.3843, Top1=N/A, LR=0.065454]2025-11-05 19:45:50,063 - INFO - Step 101780: {'train_loss_batch': 4.129165172576904, 'train_lr': 0.0654543046337755, 'batch_time': 1.6557366636344646, 'data_time': 0.003195506134571432}
2025-11-05 19:45:50 Train Epoch 040:  72%|███████▏  | 1800/2502 [49:40<19:05,  1.63s/it, Loss=3.3855, Top1=N/A, LR=0.065454]2025-11-05 19:48:34,041 - INFO - Step 101880: {'train_loss_batch': 2.829741954803467, 'train_lr': 0.0654543046337755, 'batch_time': 1.654850710106849, 'data_time': 0.0030721476976901937}
2025-11-05 19:48:34 Train Epoch 040:  76%|███████▌  | 1900/2502 [52:25<16:32,  1.65s/it, Loss=3.3897, Top1=N/A, LR=0.065454]2025-11-05 19:51:18,937 - INFO - Step 101980: {'train_loss_batch': 4.427835464477539, 'train_lr': 0.0654543046337755, 'batch_time': 1.6545403637050766, 'data_time': 0.002967184183159858}
2025-11-05 19:51:18 Train Epoch 040:  80%|███████▉  | 2000/2502 [55:10<13:50,  1.65s/it, Loss=3.3884, Top1=N/A, LR=0.065454]2025-11-05 19:54:04,427 - INFO - Step 102080: {'train_loss_batch': 3.796781301498413, 'train_lr': 0.0654543046337755, 'batch_time': 1.6545585089478119, 'data_time': 0.002873987867973972}
2025-11-05 19:54:04 Train Epoch 040:  84%|████████▍ | 2100/2502 [57:56<11:08,  1.66s/it, Loss=3.3857, Top1=N/A, LR=0.065454]2025-11-05 19:56:50,454 - INFO - Step 102180: {'train_loss_batch': 2.7364890575408936, 'train_lr': 0.0654543046337755, 'batch_time': 1.6548301100106764, 'data_time': 0.0027897422623486814}
2025-11-05 19:56:50 Train Epoch 040:  88%|████████▊ | 2200/2502 [1:00:41<08:17,  1.65s/it, Loss=3.3818, Top1=N/A, LR=0.065454]2025-11-05 19:59:35,436 - INFO - Step 102280: {'train_loss_batch': 4.504912376403809, 'train_lr': 0.0654543046337755, 'batch_time': 1.654602303065153, 'data_time': 0.002709451560159533}
2025-11-05 19:59:35 Train Epoch 040:  92%|█████████▏| 2300/2502 [1:03:27<05:34,  1.65s/it, Loss=3.3805, Top1=N/A, LR=0.065454]2025-11-05 20:02:21,239 - INFO - Step 102380: {'train_loss_batch': 4.480802059173584, 'train_lr': 0.0654543046337755, 'batch_time': 1.6547511056007276, 'data_time': 0.0026408124622807508}
2025-11-05 20:02:21 Train Epoch 040:  96%|█████████▌| 2400/2502 [1:06:13<02:47,  1.64s/it, Loss=3.3810, Top1=N/A, LR=0.065454]2025-11-05 20:05:06,673 - INFO - Step 102480: {'train_loss_batch': 5.138637542724609, 'train_lr': 0.0654543046337755, 'batch_time': 1.654734360680189, 'data_time': 0.0025762304173365873}
2025-11-05 20:05:06 Train Epoch 040: 100%|█████████▉| 2500/2502 [1:08:58<00:03,  1.66s/it, Loss=3.3804, Top1=N/A, LR=0.065454]2025-11-05 20:07:51,835 - INFO - Step 102580: {'train_loss_batch': 5.191471099853516, 'train_lr': 0.0654543046337755, 'batch_time': 1.6546095503372749, 'data_time': 0.0025494116775897063}
2025-11-05 20:07:51 Train Epoch 040: 100%|██████████| 2502/2502 [1:08:59<00:00,  1.65s/it, Loss=3.3804, Top1=N/A, LR=0.065454]
2025-11-05 20:07:54 Val Epoch 040:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 20:07:58   with torch.cuda.amp.autocast():
2025-11-05 20:07:59 Val Epoch 040: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.4683, Top1=61.89%, Top5=85.08%]
2025-11-05 20:09:45 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 20:09:45   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 20:09:45 2025-11-05 20:09:45,791 - INFO - Step 40: {'epoch': 40, 'learning_rate': 0.0636867811886538, 'train_loss': 3.3801618397092934, 'train_top1': 58.611571810986966, 'train_top5': 81.11288116852886, 'train_precision': 58.395336905202285, 'train_recall': 58.485673457097874, 'train_f1': 58.2258689233074, 'val_loss': 2.468327159576416, 'val_top1': 61.893999993896486, 'val_top5': 85.07599999511719, 'val_precision': 65.16578174569509, 'val_recall': 61.89, 'val_f1': 61.34742201214499}
2025-11-05 20:09:45 2025-11-05 20:09:45,793 - INFO - Epoch 040 Summary - LR: 0.063687, Train Loss: 3.3802, Val Loss: 2.4683, Val F1: 61.35%, Val Precision: 65.17%, Val Recall: 61.89%
2025-11-05 20:09:46 Train Epoch 041:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 40 that is less than the current step 102580. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 20:09:50 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 20:09:50   with torch.cuda.amp.autocast():
2025-11-05 20:09:52 Train Epoch 041:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.5566, Top1=59.38%, LR=0.063687]2025-11-05 20:09:52,598 - INFO - Step 102582: {'train_loss_batch': 2.556624174118042, 'train_lr': 0.0636867811886538, 'batch_time': 5.974704742431641, 'data_time': 4.339211702346802}
2025-11-05 20:09:52 Train Epoch 041:   4%|▍         | 100/2502 [02:51<1:06:32,  1.66s/it, Loss=3.2698, Top1=N/A, LR=0.063687]   2025-11-05 20:12:37,832 - INFO - Step 102682: {'train_loss_batch': 2.815399408340454, 'train_lr': 0.0636867811886538, 'batch_time': 1.6951478519062004, 'data_time': 0.04402649756705407}
2025-11-05 20:12:37 Train Epoch 041:   8%|▊         | 200/2502 [05:36<1:03:44,  1.66s/it, Loss=3.3352, Top1=N/A, LR=0.063687]2025-11-05 20:15:22,999 - INFO - Step 102782: {'train_loss_batch': 2.99139404296875, 'train_lr': 0.0636867811886538, 'batch_time': 1.673515697023762, 'data_time': 0.02261855709018992}
2025-11-05 20:15:23 Train Epoch 041:  12%|█▏        | 300/2502 [08:21<1:00:32,  1.65s/it, Loss=3.3175, Top1=N/A, LR=0.063687]2025-11-05 20:18:08,463 - INFO - Step 102882: {'train_loss_batch': 2.509155035018921, 'train_lr': 0.0636867811886538, 'batch_time': 1.6672422046281175, 'data_time': 0.015416739390934029}
2025-11-05 20:18:08 Train Epoch 041:  16%|█▌        | 400/2502 [11:07<57:27,  1.64s/it, Loss=3.3038, Top1=N/A, LR=0.063687]2025-11-05 20:20:53,799 - INFO - Step 102982: {'train_loss_batch': 2.723114252090454, 'train_lr': 0.0636867811886538, 'batch_time': 1.6637812528824272, 'data_time': 0.011824590606879713}
2025-11-05 20:20:53 Train Epoch 041:  20%|█▉        | 500/2502 [13:51<55:02,  1.65s/it, Loss=3.3101, Top1=N/A, LR=0.063687]2025-11-05 20:23:38,417 - INFO - Step 103082: {'train_loss_batch': 3.2574968338012695, 'train_lr': 0.0636867811886538, 'batch_time': 1.6602657655042088, 'data_time': 0.009670152397688753}
2025-11-05 20:23:38 Train Epoch 041:  24%|██▍       | 600/2502 [16:37<52:22,  1.65s/it, Loss=3.3188, Top1=N/A, LR=0.063687]2025-11-05 20:26:23,704 - INFO - Step 103182: {'train_loss_batch': 2.6551592350006104, 'train_lr': 0.0636867811886538, 'batch_time': 1.6590353958618622, 'data_time': 0.008224337350905636}
2025-11-05 20:26:23 Train Epoch 041:  28%|██▊       | 700/2502 [19:23<49:57,  1.66s/it, Loss=3.3367, Top1=N/A, LR=0.063687]2025-11-05 20:29:09,734 - INFO - Step 103282: {'train_loss_batch': 4.731782913208008, 'train_lr': 0.0636867811886538, 'batch_time': 1.6592148353641962, 'data_time': 0.007216721220465428}
2025-11-05 20:29:09 Train Epoch 041:  32%|███▏      | 800/2502 [22:08<47:06,  1.66s/it, Loss=3.3497, Top1=N/A, LR=0.063687]2025-11-05 20:31:55,468 - INFO - Step 103382: {'train_loss_batch': 2.7035298347473145, 'train_lr': 0.0636867811886538, 'batch_time': 1.6589805177981487, 'data_time': 0.00644814506749833}
2025-11-05 20:31:55 Train Epoch 041:  36%|███▌      | 900/2502 [24:53<44:15,  1.66s/it, Loss=3.3495, Top1=N/A, LR=0.063687]2025-11-05 20:34:39,884 - INFO - Step 103482: {'train_loss_batch': 4.317180156707764, 'train_lr': 0.0636867811886538, 'batch_time': 1.6573348074456828, 'data_time': 0.005843572161438462}
2025-11-05 20:34:39 Train Epoch 041:  40%|███▉      | 1000/2502 [27:38<41:26,  1.66s/it, Loss=3.3534, Top1=N/A, LR=0.063687]2025-11-05 20:37:25,312 - INFO - Step 103582: {'train_loss_batch': 3.5387611389160156, 'train_lr': 0.0636867811886538, 'batch_time': 1.657029942913608, 'data_time': 0.005367126855459604}
2025-11-05 20:37:25 Train Epoch 041:  44%|████▍     | 1100/2502 [30:24<38:28,  1.65s/it, Loss=3.3484, Top1=59.12%, LR=0.063687]2025-11-05 20:40:10,801 - INFO - Step 103682: {'train_loss_batch': 2.692122459411621, 'train_lr': 0.0636867811886538, 'batch_time': 1.6568355042754683, 'data_time': 0.004977898203601196}
2025-11-05 20:40:10 Train Epoch 041:  48%|████▊     | 1200/2502 [33:09<35:45,  1.65s/it, Loss=3.3419, Top1=N/A, LR=0.063687]   2025-11-05 20:42:55,908 - INFO - Step 103782: {'train_loss_batch': 4.298774719238281, 'train_lr': 0.0636867811886538, 'batch_time': 1.656355163636156, 'data_time': 0.004656632079569922}
2025-11-05 20:42:55 Train Epoch 041:  52%|█████▏    | 1300/2502 [35:54<33:07,  1.65s/it, Loss=3.3398, Top1=N/A, LR=0.063687]2025-11-05 20:45:41,594 - INFO - Step 103882: {'train_loss_batch': 4.253546714782715, 'train_lr': 0.0636867811886538, 'batch_time': 1.6563934757561065, 'data_time': 0.004388712443909583}
2025-11-05 20:45:41 Train Epoch 041:  56%|█████▌    | 1400/2502 [38:40<30:23,  1.65s/it, Loss=3.3448, Top1=N/A, LR=0.063687]2025-11-05 20:48:27,228 - INFO - Step 103982: {'train_loss_batch': 5.1196064949035645, 'train_lr': 0.0636867811886538, 'batch_time': 1.656389585143749, 'data_time': 0.004154093515694269}
2025-11-05 20:48:27 Train Epoch 041:  60%|█████▉    | 1500/2502 [41:25<27:40,  1.66s/it, Loss=3.3454, Top1=N/A, LR=0.063687]2025-11-05 20:51:12,178 - INFO - Step 104082: {'train_loss_batch': 4.420144081115723, 'train_lr': 0.0636867811886538, 'batch_time': 1.6559306088485057, 'data_time': 0.003947571704262181}
2025-11-05 20:51:12 Train Epoch 041:  64%|██████▍   | 1600/2502 [44:11<24:53,  1.66s/it, Loss=3.3371, Top1=59.14%, LR=0.063687]2025-11-05 20:53:57,860 - INFO - Step 104182: {'train_loss_batch': 2.7801618576049805, 'train_lr': 0.0636867811886538, 'batch_time': 1.6559865852954014, 'data_time': 0.0037685503891153236}
2025-11-05 20:53:57 Train Epoch 041:  68%|██████▊   | 1700/2502 [46:56<22:12,  1.66s/it, Loss=3.3408, Top1=N/A, LR=0.063687]   2025-11-05 20:56:43,021 - INFO - Step 104282: {'train_loss_batch': 2.7653236389160156, 'train_lr': 0.0636867811886538, 'batch_time': 1.655728688175576, 'data_time': 0.0036101981796004506}
2025-11-05 20:56:43 Train Epoch 041:  72%|███████▏  | 1800/2502 [49:39<19:16,  1.65s/it, Loss=3.3382, Top1=N/A, LR=0.063687]2025-11-05 20:59:26,578 - INFO - Step 104382: {'train_loss_batch': 2.6811623573303223, 'train_lr': 0.0636867811886538, 'batch_time': 1.6546090648678657, 'data_time': 0.0034703260524480755}
2025-11-05 20:59:26 Train Epoch 041:  76%|███████▌  | 1900/2502 [52:25<16:34,  1.65s/it, Loss=3.3395, Top1=N/A, LR=0.063687]2025-11-05 21:02:12,070 - INFO - Step 104482: {'train_loss_batch': 2.6669647693634033, 'train_lr': 0.0636867811886538, 'batch_time': 1.6546253504595088, 'data_time': 0.0033493963810470716}
2025-11-05 21:02:12 Train Epoch 041:  80%|███████▉  | 2000/2502 [55:11<13:55,  1.66s/it, Loss=3.3382, Top1=N/A, LR=0.063687]2025-11-05 21:04:57,897 - INFO - Step 104582: {'train_loss_batch': 2.5798556804656982, 'train_lr': 0.0636867811886538, 'batch_time': 1.6548074605999918, 'data_time': 0.0032359993737796017}
2025-11-05 21:04:57 Train Epoch 041:  84%|████████▍ | 2100/2502 [57:57<11:09,  1.67s/it, Loss=3.3493, Top1=N/A, LR=0.063687]2025-11-05 21:07:43,652 - INFO - Step 104682: {'train_loss_batch': 2.6048498153686523, 'train_lr': 0.0636867811886538, 'batch_time': 1.6549378666975338, 'data_time': 0.0031302417816859323}
2025-11-05 21:07:43 Train Epoch 041:  88%|████████▊ | 2200/2502 [1:00:42<08:21,  1.66s/it, Loss=3.3516, Top1=59.10%, LR=0.063687]2025-11-05 21:10:29,380 - INFO - Step 104782: {'train_loss_batch': 2.7201919555664062, 'train_lr': 0.0636867811886538, 'batch_time': 1.65504425387662, 'data_time': 0.0030389434367296426}
2025-11-05 21:10:29 Train Epoch 041:  92%|█████████▏| 2300/2502 [1:03:28<05:33,  1.65s/it, Loss=3.3487, Top1=N/A, LR=0.063687]   2025-11-05 21:13:15,088 - INFO - Step 104882: {'train_loss_batch': 3.41695237159729, 'train_lr': 0.0636867811886538, 'batch_time': 1.6551326033033946, 'data_time': 0.0029537437377624644}
2025-11-05 21:13:15 Train Epoch 041:  96%|█████████▌| 2400/2502 [1:06:12<02:47,  1.64s/it, Loss=3.3529, Top1=N/A, LR=0.063687]2025-11-05 21:15:59,386 - INFO - Step 104982: {'train_loss_batch': 4.3002848625183105, 'train_lr': 0.0636867811886538, 'batch_time': 1.6546265461304843, 'data_time': 0.0028786090253443085}
2025-11-05 21:15:59 Train Epoch 041: 100%|█████████▉| 2500/2502 [1:08:58<00:03,  1.65s/it, Loss=3.3523, Top1=N/A, LR=0.063687]2025-11-05 21:18:44,795 - INFO - Step 105082: {'train_loss_batch': 2.98897647857666, 'train_lr': 0.0636867811886538, 'batch_time': 1.6546048343968076, 'data_time': 0.002833506528113852}
2025-11-05 21:18:44 Train Epoch 041: 100%|██████████| 2502/2502 [1:08:59<00:00,  1.65s/it, Loss=3.3523, Top1=N/A, LR=0.063687]
2025-11-05 21:18:47 Val Epoch 041:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 21:18:51   with torch.cuda.amp.autocast():
2025-11-05 21:18:51 Val Epoch 041: 100%|██████████| 98/98 [01:52<00:00,  1.15s/it, Loss=2.3973, Top1=64.14%, Top5=86.37%]
2025-11-05 21:20:39 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 21:20:39   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 21:20:39 2025-11-05 21:20:39,710 - INFO - Step 41: {'epoch': 41, 'learning_rate': 0.06190057007615573, 'train_loss': 3.3520123559317523, 'train_top1': 59.087527056277054, 'train_top5': 81.25803233225108, 'train_precision': 58.8044033701422, 'train_recall': 58.931555258764114, 'train_f1': 58.65235725349537, 'val_loss': 2.397339331970215, 'val_top1': 64.1439999963379, 'val_top5': 86.36600001220702, 'val_precision': 66.54760537456926, 'val_recall': 64.146, 'val_f1': 63.67344538589025}
2025-11-05 21:20:39 2025-11-05 21:20:39,711 - INFO - Epoch 041 Summary - LR: 0.061901, Train Loss: 3.3520, Val Loss: 2.3973, Val F1: 63.67%, Val Precision: 66.55%, Val Recall: 64.15%
2025-11-05 21:20:40 wandb: WARNING Tried to log to step 41 that is less than the current step 105082. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 21:20:43 2025-11-05 21:20:43,471 - INFO - New best model saved with validation accuracy: 64.144%
2025-11-05 21:20:43 2025-11-05 21:20:43,472 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_042.pth
2025-11-05 21:20:43 Train Epoch 042:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 21:20:47   with torch.cuda.amp.autocast():
2025-11-05 21:20:49 Train Epoch 042:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.6132, Top1=59.77%, LR=0.061901]2025-11-05 21:20:49,281 - INFO - Step 105084: {'train_loss_batch': 2.613213539123535, 'train_lr': 0.06190057007615573, 'batch_time': 5.807955741882324, 'data_time': 4.176884412765503}
2025-11-05 21:20:49 Train Epoch 042:   4%|▍         | 100/2502 [02:51<1:06:19,  1.66s/it, Loss=3.2177, Top1=N/A, LR=0.061901]   2025-11-05 21:23:34,893 - INFO - Step 105184: {'train_loss_batch': 2.7397491931915283, 'train_lr': 0.06190057007615573, 'batch_time': 1.6972310047338504, 'data_time': 0.04235265986754162}
2025-11-05 21:23:34 Train Epoch 042:   8%|▊         | 200/2502 [05:37<1:03:01,  1.64s/it, Loss=3.2424, Top1=59.88%, LR=0.061901]2025-11-05 21:26:20,663 - INFO - Step 105284: {'train_loss_batch': 2.714360237121582, 'train_lr': 0.06190057007615573, 'batch_time': 1.6775636198508799, 'data_time': 0.02180804897896686}
2025-11-05 21:26:20 Train Epoch 042:  12%|█▏        | 300/2502 [08:21<1:00:13,  1.64s/it, Loss=3.2515, Top1=N/A, LR=0.061901]   2025-11-05 21:29:05,404 - INFO - Step 105384: {'train_loss_batch': 4.0891618728637695, 'train_lr': 0.06190057007615573, 'batch_time': 1.6675459831656017, 'data_time': 0.014899803554496893}
2025-11-05 21:29:05 Train Epoch 042:  16%|█▌        | 400/2502 [11:06<58:10,  1.66s/it, Loss=3.2666, Top1=59.90%, LR=0.061901]2025-11-05 21:31:49,527 - INFO - Step 105484: {'train_loss_batch': 2.6343555450439453, 'train_lr': 0.06190057007615573, 'batch_time': 1.6609809392705523, 'data_time': 0.011432742834685747}
2025-11-05 21:31:49 Train Epoch 042:  20%|█▉        | 500/2502 [13:51<54:53,  1.64s/it, Loss=3.2651, Top1=N/A, LR=0.061901]   2025-11-05 21:34:34,876 - INFO - Step 105584: {'train_loss_batch': 2.705446481704712, 'train_lr': 0.06190057007615573, 'batch_time': 1.6594866821152008, 'data_time': 0.00935545129452399}
2025-11-05 21:34:34 Train Epoch 042:  24%|██▍       | 600/2502 [16:36<52:23,  1.65s/it, Loss=3.2870, Top1=59.62%, LR=0.061901]2025-11-05 21:37:19,995 - INFO - Step 105684: {'train_loss_batch': 2.6428494453430176, 'train_lr': 0.06190057007615573, 'batch_time': 1.6581046244070652, 'data_time': 0.007973188965173806}
2025-11-05 21:37:19 Train Epoch 042:  28%|██▊       | 700/2502 [19:22<49:51,  1.66s/it, Loss=3.3002, Top1=N/A, LR=0.061901]   2025-11-05 21:40:06,074 - INFO - Step 105784: {'train_loss_batch': 3.730428695678711, 'train_lr': 0.06190057007615573, 'batch_time': 1.6584881960750477, 'data_time': 0.006995836780346749}
2025-11-05 21:40:06 Train Epoch 042:  32%|███▏      | 800/2502 [22:08<47:03,  1.66s/it, Loss=3.2974, Top1=59.43%, LR=0.061901]2025-11-05 21:42:52,177 - INFO - Step 105884: {'train_loss_batch': 2.585925579071045, 'train_lr': 0.06190057007615573, 'batch_time': 1.6588048571802108, 'data_time': 0.006256204419368215}
2025-11-05 21:42:52 Train Epoch 042:  36%|███▌      | 900/2502 [24:53<44:06,  1.65s/it, Loss=3.3093, Top1=N/A, LR=0.061901]   2025-11-05 21:45:36,847 - INFO - Step 105984: {'train_loss_batch': 2.7483742237091064, 'train_lr': 0.06190057007615573, 'batch_time': 1.6574610414833129, 'data_time': 0.00566446794389223}
2025-11-05 21:45:36 Train Epoch 042:  40%|███▉      | 1000/2502 [27:37<41:07,  1.64s/it, Loss=3.3133, Top1=N/A, LR=0.061901]2025-11-05 21:48:21,185 - INFO - Step 106084: {'train_loss_batch': 2.700674057006836, 'train_lr': 0.06190057007615573, 'batch_time': 1.656054265253789, 'data_time': 0.005206399864250129}
2025-11-05 21:48:21 Train Epoch 042:  44%|████▍     | 1100/2502 [30:22<38:20,  1.64s/it, Loss=3.3092, Top1=N/A, LR=0.061901]2025-11-05 21:51:05,977 - INFO - Step 106184: {'train_loss_batch': 2.9052398204803467, 'train_lr': 0.06190057007615573, 'batch_time': 1.6553154374987942, 'data_time': 0.00482435663865979}
2025-11-05 21:51:05 Train Epoch 042:  48%|████▊     | 1200/2502 [33:07<35:58,  1.66s/it, Loss=3.3025, Top1=N/A, LR=0.061901]2025-11-05 21:53:51,230 - INFO - Step 106284: {'train_loss_batch': 4.360561847686768, 'train_lr': 0.06190057007615573, 'batch_time': 1.6550830498424596, 'data_time': 0.004512030715053028}
2025-11-05 21:53:51 Train Epoch 042:  52%|█████▏    | 1300/2502 [35:53<33:07,  1.65s/it, Loss=3.3031, Top1=N/A, LR=0.061901]2025-11-05 21:56:37,108 - INFO - Step 106384: {'train_loss_batch': 2.6764919757843018, 'train_lr': 0.06190057007615573, 'batch_time': 1.6553667896440816, 'data_time': 0.004245735149398205}
2025-11-05 21:56:37 Train Epoch 042:  56%|█████▌    | 1400/2502 [38:38<30:12,  1.64s/it, Loss=3.3061, Top1=N/A, LR=0.061901]2025-11-05 21:59:22,341 - INFO - Step 106484: {'train_loss_batch': 2.8156328201293945, 'train_lr': 0.06190057007615573, 'batch_time': 1.6551501325502471, 'data_time': 0.004013341805664324}
2025-11-05 21:59:22 Train Epoch 042:  60%|█████▉    | 1500/2502 [41:24<27:40,  1.66s/it, Loss=3.3100, Top1=N/A, LR=0.061901]2025-11-05 22:02:07,977 - INFO - Step 106584: {'train_loss_batch': 2.6463096141815186, 'train_lr': 0.06190057007615573, 'batch_time': 1.6552305239029999, 'data_time': 0.0038154547727560694}
2025-11-05 22:02:07 Train Epoch 042:  64%|██████▍   | 1600/2502 [44:09<24:58,  1.66s/it, Loss=3.3120, Top1=N/A, LR=0.061901]2025-11-05 22:04:53,405 - INFO - Step 106684: {'train_loss_batch': 2.5244977474212646, 'train_lr': 0.06190057007615573, 'batch_time': 1.6551707492330385, 'data_time': 0.003644034089631099}
2025-11-05 22:04:53 Train Epoch 042:  68%|██████▊   | 1700/2502 [46:55<22:07,  1.66s/it, Loss=3.3130, Top1=N/A, LR=0.061901]2025-11-05 22:07:38,706 - INFO - Step 106784: {'train_loss_batch': 2.622162342071533, 'train_lr': 0.06190057007615573, 'batch_time': 1.6550435494283309, 'data_time': 0.00349301930247581}
2025-11-05 22:07:38 Train Epoch 042:  72%|███████▏  | 1800/2502 [49:40<19:21,  1.65s/it, Loss=3.3077, Top1=N/A, LR=0.061901]2025-11-05 22:10:23,868 - INFO - Step 106884: {'train_loss_batch': 2.536166191101074, 'train_lr': 0.06190057007615573, 'batch_time': 1.6548536455280445, 'data_time': 0.0033609064865747734}
2025-11-05 22:10:23 Train Epoch 042:  76%|███████▌  | 1900/2502 [52:25<16:32,  1.65s/it, Loss=3.3130, Top1=59.27%, LR=0.061901]2025-11-05 22:13:08,966 - INFO - Step 106984: {'train_loss_batch': 2.7131481170654297, 'train_lr': 0.06190057007615573, 'batch_time': 1.65464972170449, 'data_time': 0.003240653554243141}
2025-11-05 22:13:08 Train Epoch 042:  80%|███████▉  | 2000/2502 [55:11<13:54,  1.66s/it, Loss=3.3159, Top1=N/A, LR=0.061901]   2025-11-05 22:15:54,480 - INFO - Step 107084: {'train_loss_batch': 4.082459449768066, 'train_lr': 0.06190057007615573, 'batch_time': 1.65467428231704, 'data_time': 0.0031313089535630746}
2025-11-05 22:15:54 Train Epoch 042:  84%|████████▍ | 2100/2502 [57:56<11:07,  1.66s/it, Loss=3.3226, Top1=N/A, LR=0.061901]2025-11-05 22:18:39,579 - INFO - Step 107184: {'train_loss_batch': 3.4317641258239746, 'train_lr': 0.06190057007615573, 'batch_time': 1.6544985661104938, 'data_time': 0.003031577228762206}
2025-11-05 22:18:39 Train Epoch 042:  88%|████████▊ | 2200/2502 [1:00:40<08:11,  1.63s/it, Loss=3.3254, Top1=N/A, LR=0.061901]2025-11-05 22:21:24,086 - INFO - Step 107284: {'train_loss_batch': 4.2689924240112305, 'train_lr': 0.06190057007615573, 'batch_time': 1.65407003591625, 'data_time': 0.0029409850309892764}
2025-11-05 22:21:24 Train Epoch 042:  92%|█████████▏| 2300/2502 [1:03:24<05:34,  1.66s/it, Loss=3.3321, Top1=N/A, LR=0.061901]2025-11-05 22:24:07,998 - INFO - Step 107384: {'train_loss_batch': 4.864245414733887, 'train_lr': 0.06190057007615573, 'batch_time': 1.6534202360992896, 'data_time': 0.0028575668641661937}
2025-11-05 22:24:08 Train Epoch 042:  96%|█████████▌| 2400/2502 [1:06:09<02:48,  1.65s/it, Loss=3.3293, Top1=N/A, LR=0.061901]2025-11-05 22:26:53,038 - INFO - Step 107484: {'train_loss_batch': 2.55572247505188, 'train_lr': 0.06190057007615573, 'batch_time': 1.6532943038233416, 'data_time': 0.002780662581901757}
2025-11-05 22:26:53 Train Epoch 042: 100%|█████████▉| 2500/2502 [1:08:54<00:03,  1.66s/it, Loss=3.3345, Top1=59.20%, LR=0.061901]2025-11-05 22:29:38,139 - INFO - Step 107584: {'train_loss_batch': 2.6465024948120117, 'train_lr': 0.06190057007615573, 'batch_time': 1.653202936202228, 'data_time': 0.002741760656577213}
2025-11-05 22:29:38 Train Epoch 042: 100%|██████████| 2502/2502 [1:08:56<00:00,  1.65s/it, Loss=3.3345, Top1=59.20%, LR=0.061901]
2025-11-05 22:29:40 Val Epoch 042:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 22:29:44   with torch.cuda.amp.autocast():
2025-11-05 22:29:45 Val Epoch 042: 100%|██████████| 98/98 [01:55<00:00,  1.18s/it, Loss=2.4097, Top1=64.01%, Top5=86.26%]
2025-11-05 22:31:36 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 22:31:36   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 22:31:36 2025-11-05 22:31:36,453 - INFO - Step 42: {'epoch': 42, 'learning_rate': 0.06009811104580473, 'train_loss': 3.3342747433865956, 'train_top1': 59.2035950466805, 'train_top5': 81.38169411307054, 'train_precision': 58.96969397697808, 'train_recall': 59.1012987051265, 'train_f1': 58.8216008737604, 'val_loss': 2.409653882751465, 'val_top1': 64.00600001953126, 'val_top5': 86.26199998779298, 'val_precision': 66.26939575309862, 'val_recall': 64.00200000000001, 'val_f1': 63.33017638501565}
2025-11-05 22:31:36 2025-11-05 22:31:36,454 - INFO - Epoch 042 Summary - LR: 0.060098, Train Loss: 3.3343, Val Loss: 2.4097, Val F1: 63.33%, Val Precision: 66.27%, Val Recall: 64.00%
2025-11-05 22:31:36 Train Epoch 043:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 42 that is less than the current step 107584. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 22:31:40 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 22:31:40   with torch.cuda.amp.autocast():
2025-11-05 22:31:42 Train Epoch 043:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.3326, Top1=N/A, LR=0.060098]2025-11-05 22:31:42,533 - INFO - Step 107586: {'train_loss_batch': 4.332602024078369, 'train_lr': 0.06009811104580473, 'batch_time': 5.654726266860962, 'data_time': 3.9981131553649902}
2025-11-05 22:31:42 Train Epoch 043:   4%|▍         | 100/2502 [02:51<1:06:36,  1.66s/it, Loss=3.1746, Top1=N/A, LR=0.060098]2025-11-05 22:34:28,431 - INFO - Step 107686: {'train_loss_batch': 4.107810974121094, 'train_lr': 0.06009811104580473, 'batch_time': 1.6985457014329364, 'data_time': 0.04066813582240945}
2025-11-05 22:34:28 Train Epoch 043:   8%|▊         | 200/2502 [05:37<1:03:38,  1.66s/it, Loss=3.2672, Top1=N/A, LR=0.060098]2025-11-05 22:37:13,976 - INFO - Step 107786: {'train_loss_batch': 3.4160571098327637, 'train_lr': 0.06009811104580473, 'batch_time': 1.6771029050077373, 'data_time': 0.02094987850284102}
2025-11-05 22:37:13 Train Epoch 043:  12%|█▏        | 300/2502 [08:22<1:00:46,  1.66s/it, Loss=3.2880, Top1=N/A, LR=0.060098]2025-11-05 22:39:59,006 - INFO - Step 107886: {'train_loss_batch': 3.3843345642089844, 'train_lr': 0.06009811104580473, 'batch_time': 1.6681997467117056, 'data_time': 0.014316770325467435}
2025-11-05 22:39:59 Train Epoch 043:  16%|█▌        | 400/2502 [11:07<58:05,  1.66s/it, Loss=3.2820, Top1=N/A, LR=0.060098]2025-11-05 22:42:44,871 - INFO - Step 107986: {'train_loss_batch': 4.8639421463012695, 'train_lr': 0.06009811104580473, 'batch_time': 1.6658157511542266, 'data_time': 0.010992661973187453}
2025-11-05 22:42:44 Train Epoch 043:  20%|█▉        | 500/2502 [13:53<54:34,  1.64s/it, Loss=3.2843, Top1=N/A, LR=0.060098]2025-11-05 22:45:30,168 - INFO - Step 108086: {'train_loss_batch': 4.479192733764648, 'train_lr': 0.06009811104580473, 'batch_time': 1.6632516807662752, 'data_time': 0.009001922702598952}
2025-11-05 22:45:30 Train Epoch 043:  24%|██▍       | 600/2502 [16:37<52:06,  1.64s/it, Loss=3.3044, Top1=N/A, LR=0.060098]2025-11-05 22:48:14,875 - INFO - Step 108186: {'train_loss_batch': 5.116301536560059, 'train_lr': 0.06009811104580473, 'batch_time': 1.6605592543590881, 'data_time': 0.007669265575694562}
2025-11-05 22:48:14 Train Epoch 043:  28%|██▊       | 700/2502 [19:23<49:59,  1.66s/it, Loss=3.3109, Top1=N/A, LR=0.060098]2025-11-05 22:51:00,470 - INFO - Step 108286: {'train_loss_batch': 2.561957597732544, 'train_lr': 0.06009811104580473, 'batch_time': 1.659901179532692, 'data_time': 0.006723420596156753}
2025-11-05 22:51:00 Train Epoch 043:  32%|███▏      | 800/2502 [22:06<46:25,  1.64s/it, Loss=3.3174, Top1=60.14%, LR=0.060098]2025-11-05 22:53:43,868 - INFO - Step 108386: {'train_loss_batch': 2.6701931953430176, 'train_lr': 0.06009811104580473, 'batch_time': 1.6566650676965415, 'data_time': 0.006003790878029203}
2025-11-05 22:53:43 Train Epoch 043:  36%|███▌      | 900/2502 [24:52<44:16,  1.66s/it, Loss=3.3085, Top1=N/A, LR=0.060098]   2025-11-05 22:56:29,417 - INFO - Step 108486: {'train_loss_batch': 3.844862461090088, 'train_lr': 0.06009811104580473, 'batch_time': 1.6565339223923086, 'data_time': 0.005444925183329016}
2025-11-05 22:56:29 Train Epoch 043:  40%|███▉      | 1000/2502 [27:37<41:34,  1.66s/it, Loss=3.3048, Top1=N/A, LR=0.060098]2025-11-05 22:59:14,736 - INFO - Step 108586: {'train_loss_batch': 2.9027202129364014, 'train_lr': 0.06009811104580473, 'batch_time': 1.6561993183075012, 'data_time': 0.005001681906121832}
2025-11-05 22:59:14 Train Epoch 043:  44%|████▍     | 1100/2502 [30:23<38:21,  1.64s/it, Loss=3.3088, Top1=N/A, LR=0.060098]2025-11-05 23:02:00,012 - INFO - Step 108686: {'train_loss_batch': 2.691246747970581, 'train_lr': 0.06009811104580473, 'batch_time': 1.6558875569855485, 'data_time': 0.004642776962197119}
2025-11-05 23:02:00 Train Epoch 043:  48%|████▊     | 1200/2502 [33:08<35:54,  1.66s/it, Loss=3.3193, Top1=N/A, LR=0.060098]2025-11-05 23:04:45,372 - INFO - Step 108786: {'train_loss_batch': 3.820714235305786, 'train_lr': 0.06009811104580473, 'batch_time': 1.655696904430977, 'data_time': 0.004342606423002397}
2025-11-05 23:04:45 Train Epoch 043:  52%|█████▏    | 1300/2502 [35:53<32:43,  1.63s/it, Loss=3.3230, Top1=N/A, LR=0.060098]2025-11-05 23:07:29,902 - INFO - Step 108886: {'train_loss_batch': 3.3130245208740234, 'train_lr': 0.06009811104580473, 'batch_time': 1.6548971176880494, 'data_time': 0.0040871149571834395}
2025-11-05 23:07:29 Train Epoch 043:  56%|█████▌    | 1400/2502 [38:38<30:24,  1.66s/it, Loss=3.3241, Top1=N/A, LR=0.060098]2025-11-05 23:10:15,019 - INFO - Step 108986: {'train_loss_batch': 2.539616584777832, 'train_lr': 0.06009811104580473, 'batch_time': 1.6546307985141053, 'data_time': 0.0038671580320762617}
2025-11-05 23:10:15 Train Epoch 043:  60%|█████▉    | 1500/2502 [41:23<27:33,  1.65s/it, Loss=3.3241, Top1=N/A, LR=0.060098]2025-11-05 23:13:00,694 - INFO - Step 109086: {'train_loss_batch': 3.372499465942383, 'train_lr': 0.06009811104580473, 'batch_time': 1.654772356936806, 'data_time': 0.0036772638062966974}
2025-11-05 23:13:00 Train Epoch 043:  64%|██████▍   | 1600/2502 [44:09<24:56,  1.66s/it, Loss=3.3275, Top1=N/A, LR=0.060098]2025-11-05 23:15:46,865 - INFO - Step 109186: {'train_loss_batch': 2.9176197052001953, 'train_lr': 0.06009811104580473, 'batch_time': 1.6552056375702495, 'data_time': 0.0035104979432276978}
2025-11-05 23:15:46 Train Epoch 043:  68%|██████▊   | 1700/2502 [46:55<22:09,  1.66s/it, Loss=3.3302, Top1=N/A, LR=0.060098]2025-11-05 23:18:32,789 - INFO - Step 109286: {'train_loss_batch': 2.5447611808776855, 'train_lr': 0.06009811104580473, 'batch_time': 1.6554424259537883, 'data_time': 0.003364271868123229}
2025-11-05 23:18:32 Train Epoch 043:  72%|███████▏  | 1800/2502 [49:41<19:22,  1.66s/it, Loss=3.3338, Top1=N/A, LR=0.060098]2025-11-05 23:21:18,476 - INFO - Step 109386: {'train_loss_batch': 2.659787654876709, 'train_lr': 0.06009811104580473, 'batch_time': 1.6555213430469264, 'data_time': 0.0032345359024903565}
2025-11-05 23:21:18 Train Epoch 043:  76%|███████▌  | 1900/2502 [52:26<16:25,  1.64s/it, Loss=3.3291, Top1=59.61%, LR=0.060098]2025-11-05 23:24:03,624 - INFO - Step 109486: {'train_loss_batch': 2.666924476623535, 'train_lr': 0.06009811104580473, 'batch_time': 1.6553086041777088, 'data_time': 0.003117526098027598}
2025-11-05 23:24:03 Train Epoch 043:  80%|███████▉  | 2000/2502 [55:12<13:52,  1.66s/it, Loss=3.3291, Top1=N/A, LR=0.060098]   2025-11-05 23:26:49,379 - INFO - Step 109586: {'train_loss_batch': 2.7121505737304688, 'train_lr': 0.06009811104580473, 'batch_time': 1.6554203383747428, 'data_time': 0.0030133120123592987}
2025-11-05 23:26:49 Train Epoch 043:  84%|████████▍ | 2100/2502 [57:58<11:07,  1.66s/it, Loss=3.3255, Top1=59.55%, LR=0.060098]2025-11-05 23:29:35,345 - INFO - Step 109686: {'train_loss_batch': 2.6685051918029785, 'train_lr': 0.06009811104580473, 'batch_time': 1.6556226197224126, 'data_time': 0.0029215186281127056}
2025-11-05 23:29:35 Train Epoch 043:  88%|████████▊ | 2200/2502 [1:00:43<08:16,  1.65s/it, Loss=3.3292, Top1=N/A, LR=0.060098]   2025-11-05 23:32:20,850 - INFO - Step 109786: {'train_loss_batch': 2.6406428813934326, 'train_lr': 0.06009811104580473, 'batch_time': 1.655596402166974, 'data_time': 0.0028362202676845}
2025-11-05 23:32:20 Train Epoch 043:  92%|█████████▏| 2300/2502 [1:03:29<05:34,  1.66s/it, Loss=3.3261, Top1=N/A, LR=0.060098]2025-11-05 23:35:06,184 - INFO - Step 109886: {'train_loss_batch': 4.314788818359375, 'train_lr': 0.06009811104580473, 'batch_time': 1.6554982441500548, 'data_time': 0.002756923864531237}
2025-11-05 23:35:06 Train Epoch 043:  96%|█████████▌| 2400/2502 [1:06:13<02:47,  1.64s/it, Loss=3.3283, Top1=N/A, LR=0.060098]2025-11-05 23:37:50,666 - INFO - Step 109986: {'train_loss_batch': 4.5622944831848145, 'train_lr': 0.06009811104580473, 'batch_time': 1.6550530472778866, 'data_time': 0.0026835845143335256}
2025-11-05 23:37:50 Train Epoch 043: 100%|█████████▉| 2500/2502 [1:08:58<00:03,  1.64s/it, Loss=3.3280, Top1=59.49%, LR=0.060098]2025-11-05 23:40:35,392 - INFO - Step 110086: {'train_loss_batch': 2.5456831455230713, 'train_lr': 0.06009811104580473, 'batch_time': 1.6547415362315767, 'data_time': 0.0026420166567772303}
2025-11-05 23:40:35 Train Epoch 043: 100%|██████████| 2502/2502 [1:09:00<00:00,  1.65s/it, Loss=3.3280, Top1=59.49%, LR=0.060098]
2025-11-05 23:40:37 Val Epoch 043:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 23:40:42   with torch.cuda.amp.autocast():
2025-11-05 23:40:42 Val Epoch 043: 100%|██████████| 98/98 [01:54<00:00,  1.17s/it, Loss=2.4060, Top1=63.11%, Top5=85.92%]
2025-11-05 23:42:32 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-05 23:42:32   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-05 23:42:32 2025-11-05 23:42:32,503 - INFO - Step 43: {'epoch': 43, 'learning_rate': 0.05828186603982559, 'train_loss': 3.3276603882261315, 'train_top1': 59.49497767857143, 'train_top5': 81.69005102040816, 'train_precision': 59.275073285883664, 'train_recall': 59.35551765016267, 'train_f1': 59.11017167295661, 'val_loss': 2.4059602268218994, 'val_top1': 63.111999973144535, 'val_top5': 85.92399998291016, 'val_precision': 66.09577580685564, 'val_recall': 63.10800000000001, 'val_f1': 62.6423763618144}
2025-11-05 23:42:32 2025-11-05 23:42:32,505 - INFO - Epoch 043 Summary - LR: 0.058282, Train Loss: 3.3277, Val Loss: 2.4060, Val F1: 62.64%, Val Precision: 66.10%, Val Recall: 63.11%
2025-11-05 23:42:33 Train Epoch 044:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-05 23:42:37   with torch.cuda.amp.autocast():
2025-11-05 23:42:39 Train Epoch 044:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.5999, Top1=N/A, LR=0.058282]2025-11-05 23:42:39,038 - INFO - Step 110088: {'train_loss_batch': 2.599932909011841, 'train_lr': 0.05828186603982559, 'batch_time': 5.801514148712158, 'data_time': 4.148518085479736}
2025-11-05 23:42:39 Train Epoch 044:   0%|          | 1/2502 [00:05<4:01:59,  5.81s/it, Loss=2.5999, Top1=N/A, LR=0.058282]wandb: WARNING Tried to log to step 43 that is less than the current step 110086. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-05 23:42:40 Train Epoch 044:   4%|▍         | 100/2502 [02:51<1:06:08,  1.65s/it, Loss=3.2955, Top1=N/A, LR=0.058282]2025-11-05 23:45:24,652 - INFO - Step 110188: {'train_loss_batch': 2.931469678878784, 'train_lr': 0.05828186603982559, 'batch_time': 1.6971881082742521, 'data_time': 0.04212936552444307}
2025-11-05 23:45:24 Train Epoch 044:   8%|▊         | 200/2502 [05:37<1:03:25,  1.65s/it, Loss=3.3589, Top1=N/A, LR=0.058282]2025-11-05 23:48:10,556 - INFO - Step 110288: {'train_loss_batch': 2.9639432430267334, 'train_lr': 0.05828186603982559, 'batch_time': 1.6782055731436507, 'data_time': 0.021675581955791112}
2025-11-05 23:48:10 Train Epoch 044:  12%|█▏        | 300/2502 [08:23<1:00:59,  1.66s/it, Loss=3.2852, Top1=59.99%, LR=0.058282]2025-11-05 23:50:56,393 - INFO - Step 110388: {'train_loss_batch': 2.6117100715637207, 'train_lr': 0.05828186603982559, 'batch_time': 1.6716133019456831, 'data_time': 0.014804352953584487}
2025-11-05 23:50:56 Train Epoch 044:  16%|█▌        | 400/2502 [11:08<58:00,  1.66s/it, Loss=3.3409, Top1=N/A, LR=0.058282]   2025-11-05 23:53:42,049 - INFO - Step 110488: {'train_loss_batch': 3.226928949356079, 'train_lr': 0.05828186603982559, 'batch_time': 1.667862071063453, 'data_time': 0.011371901505011276}
2025-11-05 23:53:42 Train Epoch 044:  20%|█▉        | 500/2502 [13:54<54:42,  1.64s/it, Loss=3.3321, Top1=N/A, LR=0.058282]2025-11-05 23:56:27,667 - INFO - Step 110588: {'train_loss_batch': 2.6580095291137695, 'train_lr': 0.05828186603982559, 'batch_time': 1.6655290374260938, 'data_time': 0.009302710820576864}
2025-11-05 23:56:27 Train Epoch 044:  24%|██▍       | 600/2502 [16:38<52:23,  1.65s/it, Loss=3.3265, Top1=60.09%, LR=0.058282]2025-11-05 23:59:11,780 - INFO - Step 110688: {'train_loss_batch': 2.6376900672912598, 'train_lr': 0.05828186603982559, 'batch_time': 1.6614684281055623, 'data_time': 0.007928670146897708}
2025-11-05 23:59:11 Train Epoch 044:  28%|██▊       | 700/2502 [19:24<49:30,  1.65s/it, Loss=3.3413, Top1=N/A, LR=0.058282]   2025-11-06 00:01:57,494 - INFO - Step 110788: {'train_loss_batch': 4.372244834899902, 'train_lr': 0.05828186603982559, 'batch_time': 1.6608498671255507, 'data_time': 0.006936175336851373}
2025-11-06 00:01:57 Train Epoch 044:  32%|███▏      | 800/2502 [22:08<47:06,  1.66s/it, Loss=3.3347, Top1=N/A, LR=0.058282]2025-11-06 00:04:42,170 - INFO - Step 110888: {'train_loss_batch': 3.9745986461639404, 'train_lr': 0.05828186603982559, 'batch_time': 1.659090697839763, 'data_time': 0.0061947400501456}
2025-11-06 00:04:42 Train Epoch 044:  36%|███▌      | 900/2502 [24:54<44:20,  1.66s/it, Loss=3.3162, Top1=N/A, LR=0.058282]2025-11-06 00:07:28,078 - INFO - Step 110988: {'train_loss_batch': 2.757417678833008, 'train_lr': 0.05828186603982559, 'batch_time': 1.659090063283499, 'data_time': 0.005624122545536562}
2025-11-06 00:07:28 Train Epoch 044:  40%|███▉      | 1000/2502 [27:40<41:32,  1.66s/it, Loss=3.3158, Top1=N/A, LR=0.058282]2025-11-06 00:10:13,975 - INFO - Step 111088: {'train_loss_batch': 3.592057228088379, 'train_lr': 0.05828186603982559, 'batch_time': 1.6590778487069267, 'data_time': 0.005163937300949783}
2025-11-06 00:10:13 Train Epoch 044:  44%|████▍     | 1100/2502 [30:26<38:29,  1.65s/it, Loss=3.3095, Top1=N/A, LR=0.058282]2025-11-06 00:12:59,755 - INFO - Step 111188: {'train_loss_batch': 2.6510393619537354, 'train_lr': 0.05828186603982559, 'batch_time': 1.6589611479631887, 'data_time': 0.004784575166970789}
2025-11-06 00:12:59 Train Epoch 044:  48%|████▊     | 1200/2502 [33:12<36:05,  1.66s/it, Loss=3.3218, Top1=N/A, LR=0.058282]2025-11-06 00:15:45,491 - INFO - Step 111288: {'train_loss_batch': 3.9712014198303223, 'train_lr': 0.05828186603982559, 'batch_time': 1.6588281495287258, 'data_time': 0.004468795560380998}
2025-11-06 00:15:45 Train Epoch 044:  52%|█████▏    | 1300/2502 [35:57<33:16,  1.66s/it, Loss=3.3208, Top1=N/A, LR=0.058282]2025-11-06 00:18:30,582 - INFO - Step 111388: {'train_loss_batch': 2.7452585697174072, 'train_lr': 0.05828186603982559, 'batch_time': 1.6582193008117911, 'data_time': 0.004202384567553955}
2025-11-06 00:18:30 Train Epoch 044:  56%|█████▌    | 1400/2502 [38:41<30:09,  1.64s/it, Loss=3.3195, Top1=N/A, LR=0.058282]2025-11-06 00:21:15,075 - INFO - Step 111488: {'train_loss_batch': 4.0122389793396, 'train_lr': 0.05828186603982559, 'batch_time': 1.6572704687874118, 'data_time': 0.003973536113600149}
2025-11-06 00:21:15 Train Epoch 044:  60%|█████▉    | 1500/2502 [41:27<27:46,  1.66s/it, Loss=3.3132, Top1=N/A, LR=0.058282]2025-11-06 00:24:00,570 - INFO - Step 111588: {'train_loss_batch': 3.3779118061065674, 'train_lr': 0.05828186603982559, 'batch_time': 1.6571156004919043, 'data_time': 0.0037768525651579775}
2025-11-06 00:24:00 Train Epoch 044:  64%|██████▍   | 1600/2502 [44:12<24:54,  1.66s/it, Loss=3.3154, Top1=N/A, LR=0.058282]2025-11-06 00:26:46,222 - INFO - Step 111688: {'train_loss_batch': 4.25177001953125, 'train_lr': 0.05828186603982559, 'batch_time': 1.6570782269782234, 'data_time': 0.0036055286700542384}
2025-11-06 00:26:46 Train Epoch 044:  68%|██████▊   | 1700/2502 [46:58<22:03,  1.65s/it, Loss=3.3160, Top1=N/A, LR=0.058282]2025-11-06 00:29:31,529 - INFO - Step 111788: {'train_loss_batch': 2.913632392883301, 'train_lr': 0.05828186603982559, 'batch_time': 1.656842585103362, 'data_time': 0.0034537042049293584}
2025-11-06 00:29:31 Train Epoch 044:  72%|███████▏  | 1800/2502 [49:43<19:16,  1.65s/it, Loss=3.3183, Top1=59.76%, LR=0.058282]2025-11-06 00:32:16,404 - INFO - Step 111888: {'train_loss_batch': 2.580610990524292, 'train_lr': 0.05828186603982559, 'batch_time': 1.6563932796374485, 'data_time': 0.0033174624117396395}
2025-11-06 00:32:16 Train Epoch 044:  76%|███████▌  | 1900/2502 [52:27<16:28,  1.64s/it, Loss=3.3181, Top1=N/A, LR=0.058282]   2025-11-06 00:35:01,156 - INFO - Step 111988: {'train_loss_batch': 4.804800987243652, 'train_lr': 0.05828186603982559, 'batch_time': 1.6559261911985186, 'data_time': 0.0031951251373612084}
2025-11-06 00:35:01 Train Epoch 044:  80%|███████▉  | 2000/2502 [55:13<13:51,  1.66s/it, Loss=3.3131, Top1=59.77%, LR=0.058282]2025-11-06 00:37:46,745 - INFO - Step 112088: {'train_loss_batch': 2.6486785411834717, 'train_lr': 0.05828186603982559, 'batch_time': 1.6559241715221034, 'data_time': 0.0030885850829163054}
2025-11-06 00:37:46 Train Epoch 044:  84%|████████▍ | 2100/2502 [57:59<11:05,  1.65s/it, Loss=3.3083, Top1=N/A, LR=0.058282]   2025-11-06 00:40:32,619 - INFO - Step 112188: {'train_loss_batch': 2.809948205947876, 'train_lr': 0.05828186603982559, 'batch_time': 1.6560581284213896, 'data_time': 0.002989767052797066}
2025-11-06 00:40:32 Train Epoch 044:  88%|████████▊ | 2200/2502 [1:00:45<08:21,  1.66s/it, Loss=3.3110, Top1=59.71%, LR=0.058282]2025-11-06 00:43:18,426 - INFO - Step 112288: {'train_loss_batch': 2.6937239170074463, 'train_lr': 0.05828186603982559, 'batch_time': 1.6561495070563614, 'data_time': 0.0029015866045191416}
2025-11-06 00:43:18 Train Epoch 044:  92%|█████████▏| 2300/2502 [1:03:31<05:35,  1.66s/it, Loss=3.3181, Top1=N/A, LR=0.058282]   2025-11-06 00:46:04,244 - INFO - Step 112388: {'train_loss_batch': 3.7157998085021973, 'train_lr': 0.05828186603982559, 'batch_time': 1.6562376345618297, 'data_time': 0.0028225047232948246}
2025-11-06 00:46:04 Train Epoch 044:  96%|█████████▌| 2400/2502 [1:06:16<02:48,  1.66s/it, Loss=3.3146, Top1=N/A, LR=0.058282]2025-11-06 00:48:49,715 - INFO - Step 112488: {'train_loss_batch': 3.7303221225738525, 'train_lr': 0.05828186603982559, 'batch_time': 1.656173838222588, 'data_time': 0.0027483589993770397}
2025-11-06 00:48:49 Train Epoch 044: 100%|█████████▉| 2500/2502 [1:09:01<00:03,  1.66s/it, Loss=3.3104, Top1=N/A, LR=0.058282]2025-11-06 00:51:35,183 - INFO - Step 112588: {'train_loss_batch': 2.991243362426758, 'train_lr': 0.05828186603982559, 'batch_time': 1.6561140586070564, 'data_time': 0.002700497655094456}
2025-11-06 00:51:35 Train Epoch 044: 100%|██████████| 2502/2502 [1:09:03<00:00,  1.66s/it, Loss=3.3104, Top1=N/A, LR=0.058282]
2025-11-06 00:51:37 Val Epoch 044:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 00:51:41   with torch.cuda.amp.autocast():
2025-11-06 00:51:42 Val Epoch 044: 100%|██████████| 98/98 [01:52<00:00,  1.15s/it, Loss=2.3796, Top1=64.13%, Top5=86.25%]
2025-11-06 00:53:30 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 00:53:30   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 00:53:30 2025-11-06 00:53:30,248 - INFO - Step 44: {'epoch': 44, 'learning_rate': 0.05645431583042753, 'train_loss': 3.310184887082552, 'train_top1': 59.70695554748062, 'train_top5': 81.75758539244185, 'train_precision': 59.48992636506974, 'train_recall': 59.59418270704132, 'train_f1': 59.33776826510629, 'val_loss': 2.3795962186431883, 'val_top1': 64.12599998291016, 'val_top5': 86.25199999267578, 'val_precision': 66.62772129965717, 'val_recall': 64.128, 'val_f1': 63.68421084628322}
2025-11-06 00:53:30 2025-11-06 00:53:30,249 - INFO - Epoch 044 Summary - LR: 0.056454, Train Loss: 3.3102, Val Loss: 2.3796, Val F1: 63.68%, Val Precision: 66.63%, Val Recall: 64.13%
2025-11-06 00:53:30 wandb: WARNING Tried to log to step 44 that is less than the current step 112588. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 00:53:30 Train Epoch 045:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 00:53:34   with torch.cuda.amp.autocast():
2025-11-06 00:53:35 Train Epoch 045:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.6162, Top1=N/A, LR=0.056454]2025-11-06 00:53:35,930 - INFO - Step 112590: {'train_loss_batch': 2.6161863803863525, 'train_lr': 0.05645431583042753, 'batch_time': 5.25646185874939, 'data_time': 3.6101996898651123}
2025-11-06 00:53:35 Train Epoch 045:   4%|▍         | 100/2502 [02:50<1:06:18,  1.66s/it, Loss=3.1275, Top1=N/A, LR=0.056454]2025-11-06 00:56:21,578 - INFO - Step 112690: {'train_loss_batch': 2.706174612045288, 'train_lr': 0.05645431583042753, 'batch_time': 1.692133853931238, 'data_time': 0.03674736117372419}
2025-11-06 00:56:21 Train Epoch 045:   8%|▊         | 200/2502 [05:35<1:02:54,  1.64s/it, Loss=3.2330, Top1=59.96%, LR=0.056454]2025-11-06 00:59:06,504 - INFO - Step 112790: {'train_loss_batch': 2.705853223800659, 'train_lr': 0.05645431583042753, 'batch_time': 1.6708047069720369, 'data_time': 0.018987876265796264}
2025-11-06 00:59:06 Train Epoch 045:  12%|█▏        | 300/2502 [08:20<1:00:50,  1.66s/it, Loss=3.2403, Top1=N/A, LR=0.056454]   2025-11-06 01:01:51,568 - INFO - Step 112890: {'train_loss_batch': 4.168609619140625, 'train_lr': 0.05645431583042753, 'batch_time': 1.6641023610517431, 'data_time': 0.013018513834753703}
2025-11-06 01:01:51 Train Epoch 045:  16%|█▌        | 400/2502 [11:06<58:01,  1.66s/it, Loss=3.2683, Top1=N/A, LR=0.056454]2025-11-06 01:04:36,754 - INFO - Step 112990: {'train_loss_batch': 2.6361992359161377, 'train_lr': 0.05645431583042753, 'batch_time': 1.6610491026072134, 'data_time': 0.01001008728198577}
2025-11-06 01:04:36 Train Epoch 045:  20%|█▉        | 500/2502 [13:51<55:02,  1.65s/it, Loss=3.2773, Top1=N/A, LR=0.056454]2025-11-06 01:07:22,163 - INFO - Step 113090: {'train_loss_batch': 4.112020492553711, 'train_lr': 0.05645431583042753, 'batch_time': 1.6596600347888208, 'data_time': 0.00821414345990636}
2025-11-06 01:07:22 Train Epoch 045:  24%|██▍       | 600/2502 [16:36<52:41,  1.66s/it, Loss=3.2568, Top1=N/A, LR=0.056454]2025-11-06 01:10:07,548 - INFO - Step 113190: {'train_loss_batch': 5.0447096824646, 'train_lr': 0.05645431583042753, 'batch_time': 1.6586934984622899, 'data_time': 0.007011168411686496}
2025-11-06 01:10:07 Train Epoch 045:  28%|██▊       | 700/2502 [19:22<49:53,  1.66s/it, Loss=3.2729, Top1=N/A, LR=0.056454]2025-11-06 01:12:53,145 - INFO - Step 113290: {'train_loss_batch': 2.514655351638794, 'train_lr': 0.05645431583042753, 'batch_time': 1.6583044426927553, 'data_time': 0.006154038596595405}
2025-11-06 01:12:53 Train Epoch 045:  32%|███▏      | 800/2502 [22:07<46:52,  1.65s/it, Loss=3.2828, Top1=N/A, LR=0.056454]2025-11-06 01:15:38,229 - INFO - Step 113390: {'train_loss_batch': 3.920626163482666, 'train_lr': 0.05645431583042753, 'batch_time': 1.657372165410855, 'data_time': 0.005512371194198932}
2025-11-06 01:15:38 Train Epoch 045:  36%|███▌      | 900/2502 [24:53<44:16,  1.66s/it, Loss=3.2757, Top1=60.24%, LR=0.056454]2025-11-06 01:18:24,042 - INFO - Step 113490: {'train_loss_batch': 2.5149998664855957, 'train_lr': 0.05645431583042753, 'batch_time': 1.6574554215790032, 'data_time': 0.005013470644426928}
2025-11-06 01:18:24 Train Epoch 045:  40%|███▉      | 1000/2502 [27:38<41:09,  1.64s/it, Loss=3.2754, Top1=N/A, LR=0.056454]   2025-11-06 01:21:08,797 - INFO - Step 113590: {'train_loss_batch': 2.520986318588257, 'train_lr': 0.05645431583042753, 'batch_time': 1.6564664597754235, 'data_time': 0.004615735578965712}
2025-11-06 01:21:08 Train Epoch 045:  44%|████▍     | 1100/2502 [30:23<38:29,  1.65s/it, Loss=3.2780, Top1=60.18%, LR=0.056454]2025-11-06 01:23:54,085 - INFO - Step 113690: {'train_loss_batch': 2.626467227935791, 'train_lr': 0.05645431583042753, 'batch_time': 1.6561403096967346, 'data_time': 0.004287570525471672}
2025-11-06 01:23:54 Train Epoch 045:  48%|████▊     | 1200/2502 [33:08<35:37,  1.64s/it, Loss=3.2824, Top1=N/A, LR=0.056454]   2025-11-06 01:26:38,783 - INFO - Step 113790: {'train_loss_batch': 4.062000274658203, 'train_lr': 0.05645431583042753, 'batch_time': 1.6553769699242789, 'data_time': 0.004016492686402688}
2025-11-06 01:26:38 Train Epoch 045:  52%|█████▏    | 1300/2502 [35:53<33:13,  1.66s/it, Loss=3.2819, Top1=N/A, LR=0.056454]2025-11-06 01:29:24,160 - INFO - Step 113890: {'train_loss_batch': 3.529956102371216, 'train_lr': 0.05645431583042753, 'batch_time': 1.6552533137624215, 'data_time': 0.003785237269068021}
2025-11-06 01:29:24 Train Epoch 045:  56%|█████▌    | 1400/2502 [38:38<30:27,  1.66s/it, Loss=3.2806, Top1=N/A, LR=0.056454]2025-11-06 01:32:09,428 - INFO - Step 113990: {'train_loss_batch': 4.190556526184082, 'train_lr': 0.05645431583042753, 'batch_time': 1.6550697825960736, 'data_time': 0.0035872855925032448}
2025-11-06 01:32:09 Train Epoch 045:  60%|█████▉    | 1500/2502 [41:24<27:44,  1.66s/it, Loss=3.2807, Top1=N/A, LR=0.056454]2025-11-06 01:34:55,010 - INFO - Step 114090: {'train_loss_batch': 4.306851387023926, 'train_lr': 0.05645431583042753, 'batch_time': 1.6551195593534669, 'data_time': 0.0034191184326619168}
2025-11-06 01:34:55 Train Epoch 045:  64%|██████▍   | 1600/2502 [44:10<24:56,  1.66s/it, Loss=3.2867, Top1=N/A, LR=0.056454]2025-11-06 01:37:40,919 - INFO - Step 114190: {'train_loss_batch': 4.34182071685791, 'train_lr': 0.05645431583042753, 'batch_time': 1.6553674027146883, 'data_time': 0.0032718275428786268}
2025-11-06 01:37:40 Train Epoch 045:  68%|██████▊   | 1700/2502 [46:55<22:11,  1.66s/it, Loss=3.2880, Top1=N/A, LR=0.056454]2025-11-06 01:40:26,447 - INFO - Step 114290: {'train_loss_batch': 2.4729084968566895, 'train_lr': 0.05645431583042753, 'batch_time': 1.6553620295549827, 'data_time': 0.0031388241848056978}
2025-11-06 01:40:26 Train Epoch 045:  72%|███████▏  | 1800/2502 [49:41<19:11,  1.64s/it, Loss=3.2890, Top1=N/A, LR=0.056454]2025-11-06 01:43:11,932 - INFO - Step 114390: {'train_loss_batch': 2.7707488536834717, 'train_lr': 0.05645431583042753, 'batch_time': 1.655333818693018, 'data_time': 0.0030206243969347}
2025-11-06 01:43:11 Train Epoch 045:  76%|███████▌  | 1900/2502 [52:25<16:38,  1.66s/it, Loss=3.2966, Top1=N/A, LR=0.056454]2025-11-06 01:45:56,261 - INFO - Step 114490: {'train_loss_batch': 3.300658941268921, 'train_lr': 0.05645431583042753, 'batch_time': 1.6547000943705135, 'data_time': 0.0029139667984562634}
2025-11-06 01:45:56 Train Epoch 045:  80%|███████▉  | 2000/2502 [55:11<13:48,  1.65s/it, Loss=3.3027, Top1=60.07%, LR=0.056454]2025-11-06 01:48:42,138 - INFO - Step 114590: {'train_loss_batch': 2.713107109069824, 'train_lr': 0.05645431583042753, 'batch_time': 1.6549033365626147, 'data_time': 0.002821071334983753}
2025-11-06 01:48:42 Train Epoch 045:  84%|████████▍ | 2100/2502 [57:56<11:08,  1.66s/it, Loss=3.3033, Top1=N/A, LR=0.056454]   2025-11-06 01:51:27,433 - INFO - Step 114690: {'train_loss_batch': 4.020378112792969, 'train_lr': 0.05645431583042753, 'batch_time': 1.6548101282414795, 'data_time': 0.00273313050267811}
2025-11-06 01:51:27 Train Epoch 045:  88%|████████▊ | 2200/2502 [1:00:41<08:21,  1.66s/it, Loss=3.2967, Top1=N/A, LR=0.056454]2025-11-06 01:54:12,581 - INFO - Step 114790: {'train_loss_batch': 2.8648579120635986, 'train_lr': 0.05645431583042753, 'batch_time': 1.6546589347894818, 'data_time': 0.0026553335757431035}
2025-11-06 01:54:12 Train Epoch 045:  92%|█████████▏| 2300/2502 [1:03:26<05:34,  1.65s/it, Loss=3.2990, Top1=N/A, LR=0.056454]2025-11-06 01:56:57,616 - INFO - Step 114890: {'train_loss_batch': 2.674553394317627, 'train_lr': 0.05645431583042753, 'batch_time': 1.6544714228892419, 'data_time': 0.002585566598195089}
2025-11-06 01:56:57 Train Epoch 045:  96%|█████████▌| 2400/2502 [1:06:12<02:48,  1.65s/it, Loss=3.3008, Top1=N/A, LR=0.056454]2025-11-06 01:59:43,394 - INFO - Step 114990: {'train_loss_batch': 3.5035886764526367, 'train_lr': 0.05645431583042753, 'batch_time': 1.6546090828880873, 'data_time': 0.002521294050045879}
2025-11-06 01:59:43 Train Epoch 045: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.66s/it, Loss=3.3012, Top1=N/A, LR=0.056454]2025-11-06 02:02:27,649 - INFO - Step 115090: {'train_loss_batch': 2.6860909461975098, 'train_lr': 0.05645431583042753, 'batch_time': 1.6541268390829398, 'data_time': 0.002483935224585703}
2025-11-06 02:02:27 Train Epoch 045: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=3.3012, Top1=N/A, LR=0.056454]
2025-11-06 02:02:29 Val Epoch 045:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 02:02:34   with torch.cuda.amp.autocast():
2025-11-06 02:02:34 Val Epoch 045: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=2.3885, Top1=63.56%, Top5=86.19%]
2025-11-06 02:04:20 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 02:04:20   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 02:04:20 2025-11-06 02:04:20,495 - INFO - Step 45: {'epoch': 45, 'learning_rate': 0.054617956631367794, 'train_loss': 3.301126488583456, 'train_top1': 60.01061893203884, 'train_top5': 82.02783677184466, 'train_precision': 59.79127217810844, 'train_recall': 59.87610345897736, 'train_f1': 59.62748661834466, 'val_loss': 2.38849863861084, 'val_top1': 63.563999975585936, 'val_top5': 86.188, 'val_precision': 65.99448348272587, 'val_recall': 63.56400000000001, 'val_f1': 62.98810168366549}
2025-11-06 02:04:20 2025-11-06 02:04:20,497 - INFO - Epoch 045 Summary - LR: 0.054618, Train Loss: 3.3011, Val Loss: 2.3885, Val F1: 62.99%, Val Precision: 65.99%, Val Recall: 63.56%
2025-11-06 02:04:20 wandb: WARNING Tried to log to step 45 that is less than the current step 115090. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 02:04:21 Train Epoch 046:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 02:04:25   with torch.cuda.amp.autocast():
2025-11-06 02:04:27 Train Epoch 046:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.2441, Top1=N/A, LR=0.054618]2025-11-06 02:04:27,207 - INFO - Step 115092: {'train_loss_batch': 4.244084358215332, 'train_lr': 0.054617956631367794, 'batch_time': 5.918447256088257, 'data_time': 4.285725116729736}
2025-11-06 02:04:27 Train Epoch 046:   4%|▍         | 100/2502 [02:51<1:06:19,  1.66s/it, Loss=3.3764, Top1=N/A, LR=0.054618]2025-11-06 02:07:12,792 - INFO - Step 115192: {'train_loss_batch': 2.532721996307373, 'train_lr': 0.054617956631367794, 'batch_time': 1.6980571062257974, 'data_time': 0.04346879638067567}
2025-11-06 02:07:12 Train Epoch 046:   8%|▊         | 200/2502 [05:37<1:03:38,  1.66s/it, Loss=3.3209, Top1=N/A, LR=0.054618]2025-11-06 02:09:58,657 - INFO - Step 115292: {'train_loss_batch': 3.220655918121338, 'train_lr': 0.054617956631367794, 'batch_time': 1.678449312845866, 'data_time': 0.02234341849142046}
2025-11-06 02:09:58 Train Epoch 046:  12%|█▏        | 300/2502 [08:21<1:00:23,  1.65s/it, Loss=3.3562, Top1=N/A, LR=0.054618]2025-11-06 02:12:42,715 - INFO - Step 115392: {'train_loss_batch': 3.958343505859375, 'train_lr': 0.054617956631367794, 'batch_time': 1.665866842301581, 'data_time': 0.015239698150229216}
2025-11-06 02:12:42 Train Epoch 046:  16%|█▌        | 400/2502 [11:06<57:48,  1.65s/it, Loss=3.3325, Top1=N/A, LR=0.054618]2025-11-06 02:15:28,244 - INFO - Step 115492: {'train_loss_batch': 2.6444592475891113, 'train_lr': 0.054617956631367794, 'batch_time': 1.6632296605003147, 'data_time': 0.011691350294764797}
2025-11-06 02:15:28 Train Epoch 046:  20%|█▉        | 500/2502 [13:52<55:19,  1.66s/it, Loss=3.3033, Top1=N/A, LR=0.054618]2025-11-06 02:18:13,692 - INFO - Step 115592: {'train_loss_batch': 3.166548252105713, 'train_lr': 0.054617956631367794, 'batch_time': 1.6614819757952661, 'data_time': 0.009550376804526932}
2025-11-06 02:18:13 Train Epoch 046:  24%|██▍       | 600/2502 [16:38<52:40,  1.66s/it, Loss=3.3100, Top1=N/A, LR=0.054618]2025-11-06 02:20:59,424 - INFO - Step 115692: {'train_loss_batch': 4.286520957946777, 'train_lr': 0.054617956631367794, 'batch_time': 1.6607882607598075, 'data_time': 0.008126481003848567}
2025-11-06 02:20:59 Train Epoch 046:  28%|██▊       | 700/2502 [19:24<49:54,  1.66s/it, Loss=3.3078, Top1=N/A, LR=0.054618]2025-11-06 02:23:45,356 - INFO - Step 115792: {'train_loss_batch': 5.080985069274902, 'train_lr': 0.054617956631367794, 'batch_time': 1.660578369244019, 'data_time': 0.0071157773109033345}
2025-11-06 02:23:45 Train Epoch 046:  32%|███▏      | 800/2502 [22:09<46:56,  1.65s/it, Loss=3.3025, Top1=N/A, LR=0.054618]2025-11-06 02:26:31,147 - INFO - Step 115892: {'train_loss_batch': 2.660824775695801, 'train_lr': 0.054617956631367794, 'batch_time': 1.6602451610803306, 'data_time': 0.006364881024973818}
2025-11-06 02:26:31 Train Epoch 046:  36%|███▌      | 900/2502 [24:54<44:09,  1.65s/it, Loss=3.2983, Top1=N/A, LR=0.054618]2025-11-06 02:29:15,866 - INFO - Step 115992: {'train_loss_batch': 2.6837399005889893, 'train_lr': 0.054617956631367794, 'batch_time': 1.6587961815570489, 'data_time': 0.0057749570937585355}
2025-11-06 02:29:15 Train Epoch 046:  40%|███▉      | 1000/2502 [27:39<41:37,  1.66s/it, Loss=3.2875, Top1=60.54%, LR=0.054618]2025-11-06 02:32:01,121 - INFO - Step 116092: {'train_loss_batch': 2.7271780967712402, 'train_lr': 0.054617956631367794, 'batch_time': 1.658172580031129, 'data_time': 0.005303114920586616}
2025-11-06 02:32:01 Train Epoch 046:  44%|████▍     | 1100/2502 [30:24<38:34,  1.65s/it, Loss=3.2763, Top1=N/A, LR=0.054618]   2025-11-06 02:34:46,267 - INFO - Step 116192: {'train_loss_batch': 4.147053241729736, 'train_lr': 0.054617956631367794, 'batch_time': 1.6575620523049115, 'data_time': 0.004916377331753626}
2025-11-06 02:34:46 Train Epoch 046:  48%|████▊     | 1200/2502 [33:10<35:43,  1.65s/it, Loss=3.2773, Top1=N/A, LR=0.054618]2025-11-06 02:37:31,912 - INFO - Step 116292: {'train_loss_batch': 4.233821868896484, 'train_lr': 0.054617956631367794, 'batch_time': 1.6574700422628437, 'data_time': 0.0045998666209046985}
2025-11-06 02:37:31 Train Epoch 046:  52%|█████▏    | 1300/2502 [35:56<33:15,  1.66s/it, Loss=3.2619, Top1=N/A, LR=0.054618]2025-11-06 02:40:17,300 - INFO - Step 116392: {'train_loss_batch': 3.9277172088623047, 'train_lr': 0.054617956631367794, 'batch_time': 1.6571934058242903, 'data_time': 0.004330467206528332}
2025-11-06 02:40:17 Train Epoch 046:  56%|█████▌    | 1400/2502 [38:41<30:16,  1.65s/it, Loss=3.2676, Top1=60.34%, LR=0.054618]2025-11-06 02:43:02,962 - INFO - Step 116492: {'train_loss_batch': 2.6855626106262207, 'train_lr': 0.054617956631367794, 'batch_time': 1.6571524800103874, 'data_time': 0.0041032011044357265}
2025-11-06 02:43:02 Train Epoch 046:  60%|█████▉    | 1500/2502 [41:27<27:41,  1.66s/it, Loss=3.2676, Top1=N/A, LR=0.054618]   2025-11-06 02:45:48,677 - INFO - Step 116592: {'train_loss_batch': 3.0464344024658203, 'train_lr': 0.054617956631367794, 'batch_time': 1.6571523631754754, 'data_time': 0.0038995224980018202}
2025-11-06 02:45:48 Train Epoch 046:  64%|██████▍   | 1600/2502 [44:13<24:57,  1.66s/it, Loss=3.2744, Top1=N/A, LR=0.054618]2025-11-06 02:48:34,347 - INFO - Step 116692: {'train_loss_batch': 2.783612012863159, 'train_lr': 0.054617956631367794, 'batch_time': 1.6571239889003126, 'data_time': 0.0037208720343623737}
2025-11-06 02:48:34 Train Epoch 046:  68%|██████▊   | 1700/2502 [46:58<21:57,  1.64s/it, Loss=3.2805, Top1=N/A, LR=0.054618]2025-11-06 02:51:19,623 - INFO - Step 116792: {'train_loss_batch': 2.562998056411743, 'train_lr': 0.054617956631367794, 'batch_time': 1.6568671050736092, 'data_time': 0.003563765425460609}
2025-11-06 02:51:19 Train Epoch 046:  72%|███████▏  | 1800/2502 [49:43<19:15,  1.65s/it, Loss=3.2801, Top1=N/A, LR=0.054618]2025-11-06 02:54:04,557 - INFO - Step 116892: {'train_loss_batch': 2.5926830768585205, 'train_lr': 0.054617956631367794, 'batch_time': 1.6564489614295006, 'data_time': 0.0034245753672174583}
2025-11-06 02:54:04 Train Epoch 046:  76%|███████▌  | 1900/2502 [52:28<16:36,  1.66s/it, Loss=3.2760, Top1=60.30%, LR=0.054618]2025-11-06 02:56:49,947 - INFO - Step 116992: {'train_loss_batch': 2.562922716140747, 'train_lr': 0.054617956631367794, 'batch_time': 1.6563146821703802, 'data_time': 0.003298078945346533}
2025-11-06 02:56:49 Train Epoch 046:  80%|███████▉  | 2000/2502 [55:14<13:53,  1.66s/it, Loss=3.2782, Top1=60.23%, LR=0.054618]2025-11-06 02:59:35,751 - INFO - Step 117092: {'train_loss_batch': 2.59182071685791, 'train_lr': 0.054617956631367794, 'batch_time': 1.6564007886107834, 'data_time': 0.0031876680792599305}
2025-11-06 02:59:35 Train Epoch 046:  84%|████████▍ | 2100/2502 [58:00<11:06,  1.66s/it, Loss=3.2828, Top1=N/A, LR=0.054618]   2025-11-06 03:02:21,648 - INFO - Step 117192: {'train_loss_batch': 2.539564371109009, 'train_lr': 0.054617956631367794, 'batch_time': 1.6565229498959904, 'data_time': 0.0030893204837455913}
2025-11-06 03:02:21 Train Epoch 046:  88%|████████▊ | 2200/2502 [1:00:46<08:20,  1.66s/it, Loss=3.2881, Top1=N/A, LR=0.054618]2025-11-06 03:05:07,606 - INFO - Step 117292: {'train_loss_batch': 4.124149799346924, 'train_lr': 0.054617956631367794, 'batch_time': 1.656661675768189, 'data_time': 0.0029963092985937456}
2025-11-06 03:05:07 Train Epoch 046:  92%|█████████▏| 2300/2502 [1:03:31<05:35,  1.66s/it, Loss=3.2911, Top1=N/A, LR=0.054618]2025-11-06 03:07:53,099 - INFO - Step 117392: {'train_loss_batch': 3.786630392074585, 'train_lr': 0.054617956631367794, 'batch_time': 1.6565865953297059, 'data_time': 0.0029121593307899632}
2025-11-06 03:07:53 Train Epoch 046:  96%|█████████▌| 2400/2502 [1:06:17<02:49,  1.66s/it, Loss=3.2943, Top1=N/A, LR=0.054618]2025-11-06 03:10:38,772 - INFO - Step 117492: {'train_loss_batch': 3.8101747035980225, 'train_lr': 0.054617956631367794, 'batch_time': 1.6565924357096884, 'data_time': 0.0028322484780231747}
2025-11-06 03:10:38 Train Epoch 046: 100%|█████████▉| 2500/2502 [1:09:03<00:03,  1.67s/it, Loss=3.2961, Top1=60.18%, LR=0.054618]2025-11-06 03:13:24,908 - INFO - Step 117592: {'train_loss_batch': 2.689387083053589, 'train_lr': 0.054617956631367794, 'batch_time': 1.656783042932119, 'data_time': 0.0028024151629326296}
2025-11-06 03:13:24 Train Epoch 046: 100%|██████████| 2502/2502 [1:09:05<00:00,  1.66s/it, Loss=3.2961, Top1=60.18%, LR=0.054618]
2025-11-06 03:13:27 Val Epoch 046:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 03:13:31   with torch.cuda.amp.autocast():
2025-11-06 03:13:32 Val Epoch 046: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.4379, Top1=62.44%, Top5=85.28%]
2025-11-06 03:15:18 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 03:15:18   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 03:15:18 2025-11-06 03:15:18,904 - INFO - Step 46: {'epoch': 46, 'learning_rate': 0.05277529668842393, 'train_loss': 3.2958117602445145, 'train_top1': 60.18560558617234, 'train_top5': 82.23673910320642, 'train_precision': 59.942124188417026, 'train_recall': 60.05417485829919, 'train_f1': 59.79368601088122, 'val_loss': 2.4379055023956298, 'val_top1': 62.43999999633789, 'val_top5': 85.27800001220703, 'val_precision': 66.12164262788488, 'val_recall': 62.436, 'val_f1': 62.08122591849137}
2025-11-06 03:15:18 2025-11-06 03:15:18,906 - INFO - Epoch 046 Summary - LR: 0.052775, Train Loss: 3.2958, Val Loss: 2.4379, Val F1: 62.08%, Val Precision: 66.12%, Val Recall: 62.44%
2025-11-06 03:15:19 Train Epoch 047:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 46 that is less than the current step 117592. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 03:15:23 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 03:15:23   with torch.cuda.amp.autocast():
2025-11-06 03:15:24 Train Epoch 047:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.6415, Top1=59.77%, LR=0.052775]2025-11-06 03:15:24,674 - INFO - Step 117594: {'train_loss_batch': 2.641512393951416, 'train_lr': 0.05277529668842393, 'batch_time': 5.333716869354248, 'data_time': 3.694674253463745}
2025-11-06 03:15:24 Train Epoch 047:   4%|▍         | 100/2502 [02:50<1:06:30,  1.66s/it, Loss=3.1604, Top1=N/A, LR=0.052775]   2025-11-06 03:18:09,794 - INFO - Step 117694: {'train_loss_batch': 2.6536035537719727, 'train_lr': 0.05277529668842393, 'batch_time': 1.6876585035040828, 'data_time': 0.03762203632014813}
2025-11-06 03:18:09 Train Epoch 047:   8%|▊         | 200/2502 [05:35<1:03:11,  1.65s/it, Loss=3.2481, Top1=N/A, LR=0.052775]2025-11-06 03:20:55,088 - INFO - Step 117794: {'train_loss_batch': 2.65879225730896, 'train_lr': 0.05277529668842393, 'batch_time': 1.6703873392361313, 'data_time': 0.019443605669695347}
2025-11-06 03:20:55 Train Epoch 047:  12%|█▏        | 300/2502 [08:20<1:00:29,  1.65s/it, Loss=3.2627, Top1=N/A, LR=0.052775]2025-11-06 03:23:39,993 - INFO - Step 117894: {'train_loss_batch': 3.982332706451416, 'train_lr': 0.05277529668842393, 'batch_time': 1.663296750217577, 'data_time': 0.013346510472091726}
2025-11-06 03:23:39 Train Epoch 047:  16%|█▌        | 400/2502 [11:05<57:44,  1.65s/it, Loss=3.2761, Top1=N/A, LR=0.052775]2025-11-06 03:26:24,938 - INFO - Step 117994: {'train_loss_batch': 4.242532253265381, 'train_lr': 0.05277529668842393, 'batch_time': 1.659842450125259, 'data_time': 0.010275643365341529}
2025-11-06 03:26:24 Train Epoch 047:  20%|█▉        | 500/2502 [13:50<55:02,  1.65s/it, Loss=3.2806, Top1=N/A, LR=0.052775]2025-11-06 03:29:09,905 - INFO - Step 118094: {'train_loss_batch': 2.4928741455078125, 'train_lr': 0.05277529668842393, 'batch_time': 1.6578125710972769, 'data_time': 0.008436424765520228}
2025-11-06 03:29:09 Train Epoch 047:  24%|██▍       | 600/2502 [16:36<52:19,  1.65s/it, Loss=3.2922, Top1=N/A, LR=0.052775]2025-11-06 03:31:55,493 - INFO - Step 118194: {'train_loss_batch': 2.922541618347168, 'train_lr': 0.05277529668842393, 'batch_time': 1.6574888653842463, 'data_time': 0.0072071790298487304}
2025-11-06 03:31:55 Train Epoch 047:  28%|██▊       | 700/2502 [19:21<49:42,  1.66s/it, Loss=3.2968, Top1=60.92%, LR=0.052775]2025-11-06 03:34:40,996 - INFO - Step 118294: {'train_loss_batch': 2.653759479522705, 'train_lr': 0.05277529668842393, 'batch_time': 1.6571393101429632, 'data_time': 0.006340554709441311}
2025-11-06 03:34:40 Train Epoch 047:  32%|███▏      | 800/2502 [22:06<46:47,  1.65s/it, Loss=3.2928, Top1=61.03%, LR=0.052775]2025-11-06 03:37:26,192 - INFO - Step 118394: {'train_loss_batch': 2.5021111965179443, 'train_lr': 0.05277529668842393, 'batch_time': 1.6564921523152516, 'data_time': 0.005677648251422782}
2025-11-06 03:37:26 Train Epoch 047:  36%|███▌      | 900/2502 [24:52<43:53,  1.64s/it, Loss=3.2879, Top1=N/A, LR=0.052775]   2025-11-06 03:40:11,650 - INFO - Step 118494: {'train_loss_batch': 2.6899356842041016, 'train_lr': 0.05277529668842393, 'batch_time': 1.6562788081618975, 'data_time': 0.005161636545179158}
2025-11-06 03:40:11 Train Epoch 047:  40%|███▉      | 1000/2502 [27:37<41:24,  1.65s/it, Loss=3.3010, Top1=N/A, LR=0.052775]2025-11-06 03:42:57,191 - INFO - Step 118594: {'train_loss_batch': 3.268582344055176, 'train_lr': 0.05277529668842393, 'batch_time': 1.6561922984166102, 'data_time': 0.0047474062287962285}
2025-11-06 03:42:57 Train Epoch 047:  44%|████▍     | 1100/2502 [30:23<38:28,  1.65s/it, Loss=3.3073, Top1=N/A, LR=0.052775]2025-11-06 03:45:42,933 - INFO - Step 118694: {'train_loss_batch': 4.425017356872559, 'train_lr': 0.05277529668842393, 'batch_time': 1.6563041909622345, 'data_time': 0.004409056593351858}
2025-11-06 03:45:42 Train Epoch 047:  48%|████▊     | 1200/2502 [33:08<36:05,  1.66s/it, Loss=3.3068, Top1=N/A, LR=0.052775]2025-11-06 03:48:28,164 - INFO - Step 118794: {'train_loss_batch': 4.213803768157959, 'train_lr': 0.05277529668842393, 'batch_time': 1.6559706315906915, 'data_time': 0.004134937289553221}
2025-11-06 03:48:28 Train Epoch 047:  52%|█████▏    | 1300/2502 [35:54<32:51,  1.64s/it, Loss=3.3168, Top1=60.78%, LR=0.052775]2025-11-06 03:51:13,508 - INFO - Step 118894: {'train_loss_batch': 2.715404987335205, 'train_lr': 0.05277529668842393, 'batch_time': 1.6557759729190389, 'data_time': 0.0039001649201236993}
2025-11-06 03:51:13 Train Epoch 047:  56%|█████▌    | 1400/2502 [38:39<30:27,  1.66s/it, Loss=3.3233, Top1=N/A, LR=0.052775]   2025-11-06 03:53:58,589 - INFO - Step 118994: {'train_loss_batch': 3.562023162841797, 'train_lr': 0.05277529668842393, 'batch_time': 1.6554209294274906, 'data_time': 0.003692697576758353}
2025-11-06 03:53:58 Train Epoch 047:  60%|█████▉    | 1500/2502 [41:25<27:35,  1.65s/it, Loss=3.3156, Top1=60.72%, LR=0.052775]2025-11-06 03:56:44,418 - INFO - Step 119094: {'train_loss_batch': 2.54691743850708, 'train_lr': 0.05277529668842393, 'batch_time': 1.655612504776123, 'data_time': 0.0035150897733216917}
2025-11-06 03:56:44 Train Epoch 047:  64%|██████▍   | 1600/2502 [44:10<24:51,  1.65s/it, Loss=3.3131, Top1=N/A, LR=0.052775]   2025-11-06 03:59:29,994 - INFO - Step 119194: {'train_loss_batch': 2.538095474243164, 'train_lr': 0.05277529668842393, 'batch_time': 1.6556208409196804, 'data_time': 0.003363307753925693}
2025-11-06 03:59:29 Train Epoch 047:  68%|██████▊   | 1700/2502 [46:55<21:49,  1.63s/it, Loss=3.3120, Top1=60.57%, LR=0.052775]2025-11-06 04:02:14,922 - INFO - Step 119294: {'train_loss_batch': 2.6105175018310547, 'train_lr': 0.05277529668842393, 'batch_time': 1.6552483347287814, 'data_time': 0.003225253652923602}
2025-11-06 04:02:14 Train Epoch 047:  72%|███████▏  | 1800/2502 [49:39<19:26,  1.66s/it, Loss=3.3165, Top1=N/A, LR=0.052775]   2025-11-06 04:04:59,075 - INFO - Step 119394: {'train_loss_batch': 3.2501220703125, 'train_lr': 0.05277529668842393, 'batch_time': 1.6544865471068386, 'data_time': 0.003100975561909779}
2025-11-06 04:04:59 Train Epoch 047:  76%|███████▌  | 1900/2502 [52:25<16:38,  1.66s/it, Loss=3.3086, Top1=N/A, LR=0.052775]2025-11-06 04:07:44,984 - INFO - Step 119494: {'train_loss_batch': 4.481721878051758, 'train_lr': 0.05277529668842393, 'batch_time': 1.6547285786808823, 'data_time': 0.0029950985966200327}
2025-11-06 04:07:44 Train Epoch 047:  80%|███████▉  | 2000/2502 [55:11<13:47,  1.65s/it, Loss=3.3092, Top1=N/A, LR=0.052775]2025-11-06 04:10:30,715 - INFO - Step 119594: {'train_loss_batch': 4.638685703277588, 'train_lr': 0.05277529668842393, 'batch_time': 1.654857850920731, 'data_time': 0.0029001183536039596}
2025-11-06 04:10:30 Train Epoch 047:  84%|████████▍ | 2100/2502 [57:56<11:08,  1.66s/it, Loss=3.3096, Top1=N/A, LR=0.052775]2025-11-06 04:13:15,766 - INFO - Step 119694: {'train_loss_batch': 4.7057294845581055, 'train_lr': 0.05277529668842393, 'batch_time': 1.6546503892687034, 'data_time': 0.0028132470887595844}
2025-11-06 04:13:15 Train Epoch 047:  88%|████████▊ | 2200/2502 [1:00:42<08:16,  1.64s/it, Loss=3.3108, Top1=N/A, LR=0.052775]2025-11-06 04:16:01,349 - INFO - Step 119794: {'train_loss_batch': 2.634167194366455, 'train_lr': 0.05277529668842393, 'batch_time': 1.6547040150740753, 'data_time': 0.002734914252780774}
2025-11-06 04:16:01 Train Epoch 047:  92%|█████████▏| 2300/2502 [1:03:27<05:35,  1.66s/it, Loss=3.3062, Top1=60.50%, LR=0.052775]2025-11-06 04:18:46,834 - INFO - Step 119894: {'train_loss_batch': 2.645526885986328, 'train_lr': 0.05277529668842393, 'batch_time': 1.6547104608800192, 'data_time': 0.002662783858569898}
2025-11-06 04:18:46 Train Epoch 047:  96%|█████████▌| 2400/2502 [1:06:13<02:49,  1.66s/it, Loss=3.3049, Top1=N/A, LR=0.052775]   2025-11-06 04:21:32,541 - INFO - Step 119994: {'train_loss_batch': 2.943575143814087, 'train_lr': 0.05277529668842393, 'batch_time': 1.6548085547347509, 'data_time': 0.002596417251898318}
2025-11-06 04:21:32 Train Epoch 047: 100%|█████████▉| 2500/2502 [1:08:58<00:03,  1.65s/it, Loss=3.3076, Top1=60.46%, LR=0.052775]2025-11-06 04:24:18,038 - INFO - Step 120094: {'train_loss_batch': 2.5790514945983887, 'train_lr': 0.05277529668842393, 'batch_time': 1.6548147658165433, 'data_time': 0.0025731406656087563}
2025-11-06 04:24:18 Train Epoch 047: 100%|██████████| 2502/2502 [1:09:00<00:00,  1.65s/it, Loss=3.3076, Top1=60.46%, LR=0.052775]
2025-11-06 04:24:20 Val Epoch 047:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 04:24:24   with torch.cuda.amp.autocast():
2025-11-06 04:24:25 Val Epoch 047: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=2.2942, Top1=65.62%, Top5=87.45%]
2025-11-06 04:26:10 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 04:26:10   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 04:26:10 2025-11-06 04:26:10,987 - INFO - Step 47: {'epoch': 47, 'learning_rate': 0.050928852853431704, 'train_loss': 3.307926562859667, 'train_top1': 60.456335399797574, 'train_top5': 82.33924278846153, 'train_precision': 60.21693301444855, 'train_recall': 60.33887300200107, 'train_f1': 60.077245023537394, 'val_loss': 2.294189951248169, 'val_top1': 65.61599997314453, 'val_top5': 87.45400001220703, 'val_precision': 67.73175118313232, 'val_recall': 65.61, 'val_f1': 65.08503869961355}
2025-11-06 04:26:10 2025-11-06 04:26:10,989 - INFO - Epoch 047 Summary - LR: 0.050929, Train Loss: 3.3079, Val Loss: 2.2942, Val F1: 65.09%, Val Precision: 67.73%, Val Recall: 65.61%
2025-11-06 04:26:14 2025-11-06 04:26:14,475 - INFO - New best model saved with validation accuracy: 65.616%
2025-11-06 04:26:14 2025-11-06 04:26:14,476 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_048.pth
2025-11-06 04:26:14 Train Epoch 048:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 04:26:18   with torch.cuda.amp.autocast():
2025-11-06 04:26:20 Train Epoch 048:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.5461, Top1=N/A, LR=0.050929]2025-11-06 04:26:20,270 - INFO - Step 120096: {'train_loss_batch': 3.546051025390625, 'train_lr': 0.050928852853431704, 'batch_time': 5.79082465171814, 'data_time': 4.143150568008423}
2025-11-06 04:26:20 Train Epoch 048:   0%|          | 1/2502 [00:05<4:01:30,  5.79s/it, Loss=3.5461, Top1=N/A, LR=0.050929]wandb: WARNING Tried to log to step 47 that is less than the current step 120094. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 04:26:21 Train Epoch 048:   4%|▍         | 100/2502 [02:51<1:06:36,  1.66s/it, Loss=3.3960, Top1=N/A, LR=0.050929]2025-11-06 04:29:05,823 - INFO - Step 120196: {'train_loss_batch': 3.014697790145874, 'train_lr': 0.050928852853431704, 'batch_time': 1.6964817944139536, 'data_time': 0.042095337763871296}
2025-11-06 04:29:05 Train Epoch 048:   8%|▊         | 200/2502 [05:36<1:03:38,  1.66s/it, Loss=3.3011, Top1=N/A, LR=0.050929]2025-11-06 04:31:50,785 - INFO - Step 120296: {'train_loss_batch': 3.3767075538635254, 'train_lr': 0.050928852853431704, 'batch_time': 1.6731672275125684, 'data_time': 0.021660676643029966}
2025-11-06 04:31:50 Train Epoch 048:  12%|█▏        | 300/2502 [08:22<1:01:11,  1.67s/it, Loss=3.3015, Top1=N/A, LR=0.050929]2025-11-06 04:34:36,520 - INFO - Step 120396: {'train_loss_batch': 2.547930955886841, 'train_lr': 0.050928852853431704, 'batch_time': 1.6679112325079022, 'data_time': 0.014799883991380863}
2025-11-06 04:34:36 Train Epoch 048:  16%|█▌        | 400/2502 [11:07<57:59,  1.66s/it, Loss=3.2823, Top1=N/A, LR=0.050929]2025-11-06 04:37:22,207 - INFO - Step 120496: {'train_loss_batch': 4.734046936035156, 'train_lr': 0.050928852853431704, 'batch_time': 1.665157774737351, 'data_time': 0.011362090669665254}
2025-11-06 04:37:22 Train Epoch 048:  20%|█▉        | 500/2502 [13:52<54:56,  1.65s/it, Loss=3.2833, Top1=61.41%, LR=0.050929]2025-11-06 04:40:07,112 - INFO - Step 120596: {'train_loss_batch': 2.5553579330444336, 'train_lr': 0.050928852853431704, 'batch_time': 1.6619415483074989, 'data_time': 0.009288679339928541}
2025-11-06 04:40:07 Train Epoch 048:  24%|██▍       | 600/2502 [16:37<51:48,  1.63s/it, Loss=3.2871, Top1=N/A, LR=0.050929]   2025-11-06 04:42:52,047 - INFO - Step 120696: {'train_loss_batch': 3.7026984691619873, 'train_lr': 0.050928852853431704, 'batch_time': 1.6598462058779802, 'data_time': 0.00790908094650497}
2025-11-06 04:42:52 Train Epoch 048:  28%|██▊       | 700/2502 [19:22<50:01,  1.67s/it, Loss=3.3055, Top1=N/A, LR=0.050929]2025-11-06 04:45:36,765 - INFO - Step 120796: {'train_loss_batch': 3.7016704082489014, 'train_lr': 0.050928852853431704, 'batch_time': 1.6580393787117385, 'data_time': 0.006928510230550072}
2025-11-06 04:45:36 Train Epoch 048:  32%|███▏      | 800/2502 [22:07<46:38,  1.64s/it, Loss=3.3138, Top1=N/A, LR=0.050929]2025-11-06 04:48:21,880 - INFO - Step 120896: {'train_loss_batch': 3.893502712249756, 'train_lr': 0.050928852853431704, 'batch_time': 1.6571783439050454, 'data_time': 0.006191554289780902}
2025-11-06 04:48:21 Train Epoch 048:  36%|███▌      | 900/2502 [24:53<44:23,  1.66s/it, Loss=3.3310, Top1=N/A, LR=0.050929]2025-11-06 04:51:07,696 - INFO - Step 120996: {'train_loss_batch': 4.2541937828063965, 'train_lr': 0.050928852853431704, 'batch_time': 1.6572866397480854, 'data_time': 0.005615994879990386}
2025-11-06 04:51:07 Train Epoch 048:  40%|███▉      | 1000/2502 [27:38<41:24,  1.65s/it, Loss=3.3254, Top1=N/A, LR=0.050929]2025-11-06 04:53:52,997 - INFO - Step 121096: {'train_loss_batch': 4.068451881408691, 'train_lr': 0.050928852853431704, 'batch_time': 1.6568601276729251, 'data_time': 0.005160053769548933}
2025-11-06 04:53:52 Train Epoch 048:  44%|████▍     | 1100/2502 [30:23<38:31,  1.65s/it, Loss=3.3261, Top1=60.97%, LR=0.050929]2025-11-06 04:56:38,011 - INFO - Step 121196: {'train_loss_batch': 2.534907579421997, 'train_lr': 0.050928852853431704, 'batch_time': 1.656249206787234, 'data_time': 0.004783291041472086}
2025-11-06 04:56:38 Train Epoch 048:  48%|████▊     | 1200/2502 [33:09<36:02,  1.66s/it, Loss=3.3171, Top1=N/A, LR=0.050929]   2025-11-06 04:59:23,654 - INFO - Step 121296: {'train_loss_batch': 2.565202474594116, 'train_lr': 0.050928852853431704, 'batch_time': 1.6562646654225905, 'data_time': 0.004472059572269081}
2025-11-06 04:59:23 Train Epoch 048:  52%|█████▏    | 1300/2502 [35:54<33:14,  1.66s/it, Loss=3.3029, Top1=N/A, LR=0.050929]2025-11-06 05:02:09,367 - INFO - Step 121396: {'train_loss_batch': 3.645170211791992, 'train_lr': 0.050928852853431704, 'batch_time': 1.6563304478530239, 'data_time': 0.004209921050309952}
2025-11-06 05:02:09 Train Epoch 048:  56%|█████▌    | 1400/2502 [38:40<30:26,  1.66s/it, Loss=3.3008, Top1=N/A, LR=0.050929]2025-11-06 05:04:54,775 - INFO - Step 121496: {'train_loss_batch': 3.919745922088623, 'train_lr': 0.050928852853431704, 'batch_time': 1.6561693461089368, 'data_time': 0.003981119390047932}
2025-11-06 05:04:54 Train Epoch 048:  60%|█████▉    | 1500/2502 [41:25<27:23,  1.64s/it, Loss=3.2980, Top1=60.97%, LR=0.050929]2025-11-06 05:07:40,165 - INFO - Step 121596: {'train_loss_batch': 2.5324883460998535, 'train_lr': 0.050928852853431704, 'batch_time': 1.656018750815293, 'data_time': 0.0037784827382940996}
2025-11-06 05:07:40 Train Epoch 048:  64%|██████▍   | 1600/2502 [44:10<24:57,  1.66s/it, Loss=3.3009, Top1=N/A, LR=0.050929]   2025-11-06 05:10:25,050 - INFO - Step 121696: {'train_loss_batch': 3.812798500061035, 'train_lr': 0.050928852853431704, 'batch_time': 1.6555706613291659, 'data_time': 0.0036050135608318074}
2025-11-06 05:10:25 Train Epoch 048:  68%|██████▊   | 1700/2502 [46:56<22:10,  1.66s/it, Loss=3.3079, Top1=60.87%, LR=0.050929]2025-11-06 05:13:10,495 - INFO - Step 121796: {'train_loss_batch': 2.729266405105591, 'train_lr': 0.050928852853431704, 'batch_time': 1.6555043982450293, 'data_time': 0.0034530818777740316}
2025-11-06 05:13:10 Train Epoch 048:  72%|███████▏  | 1800/2502 [49:41<19:17,  1.65s/it, Loss=3.3086, Top1=N/A, LR=0.050929]   2025-11-06 05:15:55,901 - INFO - Step 121896: {'train_loss_batch': 4.2389726638793945, 'train_lr': 0.050928852853431704, 'batch_time': 1.6554240309351487, 'data_time': 0.0033227892996932585}
2025-11-06 05:15:55 Train Epoch 048:  76%|███████▌  | 1900/2502 [52:26<16:27,  1.64s/it, Loss=3.3028, Top1=60.82%, LR=0.050929]2025-11-06 05:18:41,119 - INFO - Step 121996: {'train_loss_batch': 2.5992681980133057, 'train_lr': 0.050928852853431704, 'batch_time': 1.655253543783526, 'data_time': 0.0031976338626835484}
2025-11-06 05:18:41 Train Epoch 048:  80%|███████▉  | 2000/2502 [55:09<13:43,  1.64s/it, Loss=3.3018, Top1=N/A, LR=0.050929]   2025-11-06 05:21:24,360 - INFO - Step 122096: {'train_loss_batch': 3.7207326889038086, 'train_lr': 0.050928852853431704, 'batch_time': 1.654112020532588, 'data_time': 0.003087137175583351}
2025-11-06 05:21:24 Train Epoch 048:  84%|████████▍ | 2100/2502 [57:55<11:07,  1.66s/it, Loss=3.2989, Top1=N/A, LR=0.050929]2025-11-06 05:24:09,942 - INFO - Step 122196: {'train_loss_batch': 2.937147378921509, 'train_lr': 0.050928852853431704, 'batch_time': 1.6541928199857034, 'data_time': 0.002990034748859941}
2025-11-06 05:24:09 Train Epoch 048:  88%|████████▊ | 2200/2502 [1:00:40<08:21,  1.66s/it, Loss=3.2987, Top1=N/A, LR=0.050929]2025-11-06 05:26:55,329 - INFO - Step 122296: {'train_loss_batch': 2.705869674682617, 'train_lr': 0.050928852853431704, 'batch_time': 1.6541781340984256, 'data_time': 0.0029013894569435103}
2025-11-06 05:26:55 Train Epoch 048:  92%|█████████▏| 2300/2502 [1:03:26<05:30,  1.64s/it, Loss=3.2971, Top1=60.74%, LR=0.050929]2025-11-06 05:29:40,555 - INFO - Step 122396: {'train_loss_batch': 2.698955774307251, 'train_lr': 0.050928852853431704, 'batch_time': 1.6540946610229836, 'data_time': 0.0028190311272731816}
2025-11-06 05:29:40 Train Epoch 048:  96%|█████████▌| 2400/2502 [1:06:11<02:48,  1.65s/it, Loss=3.2960, Top1=N/A, LR=0.050929]   2025-11-06 05:32:25,792 - INFO - Step 122496: {'train_loss_batch': 3.4602303504943848, 'train_lr': 0.050928852853431704, 'batch_time': 1.6540227235232428, 'data_time': 0.0027432017701707846}
2025-11-06 05:32:25 Train Epoch 048: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.65s/it, Loss=3.2999, Top1=N/A, LR=0.050929]2025-11-06 05:35:11,188 - INFO - Step 122596: {'train_loss_batch': 2.78423810005188, 'train_lr': 0.050928852853431704, 'batch_time': 1.6540202988666899, 'data_time': 0.002720590116309433}
2025-11-06 05:35:11 Train Epoch 048: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=3.2999, Top1=N/A, LR=0.050929]
2025-11-06 05:35:13 Val Epoch 048:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 05:35:17   with torch.cuda.amp.autocast():
2025-11-06 05:35:18 Val Epoch 048: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.3129, Top1=65.57%, Top5=87.37%]
2025-11-06 05:37:05 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 05:37:05   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 05:37:05 2025-11-06 05:37:05,414 - INFO - Step 48: {'epoch': 48, 'learning_rate': 0.04908114714656832, 'train_loss': 3.2995394365393, 'train_top1': 60.70825441919192, 'train_top5': 82.53866792929293, 'train_precision': 60.50057621894103, 'train_recall': 60.57248000124139, 'train_f1': 60.33752489227056, 'val_loss': 2.3129411503601074, 'val_top1': 65.56999997314453, 'val_top5': 87.37199999267578, 'val_precision': 68.05492112843318, 'val_recall': 65.56, 'val_f1': 65.02707272409293}
2025-11-06 05:37:05 2025-11-06 05:37:05,416 - INFO - Epoch 048 Summary - LR: 0.049081, Train Loss: 3.2995, Val Loss: 2.3129, Val F1: 65.03%, Val Precision: 68.05%, Val Recall: 65.56%
2025-11-06 05:37:06 Train Epoch 049:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 05:37:10   with torch.cuda.amp.autocast():
2025-11-06 05:37:10 wandb: WARNING Tried to log to step 48 that is less than the current step 122596. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 05:37:12 Train Epoch 049:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.5292, Top1=N/A, LR=0.049081]2025-11-06 05:37:12,012 - INFO - Step 122598: {'train_loss_batch': 2.5292062759399414, 'train_lr': 0.04908114714656832, 'batch_time': 5.798447132110596, 'data_time': 4.155773639678955}
2025-11-06 05:37:12 Train Epoch 049:   4%|▍         | 100/2502 [02:51<1:06:33,  1.66s/it, Loss=3.2912, Top1=N/A, LR=0.049081]2025-11-06 05:39:57,709 - INFO - Step 122698: {'train_loss_batch': 4.2136993408203125, 'train_lr': 0.04908114714656832, 'batch_time': 1.697989791926771, 'data_time': 0.04232808859041422}
2025-11-06 05:39:57 Train Epoch 049:   8%|▊         | 200/2502 [05:37<1:03:20,  1.65s/it, Loss=3.2872, Top1=N/A, LR=0.049081]2025-11-06 05:42:43,329 - INFO - Step 122798: {'train_loss_batch': 2.9341862201690674, 'train_lr': 0.04908114714656832, 'batch_time': 1.6772009282562863, 'data_time': 0.021783049426861663}
2025-11-06 05:42:43 Train Epoch 049:  12%|█▏        | 300/2502 [08:23<1:00:48,  1.66s/it, Loss=3.3038, Top1=N/A, LR=0.049081]2025-11-06 05:45:29,253 - INFO - Step 122898: {'train_loss_batch': 2.542332649230957, 'train_lr': 0.04908114714656832, 'batch_time': 1.6712330639164312, 'data_time': 0.014890419684375242}
2025-11-06 05:45:29 Train Epoch 049:  16%|█▌        | 400/2502 [11:08<58:15,  1.66s/it, Loss=3.3034, Top1=N/A, LR=0.049081]2025-11-06 05:48:15,145 - INFO - Step 122998: {'train_loss_batch': 3.272650718688965, 'train_lr': 0.04908114714656832, 'batch_time': 1.6681594836741611, 'data_time': 0.011435459974103437}
2025-11-06 05:48:15 Train Epoch 049:  20%|█▉        | 500/2502 [13:53<54:36,  1.64s/it, Loss=3.2986, Top1=61.19%, LR=0.049081]2025-11-06 05:50:59,930 - INFO - Step 123098: {'train_loss_batch': 2.7096166610717773, 'train_lr': 0.04908114714656832, 'batch_time': 1.6641075101917138, 'data_time': 0.009353353591736205}
2025-11-06 05:50:59 Train Epoch 049:  24%|██▍       | 600/2502 [16:38<52:37,  1.66s/it, Loss=3.3003, Top1=N/A, LR=0.049081]   2025-11-06 05:53:44,224 - INFO - Step 123198: {'train_loss_batch': 4.544064521789551, 'train_lr': 0.04908114714656832, 'batch_time': 1.6605835143420937, 'data_time': 0.00795429439195579}
2025-11-06 05:53:44 Train Epoch 049:  28%|██▊       | 700/2502 [19:23<49:33,  1.65s/it, Loss=3.3042, Top1=N/A, LR=0.049081]2025-11-06 05:56:29,741 - INFO - Step 123298: {'train_loss_batch': 5.032533168792725, 'train_lr': 0.04908114714656832, 'batch_time': 1.6598116647499945, 'data_time': 0.006955808987801153}
2025-11-06 05:56:29 Train Epoch 049:  32%|███▏      | 800/2502 [22:08<47:05,  1.66s/it, Loss=3.2926, Top1=N/A, LR=0.049081]2025-11-06 05:59:15,083 - INFO - Step 123398: {'train_loss_batch': 2.577897548675537, 'train_lr': 0.04908114714656832, 'batch_time': 1.659012766813071, 'data_time': 0.006210166119159979}
2025-11-06 05:59:15 Train Epoch 049:  36%|███▌      | 900/2502 [24:54<44:15,  1.66s/it, Loss=3.2827, Top1=61.39%, LR=0.049081]2025-11-06 06:02:00,893 - INFO - Step 123498: {'train_loss_batch': 2.8019814491271973, 'train_lr': 0.04908114714656832, 'batch_time': 1.6589118235648406, 'data_time': 0.005635213375620784}
2025-11-06 06:02:00 Train Epoch 049:  40%|███▉      | 1000/2502 [27:39<41:23,  1.65s/it, Loss=3.2770, Top1=61.29%, LR=0.049081]2025-11-06 06:04:45,736 - INFO - Step 123598: {'train_loss_batch': 2.6497979164123535, 'train_lr': 0.04908114714656832, 'batch_time': 1.6578645584704754, 'data_time': 0.005169717701045903}
2025-11-06 06:04:45 Train Epoch 049:  44%|████▍     | 1100/2502 [30:24<38:31,  1.65s/it, Loss=3.2685, Top1=61.14%, LR=0.049081]2025-11-06 06:07:31,006 - INFO - Step 123698: {'train_loss_batch': 2.5629138946533203, 'train_lr': 0.04908114714656832, 'batch_time': 1.6573947619352418, 'data_time': 0.004788676356316479}
2025-11-06 06:07:31 Train Epoch 049:  48%|████▊     | 1200/2502 [33:10<36:02,  1.66s/it, Loss=3.2657, Top1=N/A, LR=0.049081]   2025-11-06 06:10:16,387 - INFO - Step 123798: {'train_loss_batch': 3.692040205001831, 'train_lr': 0.04908114714656832, 'batch_time': 1.6570966567326149, 'data_time': 0.004472605890278813}
2025-11-06 06:10:16 Train Epoch 049:  52%|█████▏    | 1300/2502 [35:55<33:17,  1.66s/it, Loss=3.2709, Top1=61.21%, LR=0.049081]2025-11-06 06:13:01,410 - INFO - Step 123898: {'train_loss_batch': 2.533511161804199, 'train_lr': 0.04908114714656832, 'batch_time': 1.656568363022566, 'data_time': 0.00420632717886125}
2025-11-06 06:13:01 Train Epoch 049:  56%|█████▌    | 1400/2502 [38:40<29:56,  1.63s/it, Loss=3.2804, Top1=N/A, LR=0.049081]   2025-11-06 06:15:46,529 - INFO - Step 123998: {'train_loss_batch': 4.74766731262207, 'train_lr': 0.04908114714656832, 'batch_time': 1.656184247525397, 'data_time': 0.0039781724274966825}
2025-11-06 06:15:46 Train Epoch 049:  60%|█████▉    | 1500/2502 [41:23<27:32,  1.65s/it, Loss=3.2865, Top1=N/A, LR=0.049081]2025-11-06 06:18:29,824 - INFO - Step 124098: {'train_loss_batch': 4.083527088165283, 'train_lr': 0.04908114714656832, 'batch_time': 1.6546362339060439, 'data_time': 0.003776258027688572}
2025-11-06 06:18:29 Train Epoch 049:  64%|██████▍   | 1600/2502 [44:09<24:50,  1.65s/it, Loss=3.2885, Top1=N/A, LR=0.049081]2025-11-06 06:21:15,311 - INFO - Step 124198: {'train_loss_batch': 4.175769805908203, 'train_lr': 0.04908114714656832, 'batch_time': 1.6546506309866682, 'data_time': 0.003603825190899746}
2025-11-06 06:21:15 Train Epoch 049:  68%|██████▊   | 1700/2502 [46:54<22:03,  1.65s/it, Loss=3.2907, Top1=N/A, LR=0.049081]2025-11-06 06:24:00,797 - INFO - Step 124298: {'train_loss_batch': 3.6243538856506348, 'train_lr': 0.04908114714656832, 'batch_time': 1.6546627177272664, 'data_time': 0.003449198500259283}
2025-11-06 06:24:00 Train Epoch 049:  72%|███████▏  | 1800/2502 [49:40<19:23,  1.66s/it, Loss=3.2948, Top1=N/A, LR=0.049081]2025-11-06 06:26:46,308 - INFO - Step 124398: {'train_loss_batch': 3.9210939407348633, 'train_lr': 0.04908114714656832, 'batch_time': 1.6546880119976106, 'data_time': 0.0033137746680649964}
2025-11-06 06:26:46 Train Epoch 049:  76%|███████▌  | 1900/2502 [52:25<16:41,  1.66s/it, Loss=3.2961, Top1=N/A, LR=0.049081]2025-11-06 06:29:31,531 - INFO - Step 124498: {'train_loss_batch': 4.140554428100586, 'train_lr': 0.04908114714656832, 'batch_time': 1.65455835835047, 'data_time': 0.0031919029121459126}
2025-11-06 06:29:31 Train Epoch 049:  80%|███████▉  | 2000/2502 [55:10<13:55,  1.66s/it, Loss=3.2966, Top1=N/A, LR=0.049081]2025-11-06 06:32:17,118 - INFO - Step 124598: {'train_loss_batch': 4.174564361572266, 'train_lr': 0.04908114714656832, 'batch_time': 1.6546237054078474, 'data_time': 0.003087550744243052}
2025-11-06 06:32:17 Train Epoch 049:  84%|████████▍ | 2100/2502 [57:56<10:57,  1.64s/it, Loss=3.2943, Top1=N/A, LR=0.049081]2025-11-06 06:35:02,652 - INFO - Step 124698: {'train_loss_batch': 2.6685128211975098, 'train_lr': 0.04908114714656832, 'batch_time': 1.6546575969766855, 'data_time': 0.002992298760793142}
2025-11-06 06:35:02 Train Epoch 049:  88%|████████▊ | 2200/2502 [1:00:40<08:16,  1.64s/it, Loss=3.2929, Top1=N/A, LR=0.049081]2025-11-06 06:37:47,050 - INFO - Step 124798: {'train_loss_batch': 2.583799362182617, 'train_lr': 0.04908114714656832, 'batch_time': 1.6541727202310177, 'data_time': 0.0029021719811667857}
2025-11-06 06:37:47 Train Epoch 049:  92%|█████████▏| 2300/2502 [1:03:26<05:34,  1.66s/it, Loss=3.2955, Top1=N/A, LR=0.049081]2025-11-06 06:40:32,701 - INFO - Step 124898: {'train_loss_batch': 2.971370220184326, 'train_lr': 0.04908114714656832, 'batch_time': 1.654273979977803, 'data_time': 0.0028220016714905925}
2025-11-06 06:40:32 Train Epoch 049:  96%|█████████▌| 2400/2502 [1:06:11<02:48,  1.65s/it, Loss=3.2964, Top1=N/A, LR=0.049081]2025-11-06 06:43:18,058 - INFO - Step 124998: {'train_loss_batch': 3.8476366996765137, 'train_lr': 0.04908114714656832, 'batch_time': 1.6542448446384226, 'data_time': 0.0027473775211447034}
2025-11-06 06:43:18 Train Epoch 049: 100%|█████████▉| 2500/2502 [1:08:57<00:03,  1.66s/it, Loss=3.2985, Top1=N/A, LR=0.049081]2025-11-06 06:46:03,837 - INFO - Step 125098: {'train_loss_batch': 2.4833455085754395, 'train_lr': 0.04908114714656832, 'batch_time': 1.6543861702412235, 'data_time': 0.002699896222541257}
2025-11-06 06:46:03 Train Epoch 049: 100%|██████████| 2502/2502 [1:08:59<00:00,  1.65s/it, Loss=3.2985, Top1=N/A, LR=0.049081]
2025-11-06 06:46:06 Val Epoch 049:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 06:46:10   with torch.cuda.amp.autocast():
2025-11-06 06:46:11 Val Epoch 049: 100%|██████████| 98/98 [01:53<00:00,  1.16s/it, Loss=2.2930, Top1=65.67%, Top5=87.47%]
2025-11-06 06:47:59 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 06:47:59   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 06:47:59 2025-11-06 06:47:59,871 - INFO - Step 49: {'epoch': 49, 'learning_rate': 0.0472347033115761, 'train_loss': 3.298163657184604, 'train_top1': 61.09770467458678, 'train_top5': 82.66924393078513, 'train_precision': 60.898138603239126, 'train_recall': 60.95703241713497, 'train_f1': 60.726047589509626, 'val_loss': 2.29297493637085, 'val_top1': 65.66599997558593, 'val_top5': 87.47200000488282, 'val_precision': 67.68966649171017, 'val_recall': 65.666, 'val_f1': 65.1707306991031}
2025-11-06 06:47:59 2025-11-06 06:47:59,873 - INFO - Epoch 049 Summary - LR: 0.047235, Train Loss: 3.2982, Val Loss: 2.2930, Val F1: 65.17%, Val Precision: 67.69%, Val Recall: 65.67%
2025-11-06 06:48:00 wandb: WARNING Tried to log to step 49 that is less than the current step 125098. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 06:48:03 2025-11-06 06:48:03,361 - INFO - New best model saved with validation accuracy: 65.666%
2025-11-06 06:48:03 2025-11-06 06:48:03,362 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_050.pth
2025-11-06 06:48:03 Train Epoch 050:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 06:48:07   with torch.cuda.amp.autocast():
2025-11-06 06:48:09 Train Epoch 050:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.5371, Top1=63.09%, LR=0.047235]2025-11-06 06:48:09,103 - INFO - Step 125100: {'train_loss_batch': 2.53709077835083, 'train_lr': 0.0472347033115761, 'batch_time': 5.739236354827881, 'data_time': 4.097865581512451}
2025-11-06 06:48:09 Train Epoch 050:   4%|▍         | 100/2502 [02:50<1:05:57,  1.65s/it, Loss=3.3195, Top1=N/A, LR=0.047235]   2025-11-06 06:50:54,359 - INFO - Step 125200: {'train_loss_batch': 4.297191143035889, 'train_lr': 0.0472347033115761, 'batch_time': 1.6930268188514332, 'data_time': 0.041609454863142256}
2025-11-06 06:50:54 Train Epoch 050:   8%|▊         | 200/2502 [05:36<1:03:21,  1.65s/it, Loss=3.2431, Top1=N/A, LR=0.047235]2025-11-06 06:53:39,662 - INFO - Step 125300: {'train_loss_batch': 4.351841449737549, 'train_lr': 0.0472347033115761, 'batch_time': 1.673126459121704, 'data_time': 0.02136904683279161}
2025-11-06 06:53:39 Train Epoch 050:  10%|▉         | 250/2502 [06:56<1:01:49,  1.65s/it, Loss=3.2431, Top1=N/A, LR=0.047235]
2025-11-06 06:56:24 Train Epoch 050:  16%|█▌        | 400/2502 [11:05<57:30,  1.64s/it, Loss=3.2226, Top1=61.87%, LR=0.047235]2025-11-06 06:59:09,309 - INFO - Step 125500: {'train_loss_batch': 2.648921489715576, 'train_lr': 0.0472347033115761, 'batch_time': 1.6607124418986408, 'data_time': 0.0112116027651285}
2025-11-06 06:59:09 Train Epoch 050:  20%|█▉        | 500/2502 [13:51<55:22,  1.66s/it, Loss=3.2591, Top1=N/A, LR=0.047235]   2025-11-06 07:01:54,979 - INFO - Step 125600: {'train_loss_batch': 4.849362373352051, 'train_lr': 0.0472347033115761, 'batch_time': 1.6599099122121663, 'data_time': 0.009188162352510555}
2025-11-06 07:01:54 Train Epoch 050:  24%|██▍       | 600/2502 [16:36<52:11,  1.65s/it, Loss=3.2494, Top1=N/A, LR=0.047235]2025-11-06 07:04:40,216 - INFO - Step 125700: {'train_loss_batch': 2.635640859603882, 'train_lr': 0.0472347033115761, 'batch_time': 1.6586553796555557, 'data_time': 0.00783298931978705}
2025-11-06 07:04:40 Train Epoch 050:  28%|██▊       | 700/2502 [19:22<49:37,  1.65s/it, Loss=3.2534, Top1=N/A, LR=0.047235]2025-11-06 07:07:26,052 - INFO - Step 125800: {'train_loss_batch': 2.4532179832458496, 'train_lr': 0.0472347033115761, 'batch_time': 1.6586128162079292, 'data_time': 0.006863267547563888}
2025-11-06 07:07:26 Train Epoch 050:  32%|███▏      | 800/2502 [22:08<47:01,  1.66s/it, Loss=3.2442, Top1=N/A, LR=0.047235]2025-11-06 07:10:11,815 - INFO - Step 125900: {'train_loss_batch': 4.410429954528809, 'train_lr': 0.0472347033115761, 'batch_time': 1.6584895448887096, 'data_time': 0.006127835510672999}
2025-11-06 07:10:11 Train Epoch 050:  36%|███▌      | 900/2502 [24:52<43:45,  1.64s/it, Loss=3.2434, Top1=N/A, LR=0.047235]2025-11-06 07:12:55,947 - INFO - Step 126000: {'train_loss_batch': 5.001591682434082, 'train_lr': 0.0472347033115761, 'batch_time': 1.6565839506015925, 'data_time': 0.005559362925911584}
2025-11-06 07:12:55 Train Epoch 050:  40%|███▉      | 1000/2502 [27:37<41:19,  1.65s/it, Loss=3.2385, Top1=61.57%, LR=0.047235]2025-11-06 07:15:41,205 - INFO - Step 126100: {'train_loss_batch': 2.5133886337280273, 'train_lr': 0.0472347033115761, 'batch_time': 1.6561843872546673, 'data_time': 0.005104926439908358}
2025-11-06 07:15:41 Train Epoch 050:  44%|████▍     | 1100/2502 [30:21<38:02,  1.63s/it, Loss=3.2433, Top1=61.56%, LR=0.047235]2025-11-06 07:18:24,872 - INFO - Step 126200: {'train_loss_batch': 2.419275999069214, 'train_lr': 0.0472347033115761, 'batch_time': 1.6544117018052602, 'data_time': 0.004729359719452265}
2025-11-06 07:18:24 Train Epoch 050:  48%|████▊     | 1200/2502 [33:05<35:56,  1.66s/it, Loss=3.2483, Top1=N/A, LR=0.047235]   2025-11-06 07:21:09,042 - INFO - Step 126300: {'train_loss_batch': 4.204291820526123, 'train_lr': 0.0472347033115761, 'batch_time': 1.6533529683017016, 'data_time': 0.0044161481325275}
2025-11-06 07:21:09 Train Epoch 050:  52%|█████▏    | 1300/2502 [35:50<33:02,  1.65s/it, Loss=3.2660, Top1=N/A, LR=0.047235]2025-11-06 07:23:54,120 - INFO - Step 126400: {'train_loss_batch': 3.5593972206115723, 'train_lr': 0.0472347033115761, 'batch_time': 1.6531546207504213, 'data_time': 0.004155244944555589}
2025-11-06 07:23:54 Train Epoch 050:  56%|█████▌    | 1400/2502 [38:36<30:33,  1.66s/it, Loss=3.2699, Top1=N/A, LR=0.047235]2025-11-06 07:26:39,758 - INFO - Step 126500: {'train_loss_batch': 2.4134128093719482, 'train_lr': 0.0472347033115761, 'batch_time': 1.6533850054499253, 'data_time': 0.003935592332795039}
2025-11-06 07:26:39 Train Epoch 050:  60%|█████▉    | 1500/2502 [41:21<27:32,  1.65s/it, Loss=3.2703, Top1=N/A, LR=0.047235]2025-11-06 07:29:24,932 - INFO - Step 126600: {'train_loss_batch': 2.6560418605804443, 'train_lr': 0.0472347033115761, 'batch_time': 1.6532749994685856, 'data_time': 0.0037467454291438356}
2025-11-06 07:29:24 Train Epoch 050:  64%|██████▍   | 1600/2502 [44:07<24:54,  1.66s/it, Loss=3.2694, Top1=N/A, LR=0.047235]2025-11-06 07:32:10,548 - INFO - Step 126700: {'train_loss_batch': 2.548952341079712, 'train_lr': 0.0472347033115761, 'batch_time': 1.6534549594297772, 'data_time': 0.003579886387021448}
2025-11-06 07:32:10 Train Epoch 050:  68%|██████▊   | 1700/2502 [46:51<21:56,  1.64s/it, Loss=3.2655, Top1=61.44%, LR=0.047235]2025-11-06 07:34:54,777 - INFO - Step 126800: {'train_loss_batch': 2.61794114112854, 'train_lr': 0.0472347033115761, 'batch_time': 1.6527986170473272, 'data_time': 0.0034299192535113335}
2025-11-06 07:34:54 Train Epoch 050:  72%|███████▏  | 1800/2502 [49:36<19:22,  1.66s/it, Loss=3.2701, Top1=N/A, LR=0.047235]   2025-11-06 07:37:39,805 - INFO - Step 126900: {'train_loss_batch': 5.047689437866211, 'train_lr': 0.0472347033115761, 'batch_time': 1.6526587315230552, 'data_time': 0.0032969033433489507}
2025-11-06 07:37:39 Train Epoch 050:  76%|███████▌  | 1900/2502 [52:22<16:34,  1.65s/it, Loss=3.2629, Top1=N/A, LR=0.047235]2025-11-06 07:40:25,431 - INFO - Step 127000: {'train_loss_batch': 2.4648616313934326, 'train_lr': 0.0472347033115761, 'batch_time': 1.6528480377779202, 'data_time': 0.003176332335294014}
2025-11-06 07:40:25 Train Epoch 050:  80%|███████▉  | 2000/2502 [55:07<13:47,  1.65s/it, Loss=3.2582, Top1=N/A, LR=0.047235]2025-11-06 07:43:11,207 - INFO - Step 127100: {'train_loss_batch': 4.084173679351807, 'train_lr': 0.0472347033115761, 'batch_time': 1.6530934817072511, 'data_time': 0.00307170776412941}
2025-11-06 07:43:11 Train Epoch 050:  84%|████████▍ | 2100/2502 [57:53<10:59,  1.64s/it, Loss=3.2562, Top1=N/A, LR=0.047235]2025-11-06 07:45:56,474 - INFO - Step 127200: {'train_loss_batch': 2.6925714015960693, 'train_lr': 0.0472347033115761, 'batch_time': 1.6530731162362187, 'data_time': 0.002974100535055503}
2025-11-06 07:45:56 Train Epoch 050:  88%|████████▊ | 2200/2502 [1:00:37<08:20,  1.66s/it, Loss=3.2575, Top1=N/A, LR=0.047235]2025-11-06 07:48:41,235 - INFO - Step 127300: {'train_loss_batch': 2.6222896575927734, 'train_lr': 0.0472347033115761, 'batch_time': 1.652824862010909, 'data_time': 0.002885080911202195}
2025-11-06 07:48:41 Train Epoch 050:  92%|█████████▏| 2300/2502 [1:03:22<05:33,  1.65s/it, Loss=3.2510, Top1=N/A, LR=0.047235]2025-11-06 07:51:25,503 - INFO - Step 127400: {'train_loss_batch': 3.273331880569458, 'train_lr': 0.0472347033115761, 'batch_time': 1.6523834551380594, 'data_time': 0.0028033248241545582}
2025-11-06 07:51:25 Train Epoch 050:  96%|█████████▌| 2400/2502 [1:06:07<02:48,  1.66s/it, Loss=3.2513, Top1=61.31%, LR=0.047235]2025-11-06 07:54:11,008 - INFO - Step 127500: {'train_loss_batch': 2.5515923500061035, 'train_lr': 0.0472347033115761, 'batch_time': 1.6524945479142372, 'data_time': 0.0027322524887777675}
2025-11-06 07:54:11 Train Epoch 050: 100%|█████████▉| 2500/2502 [1:08:53<00:03,  1.66s/it, Loss=3.2553, Top1=N/A, LR=0.047235]   2025-11-06 07:56:56,797 - INFO - Step 127600: {'train_loss_batch': 4.280606269836426, 'train_lr': 0.0472347033115761, 'batch_time': 1.6527104064113185, 'data_time': 0.0026858587924693403}
2025-11-06 07:56:56 Train Epoch 050: 100%|██████████| 2502/2502 [1:08:55<00:00,  1.65s/it, Loss=3.2553, Top1=N/A, LR=0.047235]
2025-11-06 07:56:59 Val Epoch 050:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 07:57:03   with torch.cuda.amp.autocast():
2025-11-06 07:57:03 Val Epoch 050: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=2.3070, Top1=65.72%, Top5=87.41%]
2025-11-06 07:58:50 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 07:58:50   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 07:58:50 2025-11-06 07:58:50,048 - INFO - Step 50: {'epoch': 50, 'learning_rate': 0.04539204336863223, 'train_loss': 3.255421630674891, 'train_top1': 61.31976043897216, 'train_top5': 82.94256892398288, 'train_precision': 61.08753181508434, 'train_recall': 61.184308846936794, 'train_f1': 60.9396146966374, 'val_loss': 2.307018599319458, 'val_top1': 65.72399997070312, 'val_top5': 87.408, 'val_precision': 68.12422384758781, 'val_recall': 65.726, 'val_f1': 65.13454291340136}
2025-11-06 07:58:50 2025-11-06 07:58:50,050 - INFO - Epoch 050 Summary - LR: 0.045392, Train Loss: 3.2554, Val Loss: 2.3070, Val F1: 65.13%, Val Precision: 68.12%, Val Recall: 65.73%
2025-11-06 07:58:51 wandb: WARNING Tried to log to step 50 that is less than the current step 127600. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 07:58:53 2025-11-06 07:58:53,563 - INFO - New best model saved with validation accuracy: 65.724%
2025-11-06 07:58:53 2025-11-06 07:58:53,564 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_051.pth
2025-11-06 07:58:53 Train Epoch 051:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 07:58:57   with torch.cuda.amp.autocast():
2025-11-06 07:58:58 Train Epoch 051:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.5746, Top1=N/A, LR=0.045392]2025-11-06 07:58:58,818 - INFO - Step 127602: {'train_loss_batch': 2.574610948562622, 'train_lr': 0.04539204336863223, 'batch_time': 5.253155708312988, 'data_time': 3.572679042816162}
2025-11-06 07:58:58 Train Epoch 051:   4%|▍         | 100/2502 [02:51<1:06:17,  1.66s/it, Loss=3.2752, Top1=N/A, LR=0.045392]2025-11-06 08:01:44,581 - INFO - Step 127702: {'train_loss_batch': 3.7808661460876465, 'train_lr': 0.04539204336863223, 'batch_time': 1.6932350007614287, 'data_time': 0.03638986313697135}
2025-11-06 08:01:44 Train Epoch 051:   8%|▊         | 200/2502 [05:36<1:03:27,  1.65s/it, Loss=3.3016, Top1=N/A, LR=0.045392]2025-11-06 08:04:30,358 - INFO - Step 127802: {'train_loss_batch': 2.5841379165649414, 'train_lr': 0.04539204336863223, 'batch_time': 1.6755881950036804, 'data_time': 0.018772361290395556}
2025-11-06 08:04:30 Train Epoch 051:  12%|█▏        | 300/2502 [08:22<1:00:41,  1.65s/it, Loss=3.2998, Top1=N/A, LR=0.045392]2025-11-06 08:07:16,230 - INFO - Step 127902: {'train_loss_batch': 2.546882390975952, 'train_lr': 0.04539204336863223, 'batch_time': 1.6699827279759405, 'data_time': 0.012861471239514526}
2025-11-06 08:07:16 Train Epoch 051:  16%|█▌        | 400/2502 [11:07<57:47,  1.65s/it, Loss=3.2800, Top1=N/A, LR=0.045392]2025-11-06 08:10:01,224 - INFO - Step 128002: {'train_loss_batch': 2.499964952468872, 'train_lr': 0.04539204336863223, 'batch_time': 1.664984167959922, 'data_time': 0.009906528002008834}
2025-11-06 08:10:01 Train Epoch 051:  20%|█▉        | 500/2502 [13:53<55:16,  1.66s/it, Loss=3.2538, Top1=N/A, LR=0.045392]2025-11-06 08:12:46,805 - INFO - Step 128102: {'train_loss_batch': 4.10678768157959, 'train_lr': 0.04539204336863223, 'batch_time': 1.663152030842033, 'data_time': 0.00812609324198283}
2025-11-06 08:12:46 Train Epoch 051:  24%|██▍       | 600/2502 [16:38<52:05,  1.64s/it, Loss=3.2575, Top1=N/A, LR=0.045392]2025-11-06 08:15:31,850 - INFO - Step 128202: {'train_loss_batch': 3.0841469764709473, 'train_lr': 0.04539204336863223, 'batch_time': 1.661038082966987, 'data_time': 0.006938117911137281}
2025-11-06 08:15:31 Train Epoch 051:  28%|██▊       | 700/2502 [19:23<49:49,  1.66s/it, Loss=3.2765, Top1=N/A, LR=0.045392]2025-11-06 08:18:17,301 - INFO - Step 128302: {'train_loss_batch': 3.7922301292419434, 'train_lr': 0.04539204336863223, 'batch_time': 1.660106407252596, 'data_time': 0.006104796827264588}
2025-11-06 08:18:17 Train Epoch 051:  32%|███▏      | 800/2502 [22:09<46:52,  1.65s/it, Loss=3.2613, Top1=62.03%, LR=0.045392]2025-11-06 08:21:03,080 - INFO - Step 128402: {'train_loss_batch': 2.480729579925537, 'train_lr': 0.04539204336863223, 'batch_time': 1.6598180727416953, 'data_time': 0.005467553561397557}
2025-11-06 08:21:03 Train Epoch 051:  36%|███▌      | 900/2502 [24:55<44:12,  1.66s/it, Loss=3.2664, Top1=N/A, LR=0.045392]   2025-11-06 08:23:48,889 - INFO - Step 128502: {'train_loss_batch': 4.1342315673828125, 'train_lr': 0.04539204336863223, 'batch_time': 1.6596258895907894, 'data_time': 0.00498009521873889}
2025-11-06 08:23:48 Train Epoch 051:  40%|███▉      | 1000/2502 [27:40<41:05,  1.64s/it, Loss=3.2559, Top1=N/A, LR=0.045392]2025-11-06 08:26:33,876 - INFO - Step 128602: {'train_loss_batch': 2.990185022354126, 'train_lr': 0.04539204336863223, 'batch_time': 1.6586501426868268, 'data_time': 0.00458714630934861}
2025-11-06 08:26:33 Train Epoch 051:  44%|████▍     | 1100/2502 [30:25<38:35,  1.65s/it, Loss=3.2598, Top1=N/A, LR=0.045392]2025-11-06 08:29:18,857 - INFO - Step 128702: {'train_loss_batch': 4.0664896965026855, 'train_lr': 0.04539204336863223, 'batch_time': 1.6578475084226854, 'data_time': 0.00426683244003587}
2025-11-06 08:29:18 Train Epoch 051:  48%|████▊     | 1200/2502 [33:11<35:59,  1.66s/it, Loss=3.2687, Top1=N/A, LR=0.045392]2025-11-06 08:32:04,710 - INFO - Step 128802: {'train_loss_batch': 4.275023460388184, 'train_lr': 0.04539204336863223, 'batch_time': 1.6579043980343555, 'data_time': 0.004001060790761524}
2025-11-06 08:32:04 Train Epoch 051:  52%|█████▏    | 1300/2502 [35:56<33:15,  1.66s/it, Loss=3.2703, Top1=61.84%, LR=0.045392]2025-11-06 08:34:50,408 - INFO - Step 128902: {'train_loss_batch': 2.4735143184661865, 'train_lr': 0.04539204336863223, 'batch_time': 1.6578327550235663, 'data_time': 0.003775530829052115}
2025-11-06 08:34:50 Train Epoch 051:  56%|█████▌    | 1400/2502 [38:42<30:23,  1.65s/it, Loss=3.2664, Top1=N/A, LR=0.045392]   2025-11-06 08:37:36,225 - INFO - Step 129002: {'train_loss_batch': 2.536618709564209, 'train_lr': 0.04539204336863223, 'batch_time': 1.657856772747489, 'data_time': 0.003583300037098135}
2025-11-06 08:37:36 Train Epoch 051:  60%|█████▉    | 1500/2502 [41:27<27:18,  1.64s/it, Loss=3.2644, Top1=61.75%, LR=0.045392]2025-11-06 08:40:21,471 - INFO - Step 129102: {'train_loss_batch': 2.5937490463256836, 'train_lr': 0.04539204336863223, 'batch_time': 1.6574969655430214, 'data_time': 0.003412930509553283}
2025-11-06 08:40:21 Train Epoch 051:  64%|██████▍   | 1600/2502 [44:11<24:53,  1.66s/it, Loss=3.2647, Top1=N/A, LR=0.045392]   2025-11-06 08:43:05,372 - INFO - Step 129202: {'train_loss_batch': 3.8531479835510254, 'train_lr': 0.04539204336863223, 'batch_time': 1.6563422647139046, 'data_time': 0.003264589506264853}
2025-11-06 08:43:05 Train Epoch 051:  68%|██████▊   | 1700/2502 [46:56<22:09,  1.66s/it, Loss=3.2676, Top1=N/A, LR=0.045392]2025-11-06 08:45:49,907 - INFO - Step 129302: {'train_loss_batch': 4.930642127990723, 'train_lr': 0.04539204336863223, 'batch_time': 1.6556959302196919, 'data_time': 0.0031318262280750665}
2025-11-06 08:45:49 Train Epoch 051:  72%|███████▏  | 1800/2502 [49:41<19:21,  1.65s/it, Loss=3.2619, Top1=N/A, LR=0.045392]2025-11-06 08:48:34,894 - INFO - Step 129402: {'train_loss_batch': 2.570138454437256, 'train_lr': 0.04539204336863223, 'batch_time': 1.6553719467351597, 'data_time': 0.003013820531697885}
2025-11-06 08:48:34 Train Epoch 051:  76%|███████▌  | 1900/2502 [52:26<16:30,  1.65s/it, Loss=3.2585, Top1=N/A, LR=0.045392]2025-11-06 08:51:20,163 - INFO - Step 129502: {'train_loss_batch': 3.677828550338745, 'train_lr': 0.04539204336863223, 'batch_time': 1.6552309599880919, 'data_time': 0.002910484332024957}
2025-11-06 08:51:20 Train Epoch 051:  80%|███████▉  | 2000/2502 [55:11<13:45,  1.64s/it, Loss=3.2610, Top1=N/A, LR=0.045392]2025-11-06 08:54:05,528 - INFO - Step 129602: {'train_loss_batch': 4.638025283813477, 'train_lr': 0.04539204336863223, 'batch_time': 1.6551517649092475, 'data_time': 0.0028173761448819657}
2025-11-06 08:54:05 Train Epoch 051:  84%|████████▍ | 2100/2502 [57:56<10:59,  1.64s/it, Loss=3.2578, Top1=61.68%, LR=0.045392]2025-11-06 08:56:50,297 - INFO - Step 129702: {'train_loss_batch': 2.5271499156951904, 'train_lr': 0.04539204336863223, 'batch_time': 1.6547967413957205, 'data_time': 0.002730203094736614}
2025-11-06 08:56:50 Train Epoch 051:  88%|████████▊ | 2200/2502 [1:00:42<08:19,  1.65s/it, Loss=3.2501, Top1=N/A, LR=0.045392]   2025-11-06 08:59:35,675 - INFO - Step 129802: {'train_loss_batch': 2.8248910903930664, 'train_lr': 0.04539204336863223, 'batch_time': 1.6547503644691062, 'data_time': 0.0026547723550896167}
2025-11-06 08:59:35 Train Epoch 051:  92%|█████████▏| 2300/2502 [1:03:27<05:33,  1.65s/it, Loss=3.2506, Top1=N/A, LR=0.045392]2025-11-06 09:02:21,234 - INFO - Step 129902: {'train_loss_batch': 2.5774340629577637, 'train_lr': 0.04539204336863223, 'batch_time': 1.6547864461556459, 'data_time': 0.0025834038380900966}
2025-11-06 09:02:21 Train Epoch 051:  96%|█████████▌| 2400/2502 [1:06:12<02:47,  1.65s/it, Loss=3.2536, Top1=61.58%, LR=0.045392]2025-11-06 09:05:05,759 - INFO - Step 130002: {'train_loss_batch': 2.5791015625, 'train_lr': 0.04539204336863223, 'batch_time': 1.654389486666373, 'data_time': 0.002518111693268267}
2025-11-06 09:05:05 Train Epoch 051: 100%|█████████▉| 2500/2502 [1:08:57<00:03,  1.65s/it, Loss=3.2557, Top1=N/A, LR=0.045392]   2025-11-06 09:07:51,278 - INFO - Step 130102: {'train_loss_batch': 4.552892684936523, 'train_lr': 0.04539204336863223, 'batch_time': 1.654421600900808, 'data_time': 0.002479580582165327}
2025-11-06 09:07:51 Train Epoch 051: 100%|██████████| 2502/2502 [1:08:59<00:00,  1.65s/it, Loss=3.2557, Top1=N/A, LR=0.045392]
2025-11-06 09:07:53 Val Epoch 051:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 09:07:58   with torch.cuda.amp.autocast():
2025-11-06 09:07:58 Val Epoch 051: 100%|██████████| 98/98 [01:52<00:00,  1.15s/it, Loss=2.3056, Top1=66.12%, Top5=87.60%]
2025-11-06 09:09:46 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 09:09:46   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 09:09:46 2025-11-06 09:09:46,108 - INFO - Step 51: {'epoch': 51, 'learning_rate': 0.043555684169572506, 'train_loss': 3.256018339301185, 'train_top1': 61.585254854368934, 'train_top5': 83.00591626213593, 'train_precision': 61.3302445642706, 'train_recall': 61.42337987144791, 'train_f1': 61.18392827283553, 'val_loss': 2.3056029106140135, 'val_top1': 66.12199998046874, 'val_top5': 87.60399997558594, 'val_precision': 68.24992333040016, 'val_recall': 66.12599999999999, 'val_f1': 65.67722987046564}
2025-11-06 09:09:46 2025-11-06 09:09:46,109 - INFO - Epoch 051 Summary - LR: 0.043556, Train Loss: 3.2560, Val Loss: 2.3056, Val F1: 65.68%, Val Precision: 68.25%, Val Recall: 66.13%
2025-11-06 09:09:49 2025-11-06 09:09:49,066 - INFO - New best model saved with validation accuracy: 66.122%
2025-11-06 09:09:49 2025-11-06 09:09:49,067 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_052.pth
2025-11-06 09:09:49 Train Epoch 052:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 51 that is less than the current step 130102. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 09:09:52 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 09:09:52   with torch.cuda.amp.autocast():
2025-11-06 09:09:54 Train Epoch 052:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.5151, Top1=N/A, LR=0.043556]2025-11-06 09:09:54,478 - INFO - Step 130104: {'train_loss_batch': 2.515076160430908, 'train_lr': 0.043555684169572506, 'batch_time': 5.409244775772095, 'data_time': 3.7676990032196045}
2025-11-06 09:09:54 Train Epoch 052:   4%|▍         | 100/2502 [02:51<1:06:24,  1.66s/it, Loss=3.2663, Top1=N/A, LR=0.043556]2025-11-06 09:12:40,614 - INFO - Step 130204: {'train_loss_batch': 2.3794023990631104, 'train_lr': 0.043555684169572506, 'batch_time': 1.6984718506879146, 'data_time': 0.03842854971932893}
2025-11-06 09:12:40 Train Epoch 052:   8%|▊         | 200/2502 [05:36<1:03:25,  1.65s/it, Loss=3.2881, Top1=N/A, LR=0.043556]2025-11-06 09:15:25,719 - INFO - Step 130304: {'train_loss_batch': 4.787130832672119, 'train_lr': 0.043555684169572506, 'batch_time': 1.6748778357434628, 'data_time': 0.019823707751373747}
2025-11-06 09:15:25 Train Epoch 052:  12%|█▏        | 300/2502 [08:22<1:00:51,  1.66s/it, Loss=3.2439, Top1=N/A, LR=0.043556]2025-11-06 09:18:11,348 - INFO - Step 130404: {'train_loss_batch': 4.085217475891113, 'train_lr': 0.043555684169572506, 'batch_time': 1.6687016209890677, 'data_time': 0.013575202998924889}
2025-11-06 09:18:11 Train Epoch 052:  16%|█▌        | 400/2502 [11:08<58:17,  1.66s/it, Loss=3.2172, Top1=62.09%, LR=0.043556]2025-11-06 09:20:57,231 - INFO - Step 130504: {'train_loss_batch': 2.5275702476501465, 'train_lr': 0.043555684169572506, 'batch_time': 1.666238909053089, 'data_time': 0.010451108737479422}
2025-11-06 09:20:57 Train Epoch 052:  20%|█▉        | 500/2502 [13:53<55:00,  1.65s/it, Loss=3.2252, Top1=62.14%, LR=0.043556]2025-11-06 09:23:42,403 - INFO - Step 130604: {'train_loss_batch': 2.561728000640869, 'train_lr': 0.043555684169572506, 'batch_time': 1.663341693059651, 'data_time': 0.00856792474697212}
2025-11-06 09:23:42 Train Epoch 052:  24%|██▍       | 600/2502 [16:38<52:42,  1.66s/it, Loss=3.2086, Top1=N/A, LR=0.043556]   2025-11-06 09:26:28,041 - INFO - Step 130704: {'train_loss_batch': 3.8430099487304688, 'train_lr': 0.043555684169572506, 'batch_time': 1.6621826571751752, 'data_time': 0.007316664729062809}
2025-11-06 09:26:28 Train Epoch 052:  28%|██▊       | 700/2502 [19:24<50:01,  1.67s/it, Loss=3.1991, Top1=N/A, LR=0.043556]2025-11-06 09:29:13,649 - INFO - Step 130804: {'train_loss_batch': 4.052825927734375, 'train_lr': 0.043555684169572506, 'batch_time': 1.6613115359645088, 'data_time': 0.006419070607075168}
2025-11-06 09:29:13 Train Epoch 052:  32%|███▏      | 800/2502 [22:10<46:42,  1.65s/it, Loss=3.2261, Top1=62.19%, LR=0.043556]2025-11-06 09:31:59,165 - INFO - Step 130904: {'train_loss_batch': 2.5106091499328613, 'train_lr': 0.043555684169572506, 'batch_time': 1.6605432402626257, 'data_time': 0.005751018071740159}
2025-11-06 09:31:59 Train Epoch 052:  36%|███▌      | 900/2502 [24:55<44:26,  1.66s/it, Loss=3.2173, Top1=N/A, LR=0.043556]   2025-11-06 09:34:44,510 - INFO - Step 131004: {'train_loss_batch': 3.8721818923950195, 'train_lr': 0.043555684169572506, 'batch_time': 1.6597559261533714, 'data_time': 0.0052248373148047035}
2025-11-06 09:34:44 Train Epoch 052:  40%|███▉      | 1000/2502 [27:40<41:39,  1.66s/it, Loss=3.2319, Top1=N/A, LR=0.043556]2025-11-06 09:37:29,911 - INFO - Step 131104: {'train_loss_batch': 2.549760341644287, 'train_lr': 0.043555684169572506, 'batch_time': 1.6591815117236737, 'data_time': 0.004802844622037508}
2025-11-06 09:37:30 Train Epoch 052:  44%|████▍     | 1100/2502 [30:25<38:51,  1.66s/it, Loss=3.2335, Top1=N/A, LR=0.043556]2025-11-06 09:40:14,942 - INFO - Step 131204: {'train_loss_batch': 3.1138148307800293, 'train_lr': 0.043555684169572506, 'batch_time': 1.6583753060904771, 'data_time': 0.004456701114110574}
2025-11-06 09:40:15 Train Epoch 052:  48%|████▊     | 1200/2502 [33:11<35:46,  1.65s/it, Loss=3.2313, Top1=N/A, LR=0.043556]2025-11-06 09:43:00,446 - INFO - Step 131304: {'train_loss_batch': 2.4769647121429443, 'train_lr': 0.043555684169572506, 'batch_time': 1.6580975911301636, 'data_time': 0.004175253454394186}
2025-11-06 09:43:00 Train Epoch 052:  52%|█████▏    | 1300/2502 [35:56<33:10,  1.66s/it, Loss=3.2315, Top1=N/A, LR=0.043556]2025-11-06 09:45:45,483 - INFO - Step 131404: {'train_loss_batch': 4.079581260681152, 'train_lr': 0.043555684169572506, 'batch_time': 1.657503544597054, 'data_time': 0.003931997740479454}
2025-11-06 09:45:45 Train Epoch 052:  56%|█████▌    | 1400/2502 [38:42<30:30,  1.66s/it, Loss=3.2322, Top1=N/A, LR=0.043556]2025-11-06 09:48:31,389 - INFO - Step 131504: {'train_loss_batch': 2.668304443359375, 'train_lr': 0.043555684169572506, 'batch_time': 1.6576143932887097, 'data_time': 0.003726877883704197}
2025-11-06 09:48:31 Train Epoch 052:  60%|█████▉    | 1500/2502 [41:27<27:40,  1.66s/it, Loss=3.2423, Top1=N/A, LR=0.043556]2025-11-06 09:51:16,907 - INFO - Step 131604: {'train_loss_batch': 3.667602062225342, 'train_lr': 0.043555684169572506, 'batch_time': 1.6574517779950695, 'data_time': 0.003547494527421579}
2025-11-06 09:51:16 Train Epoch 052:  64%|██████▍   | 1600/2502 [44:12<24:47,  1.65s/it, Loss=3.2401, Top1=62.09%, LR=0.043556]2025-11-06 09:54:02,015 - INFO - Step 131704: {'train_loss_batch': 2.50545597076416, 'train_lr': 0.043555684169572506, 'batch_time': 1.657053400470941, 'data_time': 0.003388661432832126}
2025-11-06 09:54:02 Train Epoch 052:  68%|██████▊   | 1700/2502 [46:58<22:05,  1.65s/it, Loss=3.2414, Top1=N/A, LR=0.043556]   2025-11-06 09:56:47,768 - INFO - Step 131804: {'train_loss_batch': 4.224582672119141, 'train_lr': 0.043555684169572506, 'batch_time': 1.6570815284556322, 'data_time': 0.003249346544152495}
2025-11-06 09:56:47 Train Epoch 052:  72%|███████▏  | 1800/2502 [49:44<19:18,  1.65s/it, Loss=3.2400, Top1=N/A, LR=0.043556]2025-11-06 09:59:33,311 - INFO - Step 131904: {'train_loss_batch': 4.171750068664551, 'train_lr': 0.043555684169572506, 'batch_time': 1.6569899180940229, 'data_time': 0.003124308016881355}
2025-11-06 09:59:33 Train Epoch 052:  76%|███████▌  | 1900/2502 [52:29<16:35,  1.65s/it, Loss=3.2338, Top1=N/A, LR=0.043556]2025-11-06 10:02:18,987 - INFO - Step 132004: {'train_loss_batch': 2.7766823768615723, 'train_lr': 0.043555684169572506, 'batch_time': 1.6569772288399205, 'data_time': 0.00301293195516796}
2025-11-06 10:02:18 Train Epoch 052:  80%|███████▉  | 2000/2502 [55:14<13:48,  1.65s/it, Loss=3.2332, Top1=N/A, LR=0.043556]2025-11-06 10:05:04,007 - INFO - Step 132104: {'train_loss_batch': 4.189186096191406, 'train_lr': 0.043555684169572506, 'batch_time': 1.656639061827233, 'data_time': 0.0029140097805406377}
2025-11-06 10:05:04 Train Epoch 052:  84%|████████▍ | 2100/2502 [58:00<11:03,  1.65s/it, Loss=3.2315, Top1=N/A, LR=0.043556]2025-11-06 10:07:49,088 - INFO - Step 132204: {'train_loss_batch': 4.062241077423096, 'train_lr': 0.043555684169572506, 'batch_time': 1.6563611363070967, 'data_time': 0.002823998166628533}
2025-11-06 10:07:49 Train Epoch 052:  88%|████████▊ | 2200/2502 [1:00:45<08:21,  1.66s/it, Loss=3.2331, Top1=N/A, LR=0.043556]2025-11-06 10:10:34,396 - INFO - Step 132304: {'train_loss_batch': 4.282055854797363, 'train_lr': 0.043555684169572506, 'batch_time': 1.6562119938470405, 'data_time': 0.0027418880774616274}
2025-11-06 10:10:34 Train Epoch 052:  92%|█████████▏| 2300/2502 [1:03:30<05:35,  1.66s/it, Loss=3.2344, Top1=N/A, LR=0.043556]2025-11-06 10:13:19,666 - INFO - Step 132404: {'train_loss_batch': 4.698708534240723, 'train_lr': 0.043555684169572506, 'batch_time': 1.6560590963475552, 'data_time': 0.0026700213804083353}
2025-11-06 10:13:19 Train Epoch 052:  96%|█████████▌| 2400/2502 [1:06:16<02:48,  1.65s/it, Loss=3.2370, Top1=N/A, LR=0.043556]2025-11-06 10:16:05,326 - INFO - Step 132504: {'train_loss_batch': 2.4473984241485596, 'train_lr': 0.043555684169572506, 'batch_time': 1.6560815169085368, 'data_time': 0.002601467038829443}
2025-11-06 10:16:05 Train Epoch 052: 100%|█████████▉| 2500/2502 [1:09:02<00:03,  1.67s/it, Loss=3.2396, Top1=61.96%, LR=0.043556]2025-11-06 10:18:51,177 - INFO - Step 132604: {'train_loss_batch': 2.502472162246704, 'train_lr': 0.043555684169572506, 'batch_time': 1.6561789605103698, 'data_time': 0.0025984892031041588}
2025-11-06 10:18:51 Train Epoch 052: 100%|██████████| 2502/2502 [1:09:03<00:00,  1.66s/it, Loss=3.2396, Top1=61.96%, LR=0.043556]
2025-11-06 10:18:53 Val Epoch 052:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 10:18:57   with torch.cuda.amp.autocast():
2025-11-06 10:18:58 Val Epoch 052: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=2.3503, Top1=64.14%, Top5=86.40%]
2025-11-06 10:20:44 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 10:20:44   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 10:20:44 2025-11-06 10:20:44,069 - INFO - Step 52: {'epoch': 52, 'learning_rate': 0.04172813396017443, 'train_loss': 3.2399713999742894, 'train_top1': 61.958414713541664, 'train_top5': 83.36995442708333, 'train_precision': 61.757800295060605, 'train_recall': 61.84746782650411, 'train_f1': 61.60102041546786, 'val_loss': 2.3503196814727785, 'val_top1': 64.14199997314454, 'val_top5': 86.39599997070313, 'val_precision': 67.06148352245319, 'val_recall': 64.13799999999999, 'val_f1': 63.80741732464291}
2025-11-06 10:20:44 2025-11-06 10:20:44,070 - INFO - Epoch 052 Summary - LR: 0.041728, Train Loss: 3.2400, Val Loss: 2.3503, Val F1: 63.81%, Val Precision: 67.06%, Val Recall: 64.14%
2025-11-06 10:20:44 Train Epoch 053:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 10:20:48   with torch.cuda.amp.autocast():
2025-11-06 10:20:49 Train Epoch 053:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.2151, Top1=N/A, LR=0.041728]2025-11-06 10:20:49,883 - INFO - Step 132606: {'train_loss_batch': 3.2151052951812744, 'train_lr': 0.04172813396017443, 'batch_time': 5.399170160293579, 'data_time': 3.7522881031036377}
2025-11-06 10:20:49 Train Epoch 053:   0%|          | 1/2502 [00:05<3:45:12,  5.40s/it, Loss=3.2151, Top1=N/A, LR=0.041728]wandb: WARNING Tried to log to step 52 that is less than the current step 132604. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 10:20:51 Train Epoch 053:   4%|▍         | 100/2502 [02:50<1:06:21,  1.66s/it, Loss=3.2012, Top1=N/A, LR=0.041728]2025-11-06 10:23:34,991 - INFO - Step 132706: {'train_loss_batch': 2.8669214248657227, 'train_lr': 0.04172813396017443, 'batch_time': 1.6881881798848066, 'data_time': 0.0382119617839851}
2025-11-06 10:23:34 Train Epoch 053:   8%|▊         | 200/2502 [05:35<1:03:47,  1.66s/it, Loss=3.2327, Top1=N/A, LR=0.041728]2025-11-06 10:26:19,592 - INFO - Step 132806: {'train_loss_batch': 2.5465261936187744, 'train_lr': 0.04172813396017443, 'batch_time': 1.6672017372662749, 'data_time': 0.019687449754174077}
2025-11-06 10:26:19 Train Epoch 053:  12%|█▏        | 300/2502 [08:20<1:00:39,  1.65s/it, Loss=3.2396, Top1=N/A, LR=0.041728]2025-11-06 10:29:04,583 - INFO - Step 132906: {'train_loss_batch': 3.507204294204712, 'train_lr': 0.04172813396017443, 'batch_time': 1.6614573714741045, 'data_time': 0.013468036619927796}
2025-11-06 10:29:04 Train Epoch 053:  16%|█▌        | 400/2502 [11:05<58:02,  1.66s/it, Loss=3.2390, Top1=N/A, LR=0.041728]2025-11-06 10:31:50,388 - INFO - Step 133006: {'train_loss_batch': 4.466641902923584, 'train_lr': 0.04172813396017443, 'batch_time': 1.6606074253519871, 'data_time': 0.010344557631342786}
2025-11-06 10:31:50 Train Epoch 053:  20%|█▉        | 500/2502 [13:51<55:15,  1.66s/it, Loss=3.2301, Top1=N/A, LR=0.041728]2025-11-06 10:34:35,866 - INFO - Step 133106: {'train_loss_batch': 2.5107083320617676, 'train_lr': 0.04172813396017443, 'batch_time': 1.6594438862181946, 'data_time': 0.00848962494475161}
2025-11-06 10:34:35 Train Epoch 053:  24%|██▍       | 600/2502 [16:37<52:34,  1.66s/it, Loss=3.2517, Top1=62.19%, LR=0.041728]2025-11-06 10:37:21,731 - INFO - Step 133206: {'train_loss_batch': 2.4806816577911377, 'train_lr': 0.04172813396017443, 'batch_time': 1.6593108169250996, 'data_time': 0.0072458060132881965}
2025-11-06 10:37:21 Train Epoch 053:  28%|██▊       | 700/2502 [19:22<49:56,  1.66s/it, Loss=3.2546, Top1=N/A, LR=0.041728]   2025-11-06 10:40:07,360 - INFO - Step 133306: {'train_loss_batch': 2.7411677837371826, 'train_lr': 0.04172813396017443, 'batch_time': 1.6588807997111756, 'data_time': 0.006359634657899255}
2025-11-06 10:40:07 Train Epoch 053:  32%|███▏      | 800/2502 [22:08<46:48,  1.65s/it, Loss=3.2449, Top1=N/A, LR=0.041728]2025-11-06 10:42:52,775 - INFO - Step 133406: {'train_loss_batch': 4.252599716186523, 'train_lr': 0.04172813396017443, 'batch_time': 1.6582891676161025, 'data_time': 0.005689986487304078}
2025-11-06 10:42:52 Train Epoch 053:  36%|███▌      | 900/2502 [24:53<44:21,  1.66s/it, Loss=3.2489, Top1=N/A, LR=0.041728]2025-11-06 10:45:37,831 - INFO - Step 133506: {'train_loss_batch': 2.924433946609497, 'train_lr': 0.04172813396017443, 'batch_time': 1.6574314130133185, 'data_time': 0.005170586900361767}
2025-11-06 10:45:37 Train Epoch 053:  40%|███▉      | 1000/2502 [27:38<41:31,  1.66s/it, Loss=3.2447, Top1=N/A, LR=0.041728]2025-11-06 10:48:23,079 - INFO - Step 133606: {'train_loss_batch': 3.9088711738586426, 'train_lr': 0.04172813396017443, 'batch_time': 1.6569358206890918, 'data_time': 0.00475428797505595}
2025-11-06 10:48:23 Train Epoch 053:  44%|████▍     | 1100/2502 [30:24<38:55,  1.67s/it, Loss=3.2399, Top1=N/A, LR=0.041728]2025-11-06 10:51:08,882 - INFO - Step 133706: {'train_loss_batch': 2.5260963439941406, 'train_lr': 0.04172813396017443, 'batch_time': 1.6570351762624353, 'data_time': 0.004416593089090706}
2025-11-06 10:51:08 Train Epoch 053:  48%|████▊     | 1200/2502 [33:09<36:01,  1.66s/it, Loss=3.2503, Top1=N/A, LR=0.041728]2025-11-06 10:53:53,949 - INFO - Step 133806: {'train_loss_batch': 2.4578804969787598, 'train_lr': 0.04172813396017443, 'batch_time': 1.6565054086324675, 'data_time': 0.0041302215249016325}
2025-11-06 10:53:53 Train Epoch 053:  52%|█████▏    | 1300/2502 [35:54<33:01,  1.65s/it, Loss=3.2526, Top1=N/A, LR=0.041728]2025-11-06 10:56:39,071 - INFO - Step 133906: {'train_loss_batch': 2.7562880516052246, 'train_lr': 0.04172813396017443, 'batch_time': 1.656098382643422, 'data_time': 0.003891330971157065}
2025-11-06 10:56:39 Train Epoch 053:  56%|█████▌    | 1400/2502 [38:40<30:30,  1.66s/it, Loss=3.2512, Top1=N/A, LR=0.041728]2025-11-06 10:59:24,582 - INFO - Step 134006: {'train_loss_batch': 4.053884983062744, 'train_lr': 0.04172813396017443, 'batch_time': 1.6560279591265616, 'data_time': 0.003685355271551117}
2025-11-06 10:59:24 Train Epoch 053:  60%|█████▉    | 1500/2502 [41:25<27:49,  1.67s/it, Loss=3.2467, Top1=N/A, LR=0.041728]2025-11-06 11:02:09,746 - INFO - Step 134106: {'train_loss_batch': 2.4798481464385986, 'train_lr': 0.04172813396017443, 'batch_time': 1.6557355046192859, 'data_time': 0.0035063380165786286}
2025-11-06 11:02:09 Train Epoch 053:  64%|██████▍   | 1600/2502 [44:10<24:57,  1.66s/it, Loss=3.2465, Top1=N/A, LR=0.041728]2025-11-06 11:04:54,659 - INFO - Step 134206: {'train_loss_batch': 2.790701150894165, 'train_lr': 0.04172813396017443, 'batch_time': 1.6553230477749445, 'data_time': 0.0033472456982700173}
2025-11-06 11:04:54 Train Epoch 053:  68%|██████▊   | 1700/2502 [46:55<21:56,  1.64s/it, Loss=3.2419, Top1=N/A, LR=0.041728]2025-11-06 11:07:39,991 - INFO - Step 134306: {'train_loss_batch': 3.0188541412353516, 'train_lr': 0.04172813396017443, 'batch_time': 1.6552046071354745, 'data_time': 0.003213222555242378}
2025-11-06 11:07:39 Train Epoch 053:  72%|███████▏  | 1800/2502 [49:40<19:24,  1.66s/it, Loss=3.2497, Top1=N/A, LR=0.041728]2025-11-06 11:10:25,345 - INFO - Step 134406: {'train_loss_batch': 2.426870107650757, 'train_lr': 0.04172813396017443, 'batch_time': 1.6551126014650166, 'data_time': 0.0030913270889951546}
2025-11-06 11:10:25 Train Epoch 053:  76%|███████▌  | 1900/2502 [52:26<16:41,  1.66s/it, Loss=3.2458, Top1=N/A, LR=0.041728]2025-11-06 11:13:11,014 - INFO - Step 134506: {'train_loss_batch': 2.5834736824035645, 'train_lr': 0.04172813396017443, 'batch_time': 1.6551953562305577, 'data_time': 0.002980066562313459}
2025-11-06 11:13:11 Train Epoch 053:  80%|███████▉  | 2000/2502 [55:10<13:49,  1.65s/it, Loss=3.2587, Top1=N/A, LR=0.041728]2025-11-06 11:15:54,982 - INFO - Step 134606: {'train_loss_batch': 2.5431432723999023, 'train_lr': 0.04172813396017443, 'batch_time': 1.6544196554447996, 'data_time': 0.0028839074391713445}
2025-11-06 11:15:54 Train Epoch 053:  84%|████████▍ | 2100/2502 [57:55<11:04,  1.65s/it, Loss=3.2565, Top1=N/A, LR=0.041728]2025-11-06 11:18:39,946 - INFO - Step 134706: {'train_loss_batch': 3.912216901779175, 'train_lr': 0.04172813396017443, 'batch_time': 1.6541924175905194, 'data_time': 0.0027947665282171833}
2025-11-06 11:18:39 Train Epoch 053:  88%|████████▊ | 2200/2502 [1:00:41<08:19,  1.65s/it, Loss=3.2516, Top1=N/A, LR=0.041728]2025-11-06 11:21:25,533 - INFO - Step 134806: {'train_loss_batch': 2.4959962368011475, 'train_lr': 0.04172813396017443, 'batch_time': 1.6542683531619484, 'data_time': 0.002714054089467778}
2025-11-06 11:21:25 Train Epoch 053:  92%|█████████▏| 2300/2502 [1:03:26<05:36,  1.66s/it, Loss=3.2488, Top1=N/A, LR=0.041728]2025-11-06 11:24:10,713 - INFO - Step 134906: {'train_loss_batch': 2.4069252014160156, 'train_lr': 0.04172813396017443, 'batch_time': 1.654161012364387, 'data_time': 0.0026385861860571194}
2025-11-06 11:24:10 Train Epoch 053:  96%|█████████▌| 2400/2502 [1:06:11<02:48,  1.65s/it, Loss=3.2522, Top1=62.09%, LR=0.041728]2025-11-06 11:26:55,506 - INFO - Step 135006: {'train_loss_batch': 2.616175651550293, 'train_lr': 0.04172813396017443, 'batch_time': 1.6539011795984513, 'data_time': 0.002570940623031165}
2025-11-06 11:26:55 Train Epoch 053: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.66s/it, Loss=3.2491, Top1=N/A, LR=0.041728]   2025-11-06 11:29:41,054 - INFO - Step 135106: {'train_loss_batch': 4.101229667663574, 'train_lr': 0.04172813396017443, 'batch_time': 1.6539646384716988, 'data_time': 0.0025332696625634415}
2025-11-06 11:29:41 Train Epoch 053: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=3.2491, Top1=N/A, LR=0.041728]
2025-11-06 11:29:43 Val Epoch 053:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 11:29:47   with torch.cuda.amp.autocast():
2025-11-06 11:29:48 Val Epoch 053: 100%|██████████| 98/98 [01:53<00:00,  1.16s/it, Loss=2.3254, Top1=64.95%, Top5=87.04%]
2025-11-06 11:31:37 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 11:31:37   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 11:31:37 2025-11-06 11:31:37,092 - INFO - Step 53: {'epoch': 53, 'learning_rate': 0.03991188895419528, 'train_loss': 3.249355426795191, 'train_top1': 62.09462691326531, 'train_top5': 83.42952806122449, 'train_precision': 61.89219099512356, 'train_recall': 61.962177300653906, 'train_f1': 61.737457778254836, 'val_loss': 2.325397427597046, 'val_top1': 64.95400000732423, 'val_top5': 87.03999998291016, 'val_precision': 67.46355552768026, 'val_recall': 64.946, 'val_f1': 64.5477053319252}
2025-11-06 11:31:37 2025-11-06 11:31:37,094 - INFO - Epoch 053 Summary - LR: 0.039912, Train Loss: 3.2494, Val Loss: 2.3254, Val F1: 64.55%, Val Precision: 67.46%, Val Recall: 64.95%
2025-11-06 11:31:37 Train Epoch 054:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 53 that is less than the current step 135106. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 11:31:41 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 11:31:41   with torch.cuda.amp.autocast():
2025-11-06 11:31:42 Train Epoch 054:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.8774, Top1=N/A, LR=0.039912]2025-11-06 11:31:42,931 - INFO - Step 135108: {'train_loss_batch': 4.877448558807373, 'train_lr': 0.03991188895419528, 'batch_time': 5.41484808921814, 'data_time': 3.7783732414245605}
2025-11-06 11:31:42 Train Epoch 054:   4%|▍         | 100/2502 [02:49<1:05:25,  1.63s/it, Loss=3.1779, Top1=63.43%, LR=0.039912]2025-11-06 11:34:27,339 - INFO - Step 135208: {'train_loss_batch': 2.4775547981262207, 'train_lr': 0.03991188895419528, 'batch_time': 1.6814256044897702, 'data_time': 0.03847542847737227}
2025-11-06 11:34:27 Train Epoch 054:   8%|▊         | 200/2502 [05:34<1:03:35,  1.66s/it, Loss=3.1303, Top1=N/A, LR=0.039912]   2025-11-06 11:37:12,437 - INFO - Step 135308: {'train_loss_batch': 3.715118885040283, 'train_lr': 0.03991188895419528, 'batch_time': 1.6662793301824312, 'data_time': 0.019808967315142426}
2025-11-06 11:37:12 Train Epoch 054:  12%|█▏        | 300/2502 [08:20<1:00:59,  1.66s/it, Loss=3.1869, Top1=N/A, LR=0.039912]2025-11-06 11:39:58,035 - INFO - Step 135408: {'train_loss_batch': 4.986209392547607, 'train_lr': 0.03991188895419528, 'batch_time': 1.662856253278612, 'data_time': 0.013562578695557046}
2025-11-06 11:39:58 Train Epoch 054:  16%|█▌        | 400/2502 [11:05<57:56,  1.65s/it, Loss=3.1775, Top1=N/A, LR=0.039912]2025-11-06 11:42:43,271 - INFO - Step 135508: {'train_loss_batch': 4.919729232788086, 'train_lr': 0.03991188895419528, 'batch_time': 1.6602391964776855, 'data_time': 0.010427531458790463}
2025-11-06 11:42:43 Train Epoch 054:  20%|█▉        | 500/2502 [13:50<55:21,  1.66s/it, Loss=3.1971, Top1=N/A, LR=0.039912]2025-11-06 11:45:28,465 - INFO - Step 135608: {'train_loss_batch': 4.641388893127441, 'train_lr': 0.03991188895419528, 'batch_time': 1.6585818983599574, 'data_time': 0.00856065797710609}
2025-11-06 11:45:28 Train Epoch 054:  24%|██▍       | 600/2502 [16:36<52:23,  1.65s/it, Loss=3.2163, Top1=N/A, LR=0.039912]2025-11-06 11:48:13,591 - INFO - Step 135708: {'train_loss_batch': 2.6952171325683594, 'train_lr': 0.03991188895419528, 'batch_time': 1.6573627502866672, 'data_time': 0.007303164524960637}
2025-11-06 11:48:13 Train Epoch 054:  28%|██▊       | 700/2502 [19:21<49:30,  1.65s/it, Loss=3.2102, Top1=N/A, LR=0.039912]2025-11-06 11:50:59,392 - INFO - Step 135808: {'train_loss_batch': 4.2332763671875, 'train_lr': 0.03991188895419528, 'batch_time': 1.65745466411879, 'data_time': 0.0064065599237461066}
2025-11-06 11:50:59 Train Epoch 054:  32%|███▏      | 800/2502 [22:07<46:50,  1.65s/it, Loss=3.2347, Top1=N/A, LR=0.039912]2025-11-06 11:53:44,590 - INFO - Step 135908: {'train_loss_batch': 2.435427665710449, 'train_lr': 0.03991188895419528, 'batch_time': 1.656771426790216, 'data_time': 0.005729783340339804}
2025-11-06 11:53:44 Train Epoch 054:  36%|███▌      | 900/2502 [24:52<44:11,  1.66s/it, Loss=3.2464, Top1=N/A, LR=0.039912]2025-11-06 11:56:30,174 - INFO - Step 136008: {'train_loss_batch': 3.6391427516937256, 'train_lr': 0.03991188895419528, 'batch_time': 1.656668216883144, 'data_time': 0.005204604812520458}
2025-11-06 11:56:30 Train Epoch 054:  40%|███▉      | 1000/2502 [27:38<41:29,  1.66s/it, Loss=3.2320, Top1=N/A, LR=0.039912]2025-11-06 11:59:15,646 - INFO - Step 136108: {'train_loss_batch': 4.181528091430664, 'train_lr': 0.03991188895419528, 'batch_time': 1.65647326792394, 'data_time': 0.00477557320456643}
2025-11-06 11:59:15 Train Epoch 054:  44%|████▍     | 1100/2502 [30:23<38:22,  1.64s/it, Loss=3.2260, Top1=N/A, LR=0.039912]2025-11-06 12:02:00,700 - INFO - Step 136208: {'train_loss_batch': 3.7173447608947754, 'train_lr': 0.03991188895419528, 'batch_time': 1.6559339656708567, 'data_time': 0.004429372844644073}
2025-11-06 12:02:00 Train Epoch 054:  48%|████▊     | 1200/2502 [33:08<35:52,  1.65s/it, Loss=3.2277, Top1=62.71%, LR=0.039912]2025-11-06 12:04:46,198 - INFO - Step 136308: {'train_loss_batch': 2.4612998962402344, 'train_lr': 0.03991188895419528, 'batch_time': 1.655854554894961, 'data_time': 0.0041435610543282}
2025-11-06 12:04:46 Train Epoch 054:  52%|█████▏    | 1300/2502 [35:53<33:14,  1.66s/it, Loss=3.2275, Top1=N/A, LR=0.039912]   2025-11-06 12:07:31,439 - INFO - Step 136408: {'train_loss_batch': 2.5473310947418213, 'train_lr': 0.03991188895419528, 'batch_time': 1.65558963111508, 'data_time': 0.003901565743812133}
2025-11-06 12:07:31 Train Epoch 054:  56%|█████▌    | 1400/2502 [38:38<30:14,  1.65s/it, Loss=3.2289, Top1=N/A, LR=0.039912]2025-11-06 12:10:16,173 - INFO - Step 136508: {'train_loss_batch': 3.462916374206543, 'train_lr': 0.03991188895419528, 'batch_time': 1.6550008003240309, 'data_time': 0.003695999018895805}
2025-11-06 12:10:16 Train Epoch 054:  60%|█████▉    | 1500/2502 [41:24<27:30,  1.65s/it, Loss=3.2286, Top1=62.71%, LR=0.039912]2025-11-06 12:13:01,731 - INFO - Step 136608: {'train_loss_batch': 2.4791271686553955, 'train_lr': 0.03991188895419528, 'batch_time': 1.6550387773888655, 'data_time': 0.0035181350504692836}
2025-11-06 12:13:01 Train Epoch 054:  64%|██████▍   | 1600/2502 [44:09<24:52,  1.65s/it, Loss=3.2302, Top1=N/A, LR=0.039912]   2025-11-06 12:15:47,089 - INFO - Step 136708: {'train_loss_batch': 3.9013943672180176, 'train_lr': 0.03991188895419528, 'batch_time': 1.6549474883273123, 'data_time': 0.003356037253070667}
2025-11-06 12:15:47 Train Epoch 054:  68%|██████▊   | 1700/2502 [46:53<22:06,  1.65s/it, Loss=3.2306, Top1=N/A, LR=0.039912]2025-11-06 12:18:31,168 - INFO - Step 136808: {'train_loss_batch': 4.1028547286987305, 'train_lr': 0.03991188895419528, 'batch_time': 1.6541154179973927, 'data_time': 0.003212740251697}
2025-11-06 12:18:31 Train Epoch 054:  72%|███████▏  | 1800/2502 [49:39<19:15,  1.65s/it, Loss=3.2265, Top1=N/A, LR=0.039912]2025-11-06 12:21:16,610 - INFO - Step 136908: {'train_loss_batch': 3.9086861610412598, 'train_lr': 0.03991188895419528, 'batch_time': 1.6541321007560716, 'data_time': 0.003091618724825116}
2025-11-06 12:21:16 Train Epoch 054:  76%|███████▌  | 1900/2502 [52:24<16:38,  1.66s/it, Loss=3.2202, Top1=N/A, LR=0.039912]2025-11-06 12:24:02,145 - INFO - Step 137008: {'train_loss_batch': 2.63944149017334, 'train_lr': 0.03991188895419528, 'batch_time': 1.6541958808397257, 'data_time': 0.0029804800636576453}
2025-11-06 12:24:02 Train Epoch 054:  80%|███████▉  | 2000/2502 [55:09<13:40,  1.63s/it, Loss=3.2189, Top1=N/A, LR=0.039912]2025-11-06 12:26:46,640 - INFO - Step 137108: {'train_loss_batch': 3.2826390266418457, 'train_lr': 0.03991188895419528, 'batch_time': 1.6537334030118958, 'data_time': 0.0028794315801388857}
2025-11-06 12:26:46 Train Epoch 054:  84%|████████▍ | 2100/2502 [57:53<11:04,  1.65s/it, Loss=3.2173, Top1=N/A, LR=0.039912]2025-11-06 12:29:31,486 - INFO - Step 137208: {'train_loss_batch': 3.724794864654541, 'train_lr': 0.03991188895419528, 'batch_time': 1.6534825520876304, 'data_time': 0.002792277601660575}
2025-11-06 12:29:31 Train Epoch 054:  88%|████████▊ | 2200/2502 [1:00:39<08:16,  1.65s/it, Loss=3.2167, Top1=N/A, LR=0.039912]2025-11-06 12:32:16,771 - INFO - Step 137308: {'train_loss_batch': 3.3695201873779297, 'train_lr': 0.03991188895419528, 'batch_time': 1.653453985597263, 'data_time': 0.002714722658059425}
2025-11-06 12:32:16 Train Epoch 054:  92%|█████████▏| 2300/2502 [1:03:24<05:35,  1.66s/it, Loss=3.2212, Top1=62.53%, LR=0.039912]2025-11-06 12:35:02,415 - INFO - Step 137408: {'train_loss_batch': 2.4261391162872314, 'train_lr': 0.03991188895419528, 'batch_time': 1.6535835341752378, 'data_time': 0.00264188974953278}
2025-11-06 12:35:02 Train Epoch 054:  96%|█████████▌| 2400/2502 [1:06:10<02:47,  1.65s/it, Loss=3.2234, Top1=N/A, LR=0.039912]   2025-11-06 12:37:47,974 - INFO - Step 137508: {'train_loss_batch': 3.668732166290283, 'train_lr': 0.03991188895419528, 'batch_time': 1.6536668230721674, 'data_time': 0.002574140158656834}
2025-11-06 12:37:47 Train Epoch 054: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.66s/it, Loss=3.2227, Top1=N/A, LR=0.039912]2025-11-06 12:40:33,694 - INFO - Step 137608: {'train_loss_batch': 2.57804274559021, 'train_lr': 0.03991188895419528, 'batch_time': 1.653808101088178, 'data_time': 0.002547381354159996}
2025-11-06 12:40:33 Train Epoch 054: 100%|██████████| 2502/2502 [1:08:57<00:00,  1.65s/it, Loss=3.2227, Top1=N/A, LR=0.039912]
2025-11-06 12:40:35 Val Epoch 054:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 12:40:40   with torch.cuda.amp.autocast():
2025-11-06 12:40:40 Val Epoch 054: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.2635, Top1=66.77%, Top5=88.22%]
2025-11-06 12:42:27 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 12:42:27   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 12:42:27 2025-11-06 12:42:27,597 - INFO - Step 54: {'epoch': 54, 'learning_rate': 0.03810942992384429, 'train_loss': 3.2231143591024702, 'train_top1': 62.52183133732535, 'train_top5': 83.57269835329342, 'train_precision': 62.324102796220856, 'train_recall': 62.38887953702103, 'train_f1': 62.18305889355062, 'val_loss': 2.263451081008911, 'val_top1': 66.77400001953124, 'val_top5': 88.21599999023438, 'val_precision': 69.21170995191488, 'val_recall': 66.77000000000001, 'val_f1': 66.35133168149723}
2025-11-06 12:42:27 2025-11-06 12:42:27,599 - INFO - Epoch 054 Summary - LR: 0.038109, Train Loss: 3.2231, Val Loss: 2.2635, Val F1: 66.35%, Val Precision: 69.21%, Val Recall: 66.77%
2025-11-06 12:42:31 2025-11-06 12:42:30,678 - INFO - New best model saved with validation accuracy: 66.774%
2025-11-06 12:42:31 2025-11-06 12:42:30,679 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_055.pth
2025-11-06 12:42:31 Train Epoch 055:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 54 that is less than the current step 137608. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 12:42:34 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 12:42:34   with torch.cuda.amp.autocast():
2025-11-06 12:42:36 Train Epoch 055:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.7325, Top1=N/A, LR=0.038109]2025-11-06 12:42:36,032 - INFO - Step 137610: {'train_loss_batch': 4.73253059387207, 'train_lr': 0.03810942992384429, 'batch_time': 5.351834058761597, 'data_time': 3.684173583984375}
2025-11-06 12:42:36 Train Epoch 055:   4%|▍         | 100/2502 [02:50<1:06:11,  1.65s/it, Loss=3.2780, Top1=N/A, LR=0.038109]2025-11-06 12:45:21,090 - INFO - Step 137710: {'train_loss_batch': 3.3845090866088867, 'train_lr': 0.03810942992384429, 'batch_time': 1.687230110168457, 'data_time': 0.037476454630936726}
2025-11-06 12:45:21 Train Epoch 055:   8%|▊         | 200/2502 [05:36<1:03:34,  1.66s/it, Loss=3.2359, Top1=N/A, LR=0.038109]2025-11-06 12:48:06,713 - INFO - Step 137810: {'train_loss_batch': 3.6401963233947754, 'train_lr': 0.03810942992384429, 'batch_time': 1.6718054576892758, 'data_time': 0.019356860450251187}
2025-11-06 12:48:06 Train Epoch 055:  12%|█▏        | 300/2502 [08:21<1:00:51,  1.66s/it, Loss=3.1981, Top1=N/A, LR=0.038109]2025-11-06 12:50:52,349 - INFO - Step 137910: {'train_loss_batch': 3.4500186443328857, 'train_lr': 0.03810942992384429, 'batch_time': 1.6666726234347322, 'data_time': 0.013267168570990579}
2025-11-06 12:50:52 Train Epoch 055:  16%|█▌        | 400/2502 [11:06<58:08,  1.66s/it, Loss=3.2097, Top1=63.08%, LR=0.038109]2025-11-06 12:53:36,715 - INFO - Step 138010: {'train_loss_batch': 2.5017261505126953, 'train_lr': 0.03810942992384429, 'batch_time': 1.6609339987547915, 'data_time': 0.010203494098121092}
2025-11-06 12:53:36 Train Epoch 055:  20%|█▉        | 500/2502 [13:52<55:23,  1.66s/it, Loss=3.2144, Top1=N/A, LR=0.038109]   2025-11-06 12:56:22,841 - INFO - Step 138110: {'train_loss_batch': 2.553797960281372, 'train_lr': 0.03810942992384429, 'batch_time': 1.6609988783647913, 'data_time': 0.008378156406912737}
2025-11-06 12:56:22 Train Epoch 055:  24%|██▍       | 600/2502 [16:37<52:07,  1.64s/it, Loss=3.2117, Top1=N/A, LR=0.038109]2025-11-06 12:59:08,165 - INFO - Step 138210: {'train_loss_batch': 2.5806448459625244, 'train_lr': 0.03810942992384429, 'batch_time': 1.6597071713496763, 'data_time': 0.0071503727289285515}
2025-11-06 12:59:08 Train Epoch 055:  28%|██▊       | 700/2502 [19:22<49:23,  1.64s/it, Loss=3.2155, Top1=N/A, LR=0.038109]2025-11-06 13:01:53,532 - INFO - Step 138310: {'train_loss_batch': 2.8066141605377197, 'train_lr': 0.03810942992384429, 'batch_time': 1.6588454780496986, 'data_time': 0.006269948798817677}
2025-11-06 13:01:53 Train Epoch 055:  32%|███▏      | 800/2502 [22:07<46:27,  1.64s/it, Loss=3.2367, Top1=N/A, LR=0.038109]2025-11-06 13:04:38,397 - INFO - Step 138410: {'train_loss_batch': 4.781044006347656, 'train_lr': 0.03810942992384429, 'batch_time': 1.6575718265347712, 'data_time': 0.005609650736890928}
2025-11-06 13:04:38 Train Epoch 055:  36%|███▌      | 900/2502 [24:52<43:57,  1.65s/it, Loss=3.2208, Top1=N/A, LR=0.038109]2025-11-06 13:07:23,050 - INFO - Step 138510: {'train_loss_batch': 3.5581905841827393, 'train_lr': 0.03810942992384429, 'batch_time': 1.6563467471369364, 'data_time': 0.0050934805854179225}
2025-11-06 13:07:23 Train Epoch 055:  40%|███▉      | 1000/2502 [27:38<41:26,  1.66s/it, Loss=3.2101, Top1=N/A, LR=0.038109]2025-11-06 13:10:08,860 - INFO - Step 138610: {'train_loss_batch': 4.890776634216309, 'train_lr': 0.03810942992384429, 'batch_time': 1.65652148897474, 'data_time': 0.00468484314528855}
2025-11-06 13:10:08 Train Epoch 055:  44%|████▍     | 1100/2502 [30:23<38:37,  1.65s/it, Loss=3.2073, Top1=63.14%, LR=0.038109]2025-11-06 13:12:53,979 - INFO - Step 138710: {'train_loss_batch': 2.5213170051574707, 'train_lr': 0.03810942992384429, 'batch_time': 1.656036905978183, 'data_time': 0.004357598241516723}
2025-11-06 13:12:53 Train Epoch 055:  48%|████▊     | 1200/2502 [33:08<35:37,  1.64s/it, Loss=3.2007, Top1=N/A, LR=0.038109]   2025-11-06 13:15:39,429 - INFO - Step 138810: {'train_loss_batch': 2.5312533378601074, 'train_lr': 0.03810942992384429, 'batch_time': 1.6559092452583661, 'data_time': 0.0040768449451405245}
2025-11-06 13:15:39 Train Epoch 055:  52%|█████▏    | 1300/2502 [35:53<33:06,  1.65s/it, Loss=3.1951, Top1=N/A, LR=0.038109]2025-11-06 13:18:24,557 - INFO - Step 138910: {'train_loss_batch': 3.97548770904541, 'train_lr': 0.03810942992384429, 'batch_time': 1.655553238470677, 'data_time': 0.0038451774591303713}
2025-11-06 13:18:24 Train Epoch 055:  56%|█████▌    | 1400/2502 [38:38<29:55,  1.63s/it, Loss=3.1978, Top1=N/A, LR=0.038109]2025-11-06 13:21:09,342 - INFO - Step 139010: {'train_loss_batch': 2.8681106567382812, 'train_lr': 0.03810942992384429, 'batch_time': 1.655003115247608, 'data_time': 0.003647567203094243}
2025-11-06 13:21:09 Train Epoch 055:  60%|█████▉    | 1500/2502 [41:22<27:43,  1.66s/it, Loss=3.1964, Top1=N/A, LR=0.038109]2025-11-06 13:23:53,497 - INFO - Step 139110: {'train_loss_batch': 3.9534568786621094, 'train_lr': 0.03810942992384429, 'batch_time': 1.6541063984102762, 'data_time': 0.0034749404658165717}
2025-11-06 13:23:53 Train Epoch 055:  64%|██████▍   | 1600/2502 [44:07<24:55,  1.66s/it, Loss=3.2010, Top1=N/A, LR=0.038109]2025-11-06 13:26:38,642 - INFO - Step 139210: {'train_loss_batch': 3.322045087814331, 'train_lr': 0.03810942992384429, 'batch_time': 1.6539406250745785, 'data_time': 0.003321125237812183}
2025-11-06 13:26:38 Train Epoch 055:  68%|██████▊   | 1700/2502 [46:53<22:08,  1.66s/it, Loss=3.1970, Top1=N/A, LR=0.038109]2025-11-06 13:29:24,157 - INFO - Step 139310: {'train_loss_batch': 2.9276270866394043, 'train_lr': 0.03810942992384429, 'batch_time': 1.6540114576854403, 'data_time': 0.0031844351587402057}
2025-11-06 13:29:24 Train Epoch 055:  72%|███████▏  | 1800/2502 [49:39<19:24,  1.66s/it, Loss=3.1941, Top1=62.84%, LR=0.038109]2025-11-06 13:32:10,158 - INFO - Step 139410: {'train_loss_batch': 2.3679018020629883, 'train_lr': 0.03810942992384429, 'batch_time': 1.6543443569668395, 'data_time': 0.003067138127523419}
2025-11-06 13:32:10 Train Epoch 055:  76%|███████▌  | 1900/2502 [52:25<16:41,  1.66s/it, Loss=3.1861, Top1=N/A, LR=0.038109]   2025-11-06 13:34:56,031 - INFO - Step 139510: {'train_loss_batch': 3.870760440826416, 'train_lr': 0.03810942992384429, 'batch_time': 1.654574718806444, 'data_time': 0.0029595939188740996}
2025-11-06 13:34:56 Train Epoch 055:  80%|███████▉  | 2000/2502 [55:11<13:53,  1.66s/it, Loss=3.1850, Top1=62.85%, LR=0.038109]2025-11-06 13:37:41,888 - INFO - Step 139610: {'train_loss_batch': 2.415736198425293, 'train_lr': 0.03810942992384429, 'batch_time': 1.654774698241242, 'data_time': 0.002864984677232307}
2025-11-06 13:37:41 Train Epoch 055:  84%|████████▍ | 2100/2502 [57:56<11:07,  1.66s/it, Loss=3.1874, Top1=N/A, LR=0.038109]   2025-11-06 13:40:27,425 - INFO - Step 139710: {'train_loss_batch': 2.6243538856506348, 'train_lr': 0.03810942992384429, 'batch_time': 1.6548025097408958, 'data_time': 0.0027788890537223834}
2025-11-06 13:40:27 Train Epoch 055:  88%|████████▊ | 2200/2502 [1:00:42<08:22,  1.66s/it, Loss=3.1844, Top1=62.81%, LR=0.038109]2025-11-06 13:43:13,466 - INFO - Step 139810: {'train_loss_batch': 2.4714362621307373, 'train_lr': 0.03810942992384429, 'batch_time': 1.6550574423994926, 'data_time': 0.0027014359946903018}
2025-11-06 13:43:13 Train Epoch 055:  92%|█████████▏| 2300/2502 [1:03:28<05:35,  1.66s/it, Loss=3.1848, Top1=62.77%, LR=0.038109]2025-11-06 13:45:59,228 - INFO - Step 139910: {'train_loss_batch': 2.5705361366271973, 'train_lr': 0.03810942992384429, 'batch_time': 1.6551689423980116, 'data_time': 0.0026295159185310904}
2025-11-06 13:45:59 Train Epoch 055:  96%|█████████▌| 2400/2502 [1:06:13<02:49,  1.66s/it, Loss=3.1832, Top1=N/A, LR=0.038109]   2025-11-06 13:48:44,391 - INFO - Step 140010: {'train_loss_batch': 2.965167284011841, 'train_lr': 0.03810942992384429, 'batch_time': 1.655020971588173, 'data_time': 0.0025649970395025833}
2025-11-06 13:48:44 Train Epoch 055: 100%|█████████▉| 2500/2502 [1:08:59<00:03,  1.66s/it, Loss=3.1854, Top1=62.72%, LR=0.038109]2025-11-06 13:51:29,893 - INFO - Step 140110: {'train_loss_batch': 2.5421574115753174, 'train_lr': 0.03810942992384429, 'batch_time': 1.6550210352565518, 'data_time': 0.0025294670721189064}
2025-11-06 13:51:29 Train Epoch 055: 100%|██████████| 2502/2502 [1:09:01<00:00,  1.66s/it, Loss=3.1854, Top1=62.72%, LR=0.038109]
2025-11-06 13:51:32 Val Epoch 055:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 13:51:36   with torch.cuda.amp.autocast():
2025-11-06 13:51:36 Val Epoch 055: 100%|██████████| 98/98 [01:51<00:00,  1.13s/it, Loss=2.2860, Top1=66.21%, Top5=87.67%]
2025-11-06 13:53:23 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 13:53:23   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 13:53:23 2025-11-06 13:53:23,306 - INFO - Step 55: {'epoch': 55, 'learning_rate': 0.036323218811346224, 'train_loss': 3.18572305708671, 'train_top1': 62.72417023554604, 'train_top5': 83.90574812633832, 'train_precision': 62.488229629900296, 'train_recall': 62.547208503182674, 'train_f1': 62.32763941689711, 'val_loss': 2.285962166137695, 'val_top1': 66.20600000976563, 'val_top5': 87.67200001464843, 'val_precision': 68.20761858659948, 'val_recall': 66.206, 'val_f1': 65.7581187150995}
2025-11-06 13:53:23 2025-11-06 13:53:23,308 - INFO - Epoch 055 Summary - LR: 0.036323, Train Loss: 3.1857, Val Loss: 2.2860, Val F1: 65.76%, Val Precision: 68.21%, Val Recall: 66.21%
2025-11-06 13:53:24 Train Epoch 056:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 13:53:28   with torch.cuda.amp.autocast():
2025-11-06 13:53:30 Train Epoch 056:   0%|          | 0/2502 [00:06<?, ?it/s, Loss=4.1973, Top1=N/A, LR=0.036323]2025-11-06 13:53:30,351 - INFO - Step 140112: {'train_loss_batch': 4.1973161697387695, 'train_lr': 0.036323218811346224, 'batch_time': 6.2394609451293945, 'data_time': 4.562902450561523}
2025-11-06 13:53:30 Train Epoch 056:   0%|          | 1/2502 [00:06<4:20:13,  6.24s/it, Loss=4.1973, Top1=N/A, LR=0.036323]wandb: WARNING Tried to log to step 55 that is less than the current step 140110. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 13:53:32 Train Epoch 056:   4%|▍         | 100/2502 [02:51<1:05:23,  1.63s/it, Loss=3.0582, Top1=63.97%, LR=0.036323]2025-11-06 13:56:15,698 - INFO - Step 140212: {'train_loss_batch': 2.4518885612487793, 'train_lr': 0.036323218811346224, 'batch_time': 1.698886776914691, 'data_time': 0.04618231612857025}
2025-11-06 13:56:15 Train Epoch 056:   8%|▊         | 200/2502 [05:36<1:03:46,  1.66s/it, Loss=3.1598, Top1=N/A, LR=0.036323]   2025-11-06 13:59:00,924 - INFO - Step 140312: {'train_loss_batch': 3.4759716987609863, 'train_lr': 0.036323218811346224, 'batch_time': 1.6756871671818976, 'data_time': 0.023681690443807572}
2025-11-06 13:59:01 Train Epoch 056:  12%|█▏        | 300/2502 [08:22<1:00:56,  1.66s/it, Loss=3.1272, Top1=63.70%, LR=0.036323]2025-11-06 14:01:46,669 - INFO - Step 140412: {'train_loss_batch': 2.405985116958618, 'train_lr': 0.036323218811346224, 'batch_time': 1.6696285448993162, 'data_time': 0.01613593418336786}
2025-11-06 14:01:46 Train Epoch 056:  16%|█▌        | 400/2502 [11:08<57:40,  1.65s/it, Loss=3.1431, Top1=N/A, LR=0.036323]   2025-11-06 14:04:32,210 - INFO - Step 140512: {'train_loss_batch': 4.1788105964660645, 'train_lr': 0.036323218811346224, 'batch_time': 1.6660807138666547, 'data_time': 0.012356078535541335}
2025-11-06 14:04:32 Train Epoch 056:  20%|█▉        | 500/2502 [13:53<55:08,  1.65s/it, Loss=3.1316, Top1=N/A, LR=0.036323]2025-11-06 14:07:17,750 - INFO - Step 140612: {'train_loss_batch': 4.070958137512207, 'train_lr': 0.036323218811346224, 'batch_time': 1.6639482384908224, 'data_time': 0.010091678349081866}
2025-11-06 14:07:17 Train Epoch 056:  24%|██▍       | 600/2502 [16:38<51:57,  1.64s/it, Loss=3.1289, Top1=63.55%, LR=0.036323]2025-11-06 14:10:02,375 - INFO - Step 140712: {'train_loss_batch': 2.4543702602386475, 'train_lr': 0.036323218811346224, 'batch_time': 1.661003589630127, 'data_time': 0.008580802482693842}
2025-11-06 14:10:02 Train Epoch 056:  28%|██▊       | 700/2502 [19:23<49:34,  1.65s/it, Loss=3.1368, Top1=N/A, LR=0.036323]   2025-11-06 14:12:47,705 - INFO - Step 140812: {'train_loss_batch': 4.857868671417236, 'train_lr': 0.036323218811346224, 'batch_time': 1.6599042044896712, 'data_time': 0.007504270012130411}
2025-11-06 14:12:47 Train Epoch 056:  32%|███▏      | 800/2502 [22:09<46:36,  1.64s/it, Loss=3.1556, Top1=N/A, LR=0.036323]2025-11-06 14:15:33,201 - INFO - Step 140912: {'train_loss_batch': 4.092843055725098, 'train_lr': 0.036323218811346224, 'batch_time': 1.65928737471315, 'data_time': 0.0066970466228013625}
2025-11-06 14:15:33 Train Epoch 056:  36%|███▌      | 900/2502 [24:54<44:05,  1.65s/it, Loss=3.1548, Top1=N/A, LR=0.036323]2025-11-06 14:18:18,679 - INFO - Step 141012: {'train_loss_batch': 2.494053363800049, 'train_lr': 0.036323218811346224, 'batch_time': 1.6587859890966383, 'data_time': 0.006068858401756837}
2025-11-06 14:18:18 Train Epoch 056:  40%|███▉      | 1000/2502 [27:39<41:27,  1.66s/it, Loss=3.1511, Top1=N/A, LR=0.036323]2025-11-06 14:21:04,021 - INFO - Step 141112: {'train_loss_batch': 4.012495040893555, 'train_lr': 0.036323218811346224, 'batch_time': 1.6582504533506655, 'data_time': 0.005566700593336717}
2025-11-06 14:21:04 Train Epoch 056:  44%|████▍     | 1100/2502 [30:25<38:48,  1.66s/it, Loss=3.1553, Top1=N/A, LR=0.036323]2025-11-06 14:23:49,801 - INFO - Step 141212: {'train_loss_batch': 3.8215408325195312, 'train_lr': 0.036323218811346224, 'batch_time': 1.6582094068206732, 'data_time': 0.005161773931103116}
2025-11-06 14:23:49 Train Epoch 056:  48%|████▊     | 1200/2502 [33:09<35:52,  1.65s/it, Loss=3.1637, Top1=N/A, LR=0.036323]2025-11-06 14:26:34,052 - INFO - Step 141312: {'train_loss_batch': 4.129269599914551, 'train_lr': 0.036323218811346224, 'batch_time': 1.656901656142083, 'data_time': 0.004811995035404964}
2025-11-06 14:26:34 Train Epoch 056:  52%|█████▏    | 1300/2502 [35:55<33:05,  1.65s/it, Loss=3.1646, Top1=63.28%, LR=0.036323]2025-11-06 14:29:19,714 - INFO - Step 141412: {'train_loss_batch': 2.477430582046509, 'train_lr': 0.036323218811346224, 'batch_time': 1.6568796125217, 'data_time': 0.004539132576369946}
2025-11-06 14:29:19 Train Epoch 056:  56%|█████▌    | 1400/2502 [38:41<30:15,  1.65s/it, Loss=3.1705, Top1=N/A, LR=0.036323]   2025-11-06 14:32:05,536 - INFO - Step 141512: {'train_loss_batch': 3.092471122741699, 'train_lr': 0.036323218811346224, 'batch_time': 1.6569751605742493, 'data_time': 0.004285390508081298}
2025-11-06 14:32:05 Train Epoch 056:  60%|█████▉    | 1500/2502 [41:27<27:38,  1.66s/it, Loss=3.1742, Top1=N/A, LR=0.036323]2025-11-06 14:34:51,147 - INFO - Step 141612: {'train_loss_batch': 2.5743637084960938, 'train_lr': 0.036323218811346224, 'batch_time': 1.6569171418832351, 'data_time': 0.004072020801999107}
2025-11-06 14:34:51 Train Epoch 056:  64%|██████▍   | 1600/2502 [44:12<24:59,  1.66s/it, Loss=3.1730, Top1=N/A, LR=0.036323]2025-11-06 14:37:36,724 - INFO - Step 141712: {'train_loss_batch': 3.4077110290527344, 'train_lr': 0.036323218811346224, 'batch_time': 1.656845482195414, 'data_time': 0.0038790990232602275}
2025-11-06 14:37:36 Train Epoch 056:  68%|██████▊   | 1700/2502 [46:57<22:01,  1.65s/it, Loss=3.1717, Top1=N/A, LR=0.036323]2025-11-06 14:40:22,059 - INFO - Step 141812: {'train_loss_batch': 3.791121244430542, 'train_lr': 0.036323218811346224, 'batch_time': 1.6566397391088845, 'data_time': 0.0037137287494226597}
2025-11-06 14:40:22 Train Epoch 056:  72%|███████▏  | 1800/2502 [49:43<19:24,  1.66s/it, Loss=3.1669, Top1=N/A, LR=0.036323]2025-11-06 14:43:08,054 - INFO - Step 141912: {'train_loss_batch': 3.5432393550872803, 'train_lr': 0.036323218811346224, 'batch_time': 1.6568236546937392, 'data_time': 0.0035646025304460712}
2025-11-06 14:43:08 Train Epoch 056:  76%|███████▌  | 1900/2502 [52:29<16:26,  1.64s/it, Loss=3.1736, Top1=N/A, LR=0.036323]2025-11-06 14:45:53,396 - INFO - Step 142012: {'train_loss_batch': 2.377814531326294, 'train_lr': 0.036323218811346224, 'batch_time': 1.6566444243461693, 'data_time': 0.0034304688567553363}
2025-11-06 14:45:53 Train Epoch 056:  80%|███████▉  | 2000/2502 [55:12<13:40,  1.64s/it, Loss=3.1687, Top1=N/A, LR=0.036323]2025-11-06 14:48:36,352 - INFO - Step 142112: {'train_loss_batch': 2.5470643043518066, 'train_lr': 0.036323218811346224, 'batch_time': 1.6552907143277802, 'data_time': 0.0033065358857284005}
2025-11-06 14:48:36 Train Epoch 056:  84%|████████▍ | 2100/2502 [57:57<11:03,  1.65s/it, Loss=3.1667, Top1=N/A, LR=0.036323]2025-11-06 14:51:21,626 - INFO - Step 142212: {'train_loss_batch': 4.321735382080078, 'train_lr': 0.036323218811346224, 'batch_time': 1.6551689182446718, 'data_time': 0.0031991873055285127}
2025-11-06 14:51:21 Train Epoch 056:  88%|████████▊ | 2200/2502 [1:00:43<08:15,  1.64s/it, Loss=3.1715, Top1=N/A, LR=0.036323]2025-11-06 14:54:07,178 - INFO - Step 142312: {'train_loss_batch': 2.918992757797241, 'train_lr': 0.036323218811346224, 'batch_time': 1.6551849783577197, 'data_time': 0.003102728260045483}
2025-11-06 14:54:07 Train Epoch 056:  92%|█████████▏| 2300/2502 [1:03:28<05:35,  1.66s/it, Loss=3.1715, Top1=N/A, LR=0.036323]2025-11-06 14:56:52,420 - INFO - Step 142412: {'train_loss_batch': 2.5385618209838867, 'train_lr': 0.036323218811346224, 'batch_time': 1.6550649178126542, 'data_time': 0.0030184520529125732}
2025-11-06 14:56:52 Train Epoch 056:  96%|█████████▌| 2400/2502 [1:06:13<02:47,  1.65s/it, Loss=3.1724, Top1=N/A, LR=0.036323]2025-11-06 14:59:37,966 - INFO - Step 142512: {'train_loss_batch': 2.5295112133026123, 'train_lr': 0.036323218811346224, 'batch_time': 1.6550812953613738, 'data_time': 0.00293591051288368}
2025-11-06 14:59:37 Train Epoch 056: 100%|█████████▉| 2500/2502 [1:08:58<00:03,  1.63s/it, Loss=3.1751, Top1=N/A, LR=0.036323]2025-11-06 15:02:22,582 - INFO - Step 142612: {'train_loss_batch': 4.026783466339111, 'train_lr': 0.036323218811346224, 'batch_time': 1.6547244159472747, 'data_time': 0.0028830825305375895}
2025-11-06 15:02:22 Train Epoch 056: 100%|██████████| 2502/2502 [1:09:00<00:00,  1.65s/it, Loss=3.1751, Top1=N/A, LR=0.036323]
2025-11-06 15:02:24 Val Epoch 056:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 15:02:29   with torch.cuda.amp.autocast():
2025-11-06 15:02:29 Val Epoch 056: 100%|██████████| 98/98 [01:53<00:00,  1.16s/it, Loss=2.2222, Top1=67.79%, Top5=88.79%]
2025-11-06 15:04:18 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 15:04:18   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 15:04:18 2025-11-06 15:04:18,917 - INFO - Step 56: {'epoch': 56, 'learning_rate': 0.03455569536622451, 'train_loss': 3.175715840215401, 'train_top1': 63.11627730582524, 'train_top5': 84.10497572815534, 'train_precision': 62.91792976948675, 'train_recall': 62.964702296389405, 'train_f1': 62.759839387488235, 'val_loss': 2.222168432006836, 'val_top1': 67.78799998291015, 'val_top5': 88.79399997070313, 'val_precision': 69.52838339341328, 'val_recall': 67.786, 'val_f1': 67.27718573496098}
2025-11-06 15:04:18 2025-11-06 15:04:18,919 - INFO - Epoch 056 Summary - LR: 0.034556, Train Loss: 3.1757, Val Loss: 2.2222, Val F1: 67.28%, Val Precision: 69.53%, Val Recall: 67.79%
2025-11-06 15:04:21 wandb: WARNING Tried to log to step 56 that is less than the current step 142612. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 15:04:22 2025-11-06 15:04:22,424 - INFO - New best model saved with validation accuracy: 67.788%
2025-11-06 15:04:22 2025-11-06 15:04:22,425 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_057.pth
2025-11-06 15:04:22 Train Epoch 057:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 15:04:26   with torch.cuda.amp.autocast():
2025-11-06 15:04:27 Train Epoch 057:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.7147, Top1=N/A, LR=0.034556]2025-11-06 15:04:27,723 - INFO - Step 142614: {'train_loss_batch': 3.714693307876587, 'train_lr': 0.03455569536622451, 'batch_time': 5.2956578731536865, 'data_time': 3.6703414916992188}
2025-11-06 15:04:27 Train Epoch 057:   4%|▍         | 100/2502 [02:49<1:05:42,  1.64s/it, Loss=3.3133, Top1=63.30%, LR=0.034556]2025-11-06 15:07:12,149 - INFO - Step 142714: {'train_loss_batch': 2.4243502616882324, 'train_lr': 0.03455569536622451, 'batch_time': 1.6804245887416425, 'data_time': 0.037393860297627965}
2025-11-06 15:07:12 Train Epoch 057:   8%|▊         | 200/2502 [05:35<1:03:34,  1.66s/it, Loss=3.2154, Top1=N/A, LR=0.034556]   2025-11-06 15:09:57,559 - INFO - Step 142814: {'train_loss_batch': 3.0951833724975586, 'train_lr': 0.03455569536622451, 'batch_time': 1.6673263352901782, 'data_time': 0.01929658918238398}
2025-11-06 15:09:57 Train Epoch 057:  12%|█▏        | 300/2502 [08:20<1:01:00,  1.66s/it, Loss=3.1697, Top1=N/A, LR=0.034556]2025-11-06 15:12:43,303 - INFO - Step 142914: {'train_loss_batch': 4.078350067138672, 'train_lr': 0.03455569536622451, 'batch_time': 1.6640429267059529, 'data_time': 0.013248035678039753}
2025-11-06 15:12:43 Train Epoch 057:  16%|█▌        | 400/2502 [11:04<57:19,  1.64s/it, Loss=3.1685, Top1=N/A, LR=0.034556]2025-11-06 15:15:27,121 - INFO - Step 143014: {'train_loss_batch': 2.4850032329559326, 'train_lr': 0.03455569536622451, 'batch_time': 1.6575915783718043, 'data_time': 0.010195393217472067}
2025-11-06 15:15:27 Train Epoch 057:  20%|█▉        | 500/2502 [13:49<54:33,  1.64s/it, Loss=3.1525, Top1=N/A, LR=0.034556]2025-11-06 15:18:11,561 - INFO - Step 143114: {'train_loss_batch': 4.485532283782959, 'train_lr': 0.03455569536622451, 'batch_time': 1.654957291608799, 'data_time': 0.008360355438110595}
2025-11-06 15:18:11 Train Epoch 057:  24%|██▍       | 600/2502 [16:34<52:33,  1.66s/it, Loss=3.1455, Top1=N/A, LR=0.034556]2025-11-06 15:20:56,630 - INFO - Step 143214: {'train_loss_batch': 2.580457925796509, 'train_lr': 0.03455569536622451, 'batch_time': 1.6542478778001275, 'data_time': 0.007132185477385307}
2025-11-06 15:20:56 Train Epoch 057:  28%|██▊       | 700/2502 [19:20<49:54,  1.66s/it, Loss=3.1386, Top1=63.61%, LR=0.034556]2025-11-06 15:23:42,615 - INFO - Step 143314: {'train_loss_batch': 2.4673943519592285, 'train_lr': 0.03455569536622451, 'batch_time': 1.655046781017505, 'data_time': 0.006254112159304544}
2025-11-06 15:23:42 Train Epoch 057:  32%|███▏      | 800/2502 [22:06<47:04,  1.66s/it, Loss=3.1392, Top1=N/A, LR=0.034556]   2025-11-06 15:26:28,507 - INFO - Step 143414: {'train_loss_batch': 3.4055542945861816, 'train_lr': 0.03455569536622451, 'batch_time': 1.6555295389392104, 'data_time': 0.005605975935670469}
2025-11-06 15:26:28 Train Epoch 057:  36%|███▌      | 900/2502 [24:50<44:23,  1.66s/it, Loss=3.1487, Top1=N/A, LR=0.034556]2025-11-06 15:29:13,256 - INFO - Step 143514: {'train_loss_batch': 2.5466463565826416, 'train_lr': 0.03455569536622451, 'batch_time': 1.6546369435123016, 'data_time': 0.005103035322437011}
2025-11-06 15:29:13 Train Epoch 057:  40%|███▉      | 1000/2502 [27:36<41:26,  1.66s/it, Loss=3.1571, Top1=N/A, LR=0.034556]2025-11-06 15:31:58,591 - INFO - Step 143614: {'train_loss_batch': 3.987687110900879, 'train_lr': 0.03455569536622451, 'batch_time': 1.6545082374767108, 'data_time': 0.004691757998623691}
2025-11-06 15:31:58 Train Epoch 057:  44%|████▍     | 1100/2502 [30:21<38:49,  1.66s/it, Loss=3.1509, Top1=N/A, LR=0.034556]2025-11-06 15:34:43,711 - INFO - Step 143714: {'train_loss_batch': 2.3467330932617188, 'train_lr': 0.03455569536622451, 'batch_time': 1.654207243473285, 'data_time': 0.00435395721518701}
2025-11-06 15:34:43 Train Epoch 057:  48%|████▊     | 1200/2502 [33:06<35:50,  1.65s/it, Loss=3.1553, Top1=N/A, LR=0.034556]2025-11-06 15:37:29,292 - INFO - Step 143814: {'train_loss_batch': 2.625143527984619, 'train_lr': 0.03455569536622451, 'batch_time': 1.65434062391594, 'data_time': 0.004080791258990616}
2025-11-06 15:37:29 Train Epoch 057:  52%|█████▏    | 1300/2502 [35:51<33:10,  1.66s/it, Loss=3.1541, Top1=N/A, LR=0.034556]2025-11-06 15:40:14,383 - INFO - Step 143914: {'train_loss_batch': 4.013983726501465, 'train_lr': 0.03455569536622451, 'batch_time': 1.6540774643008842, 'data_time': 0.0038418416148602824}
2025-11-06 15:40:14 Train Epoch 057:  56%|█████▌    | 1400/2502 [38:36<30:12,  1.64s/it, Loss=3.1514, Top1=N/A, LR=0.034556]2025-11-06 15:42:59,415 - INFO - Step 144014: {'train_loss_batch': 4.109630584716797, 'train_lr': 0.03455569536622451, 'batch_time': 1.6538090026863637, 'data_time': 0.003636809777226472}
2025-11-06 15:42:59 Train Epoch 057:  60%|█████▉    | 1500/2502 [41:22<27:43,  1.66s/it, Loss=3.1528, Top1=N/A, LR=0.034556]2025-11-06 15:45:44,496 - INFO - Step 144114: {'train_loss_batch': 3.8344578742980957, 'train_lr': 0.03455569536622451, 'batch_time': 1.6536090270747033, 'data_time': 0.0034597205925114864}
2025-11-06 15:45:44 Train Epoch 057:  64%|██████▍   | 1600/2502 [44:07<24:48,  1.65s/it, Loss=3.1515, Top1=N/A, LR=0.034556]2025-11-06 15:48:30,163 - INFO - Step 144214: {'train_loss_batch': 4.99528694152832, 'train_lr': 0.03455569536622451, 'batch_time': 1.653799949624552, 'data_time': 0.003302998426629185}
2025-11-06 15:48:30 Train Epoch 057:  68%|██████▊   | 1700/2502 [46:53<22:06,  1.65s/it, Loss=3.1478, Top1=63.47%, LR=0.034556]2025-11-06 15:51:15,905 - INFO - Step 144314: {'train_loss_batch': 2.538686752319336, 'train_lr': 0.03455569536622451, 'batch_time': 1.6540129158091783, 'data_time': 0.0031722141391455883}
2025-11-06 15:51:16 Train Epoch 057:  72%|███████▏  | 1800/2502 [49:38<19:25,  1.66s/it, Loss=3.1501, Top1=63.43%, LR=0.034556]2025-11-06 15:54:00,994 - INFO - Step 144414: {'train_loss_batch': 2.4831180572509766, 'train_lr': 0.03455569536622451, 'batch_time': 1.6538392347074231, 'data_time': 0.003051595248360557}
2025-11-06 15:54:01 Train Epoch 057:  76%|███████▌  | 1900/2502 [52:24<16:31,  1.65s/it, Loss=3.1490, Top1=N/A, LR=0.034556]   2025-11-06 15:56:46,442 - INFO - Step 144514: {'train_loss_batch': 2.696357250213623, 'train_lr': 0.03455569536622451, 'batch_time': 1.6538728326950995, 'data_time': 0.002942291828659194}
2025-11-06 15:56:46 Train Epoch 057:  80%|███████▉  | 2000/2502 [55:09<13:52,  1.66s/it, Loss=3.1465, Top1=N/A, LR=0.034556]2025-11-06 15:59:31,704 - INFO - Step 144614: {'train_loss_batch': 2.3949198722839355, 'train_lr': 0.03455569536622451, 'batch_time': 1.6538100732320073, 'data_time': 0.0028461883331405585}
2025-11-06 15:59:31 Train Epoch 057:  84%|████████▍ | 2100/2502 [57:53<10:58,  1.64s/it, Loss=3.1527, Top1=N/A, LR=0.034556]2025-11-06 16:02:16,345 - INFO - Step 144714: {'train_loss_batch': 2.8645434379577637, 'train_lr': 0.03455569536622451, 'batch_time': 1.653457972278259, 'data_time': 0.002760257339659105}
2025-11-06 16:02:16 Train Epoch 057:  88%|████████▊ | 2200/2502 [1:00:39<08:16,  1.64s/it, Loss=3.1555, Top1=N/A, LR=0.034556]2025-11-06 16:05:01,494 - INFO - Step 144814: {'train_loss_batch': 3.9776716232299805, 'train_lr': 0.03455569536622451, 'batch_time': 1.6533682741722373, 'data_time': 0.002679142070217384}
2025-11-06 16:05:01 Train Epoch 057:  92%|█████████▏| 2300/2502 [1:03:24<05:35,  1.66s/it, Loss=3.1546, Top1=N/A, LR=0.034556]2025-11-06 16:07:47,213 - INFO - Step 144914: {'train_loss_batch': 2.8232624530792236, 'train_lr': 0.03455569536622451, 'batch_time': 1.65353413583506, 'data_time': 0.0026060204462402025}
2025-11-06 16:07:47 Train Epoch 057:  96%|█████████▌| 2400/2502 [1:06:10<02:48,  1.66s/it, Loss=3.1540, Top1=N/A, LR=0.034556]2025-11-06 16:10:33,046 - INFO - Step 145014: {'train_loss_batch': 4.143125534057617, 'train_lr': 0.03455569536622451, 'batch_time': 1.653733830906758, 'data_time': 0.00254321386296766}
2025-11-06 16:10:33 Train Epoch 057: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.67s/it, Loss=3.1572, Top1=N/A, LR=0.034556]2025-11-06 16:13:19,106 - INFO - Step 145114: {'train_loss_batch': 2.5228230953216553, 'train_lr': 0.03455569536622451, 'batch_time': 1.6540082519124004, 'data_time': 0.002533798835507301}
2025-11-06 16:13:19 Train Epoch 057: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=3.1572, Top1=N/A, LR=0.034556]
2025-11-06 16:13:21 Val Epoch 057:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 16:13:25   with torch.cuda.amp.autocast():
2025-11-06 16:13:26 Val Epoch 057: 100%|██████████| 98/98 [01:51<00:00,  1.13s/it, Loss=2.2482, Top1=67.23%, Top5=88.50%]
2025-11-06 16:15:12 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 16:15:12   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 16:15:12 2025-11-06 16:15:12,545 - INFO - Step 57: {'epoch': 57, 'learning_rate': 0.032809273812896764, 'train_loss': 3.1570462075164087, 'train_top1': 63.305758325289574, 'train_top5': 84.33540962837837, 'train_precision': 63.15586098733424, 'train_recall': 63.22003294187945, 'train_f1': 63.00770065893827, 'val_loss': 2.248230061798096, 'val_top1': 67.23200000976563, 'val_top5': 88.49800001464844, 'val_precision': 69.24529578817172, 'val_recall': 67.238, 'val_f1': 66.83023708621127}
2025-11-06 16:15:12 2025-11-06 16:15:12,546 - INFO - Epoch 057 Summary - LR: 0.032809, Train Loss: 3.1570, Val Loss: 2.2482, Val F1: 66.83%, Val Precision: 69.25%, Val Recall: 67.24%
2025-11-06 16:15:12 Train Epoch 058:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 16:15:16   with torch.cuda.amp.autocast():
2025-11-06 16:15:18 Train Epoch 058:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.9336, Top1=N/A, LR=0.032809]2025-11-06 16:15:18,174 - INFO - Step 145116: {'train_loss_batch': 3.933581829071045, 'train_lr': 0.032809273812896764, 'batch_time': 5.2004554271698, 'data_time': 3.5550341606140137}
2025-11-06 16:15:18 Train Epoch 058:   0%|          | 3/2502 [00:08<1:41:44,  2.44s/it, Loss=3.9336, Top1=N/A, LR=0.032809]wandb: WARNING Tried to log to step 57 that is less than the current step 145114. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 16:15:23 Train Epoch 058:   4%|▍         | 100/2502 [02:50<1:05:53,  1.65s/it, Loss=3.1444, Top1=N/A, LR=0.032809]2025-11-06 16:18:03,275 - INFO - Step 145216: {'train_loss_batch': 3.1394643783569336, 'train_lr': 0.032809273812896764, 'batch_time': 1.6861653044672296, 'data_time': 0.03623926285469886}
2025-11-06 16:18:03 Train Epoch 058:   8%|▊         | 200/2502 [05:35<1:03:39,  1.66s/it, Loss=3.1177, Top1=N/A, LR=0.032809]2025-11-06 16:20:48,684 - INFO - Step 145316: {'train_loss_batch': 2.8598599433898926, 'train_lr': 0.032809273812896764, 'batch_time': 1.6702020856278452, 'data_time': 0.01869731044294822}
2025-11-06 16:20:48 Train Epoch 058:  12%|█▏        | 300/2502 [08:21<1:00:12,  1.64s/it, Loss=3.0878, Top1=N/A, LR=0.032809]2025-11-06 16:23:33,985 - INFO - Step 145416: {'train_loss_batch': 2.5355489253997803, 'train_lr': 0.032809273812896764, 'batch_time': 1.664490889869259, 'data_time': 0.012803293937860533}
2025-11-06 16:23:33 Train Epoch 058:  16%|█▌        | 400/2502 [11:06<57:53,  1.65s/it, Loss=3.0809, Top1=N/A, LR=0.032809]2025-11-06 16:26:19,769 - INFO - Step 145516: {'train_loss_batch': 2.5902562141418457, 'train_lr': 0.032809273812896764, 'batch_time': 1.6628310941995825, 'data_time': 0.009864691784257008}
2025-11-06 16:26:19 Train Epoch 058:  20%|█▉        | 500/2502 [13:53<55:18,  1.66s/it, Loss=3.0934, Top1=N/A, LR=0.032809]2025-11-06 16:29:06,226 - INFO - Step 145616: {'train_loss_batch': 2.3796393871307373, 'train_lr': 0.032809273812896764, 'batch_time': 1.6631780175153843, 'data_time': 0.008130228685999583}
2025-11-06 16:29:06 Train Epoch 058:  24%|██▍       | 600/2502 [16:38<52:06,  1.64s/it, Loss=3.0967, Top1=N/A, LR=0.032809]2025-11-06 16:31:51,415 - INFO - Step 145716: {'train_loss_batch': 3.6339805126190186, 'train_lr': 0.032809273812896764, 'batch_time': 1.6613003953721084, 'data_time': 0.006925068758490082}
2025-11-06 16:31:51 Train Epoch 058:  28%|██▊       | 700/2502 [19:23<49:47,  1.66s/it, Loss=3.1115, Top1=N/A, LR=0.032809]2025-11-06 16:34:36,317 - INFO - Step 145816: {'train_loss_batch': 2.456022262573242, 'train_lr': 0.032809273812896764, 'batch_time': 1.6595475268942144, 'data_time': 0.006064531296364081}
2025-11-06 16:34:36 Train Epoch 058:  32%|███▏      | 800/2502 [22:08<46:35,  1.64s/it, Loss=3.1138, Top1=N/A, LR=0.032809]2025-11-06 16:37:21,031 - INFO - Step 145916: {'train_loss_batch': 3.606745719909668, 'train_lr': 0.032809273812896764, 'batch_time': 1.6579986249611767, 'data_time': 0.005424966228737516}
2025-11-06 16:37:21 Train Epoch 058:  36%|███▌      | 900/2502 [24:53<44:23,  1.66s/it, Loss=3.1113, Top1=N/A, LR=0.032809]2025-11-06 16:40:06,341 - INFO - Step 146016: {'train_loss_batch': 2.7445132732391357, 'train_lr': 0.032809273812896764, 'batch_time': 1.657454804495622, 'data_time': 0.004924997770032131}
2025-11-06 16:40:06 Train Epoch 058:  40%|███▉      | 1000/2502 [27:39<41:28,  1.66s/it, Loss=3.1167, Top1=N/A, LR=0.032809]2025-11-06 16:42:52,362 - INFO - Step 146116: {'train_loss_batch': 4.137858867645264, 'train_lr': 0.032809273812896764, 'batch_time': 1.657729423010385, 'data_time': 0.004526856895926949}
2025-11-06 16:42:52 Train Epoch 058:  44%|████▍     | 1100/2502 [30:25<38:49,  1.66s/it, Loss=3.1194, Top1=N/A, LR=0.032809]2025-11-06 16:45:38,286 - INFO - Step 146216: {'train_loss_batch': 2.8103630542755127, 'train_lr': 0.032809273812896764, 'batch_time': 1.6578669214551824, 'data_time': 0.004210657254876493}
2025-11-06 16:45:38 Train Epoch 058:  48%|████▊     | 1200/2502 [33:10<35:54,  1.65s/it, Loss=3.1255, Top1=N/A, LR=0.032809]2025-11-06 16:48:23,670 - INFO - Step 146316: {'train_loss_batch': 3.99796724319458, 'train_lr': 0.032809273812896764, 'batch_time': 1.6575307832173165, 'data_time': 0.003941413464891623}
2025-11-06 16:48:23 Train Epoch 058:  52%|█████▏    | 1300/2502 [35:54<32:42,  1.63s/it, Loss=3.1267, Top1=N/A, LR=0.032809]2025-11-06 16:51:07,844 - INFO - Step 146416: {'train_loss_batch': 4.020246505737305, 'train_lr': 0.032809273812896764, 'batch_time': 1.6563170287904145, 'data_time': 0.003711564828578002}
2025-11-06 16:51:07 Train Epoch 058:  56%|█████▌    | 1400/2502 [38:39<30:35,  1.67s/it, Loss=3.1264, Top1=N/A, LR=0.032809]2025-11-06 16:53:52,778 - INFO - Step 146516: {'train_loss_batch': 4.280159950256348, 'train_lr': 0.032809273812896764, 'batch_time': 1.655819170309934, 'data_time': 0.0035129124398405086}
2025-11-06 16:53:52 Train Epoch 058:  60%|█████▉    | 1500/2502 [41:25<27:47,  1.66s/it, Loss=3.1364, Top1=N/A, LR=0.032809]2025-11-06 16:56:38,544 - INFO - Step 146616: {'train_loss_batch': 3.9266724586486816, 'train_lr': 0.032809273812896764, 'batch_time': 1.6559412395215527, 'data_time': 0.003349301340101879}
2025-11-06 16:56:38 Train Epoch 058:  64%|██████▍   | 1600/2502 [44:11<25:00,  1.66s/it, Loss=3.1349, Top1=N/A, LR=0.032809]2025-11-06 16:59:24,526 - INFO - Step 146716: {'train_loss_batch': 3.8535797595977783, 'train_lr': 0.032809273812896764, 'batch_time': 1.6561837966258939, 'data_time': 0.0032044489036717316}
2025-11-06 16:59:24 Train Epoch 058:  68%|██████▊   | 1700/2502 [46:56<21:57,  1.64s/it, Loss=3.1398, Top1=N/A, LR=0.032809]2025-11-06 17:02:09,834 - INFO - Step 146816: {'train_loss_batch': 2.474388837814331, 'train_lr': 0.032809273812896764, 'batch_time': 1.6560011235214134, 'data_time': 0.00306988407204531}
2025-11-06 17:02:09 Train Epoch 058:  72%|███████▏  | 1800/2502 [49:42<19:21,  1.65s/it, Loss=3.1403, Top1=N/A, LR=0.032809]2025-11-06 17:04:55,201 - INFO - Step 146916: {'train_loss_batch': 2.640244483947754, 'train_lr': 0.032809273812896764, 'batch_time': 1.655871446631737, 'data_time': 0.0029554901885562707}
2025-11-06 17:04:55 Train Epoch 058:  76%|███████▌  | 1900/2502 [52:27<16:37,  1.66s/it, Loss=3.1439, Top1=N/A, LR=0.032809]2025-11-06 17:07:40,719 - INFO - Step 147016: {'train_loss_batch': 4.783062934875488, 'train_lr': 0.032809273812896764, 'batch_time': 1.6558349590311547, 'data_time': 0.002857818282446944}
2025-11-06 17:07:40 Train Epoch 058:  80%|███████▉  | 2000/2502 [55:13<13:53,  1.66s/it, Loss=3.1477, Top1=N/A, LR=0.032809]2025-11-06 17:10:26,710 - INFO - Step 147116: {'train_loss_batch': 3.9516983032226562, 'train_lr': 0.032809273812896764, 'batch_time': 1.6560385557248078, 'data_time': 0.0027688847131457466}
2025-11-06 17:10:26 Train Epoch 058:  84%|████████▍ | 2100/2502 [57:59<11:04,  1.65s/it, Loss=3.1456, Top1=63.97%, LR=0.032809]2025-11-06 17:13:12,518 - INFO - Step 147216: {'train_loss_batch': 2.378431558609009, 'train_lr': 0.032809273812896764, 'batch_time': 1.6561354838457067, 'data_time': 0.0026859413266806078}
2025-11-06 17:13:12 Train Epoch 058:  88%|████████▊ | 2200/2502 [1:00:44<08:17,  1.65s/it, Loss=3.1448, Top1=N/A, LR=0.032809]   2025-11-06 17:15:57,538 - INFO - Step 147316: {'train_loss_batch': 2.459604263305664, 'train_lr': 0.032809273812896764, 'batch_time': 1.655865698714302, 'data_time': 0.0026067654039035435}
2025-11-06 17:15:57 Train Epoch 058:  92%|█████████▏| 2300/2502 [1:03:29<05:34,  1.65s/it, Loss=3.1466, Top1=N/A, LR=0.032809]2025-11-06 17:18:42,810 - INFO - Step 147416: {'train_loss_batch': 4.042987823486328, 'train_lr': 0.032809273812896764, 'batch_time': 1.6557291947671093, 'data_time': 0.002533654760454386}
2025-11-06 17:18:42 Train Epoch 058:  96%|█████████▌| 2400/2502 [1:06:15<02:48,  1.65s/it, Loss=3.1445, Top1=N/A, LR=0.032809]2025-11-06 17:21:28,902 - INFO - Step 147516: {'train_loss_batch': 3.2873542308807373, 'train_lr': 0.032809273812896764, 'batch_time': 1.6559452047351995, 'data_time': 0.002467579168361805}
2025-11-06 17:21:28 Train Epoch 058: 100%|█████████▉| 2500/2502 [1:09:01<00:03,  1.66s/it, Loss=3.1425, Top1=N/A, LR=0.032809]2025-11-06 17:24:14,708 - INFO - Step 147616: {'train_loss_batch': 4.682652950286865, 'train_lr': 0.032809273812896764, 'batch_time': 1.6560297765430572, 'data_time': 0.002431882185632827}
2025-11-06 17:24:14 Train Epoch 058: 100%|██████████| 2502/2502 [1:09:03<00:00,  1.66s/it, Loss=3.1425, Top1=N/A, LR=0.032809]
2025-11-06 17:24:16 Val Epoch 058:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 17:24:21   with torch.cuda.amp.autocast():
2025-11-06 17:24:21 Val Epoch 058: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.2174, Top1=67.97%, Top5=88.73%]
2025-11-06 17:26:08 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 17:26:08   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 17:26:08 2025-11-06 17:26:08,765 - INFO - Step 58: {'epoch': 58, 'learning_rate': 0.031086339553134702, 'train_loss': 3.1428572694174677, 'train_top1': 63.85018661437247, 'train_top5': 84.53038018724696, 'train_precision': 63.65785197281154, 'train_recall': 63.70325117703315, 'train_f1': 63.493706218119684, 'val_loss': 2.2174052696228026, 'val_top1': 67.974, 'val_top5': 88.72999999511718, 'val_precision': 70.09556132368301, 'val_recall': 67.98, 'val_f1': 67.5767891825328}
2025-11-06 17:26:08 2025-11-06 17:26:08,767 - INFO - Epoch 058 Summary - LR: 0.031086, Train Loss: 3.1429, Val Loss: 2.2174, Val F1: 67.58%, Val Precision: 70.10%, Val Recall: 67.98%
2025-11-06 17:26:12 2025-11-06 17:26:12,378 - INFO - New best model saved with validation accuracy: 67.974%
2025-11-06 17:26:12 2025-11-06 17:26:12,378 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_059.pth
2025-11-06 17:26:12 Train Epoch 059:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 58 that is less than the current step 147616. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 17:26:16 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 17:26:16   with torch.cuda.amp.autocast():
2025-11-06 17:26:17 Train Epoch 059:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.5753, Top1=N/A, LR=0.031086]2025-11-06 17:26:17,617 - INFO - Step 147618: {'train_loss_batch': 2.5753426551818848, 'train_lr': 0.031086339553134702, 'batch_time': 5.23690938949585, 'data_time': 3.59940767288208}
2025-11-06 17:26:17 Train Epoch 059:   4%|▍         | 100/2502 [02:50<1:06:11,  1.65s/it, Loss=3.1803, Top1=N/A, LR=0.031086]2025-11-06 17:29:03,269 - INFO - Step 147718: {'train_loss_batch': 3.835367202758789, 'train_lr': 0.031086339553134702, 'batch_time': 1.6919753693117954, 'data_time': 0.036657944764241134}
2025-11-06 17:29:03 Train Epoch 059:   8%|▊         | 200/2502 [05:36<1:03:36,  1.66s/it, Loss=3.0622, Top1=N/A, LR=0.031086]2025-11-06 17:31:49,069 - INFO - Step 147818: {'train_loss_batch': 2.4810726642608643, 'train_lr': 0.031086339553134702, 'batch_time': 1.6750696251048378, 'data_time': 0.018904467720297437}
2025-11-06 17:31:49 Train Epoch 059:  12%|█▏        | 300/2502 [08:22<1:00:46,  1.66s/it, Loss=3.1299, Top1=N/A, LR=0.031086]2025-11-06 17:34:34,953 - INFO - Step 147918: {'train_loss_batch': 2.4225921630859375, 'train_lr': 0.031086339553134702, 'batch_time': 1.669678982706165, 'data_time': 0.012961876352760087}
2025-11-06 17:34:34 Train Epoch 059:  16%|█▌        | 400/2502 [11:08<58:08,  1.66s/it, Loss=3.1372, Top1=64.79%, LR=0.031086]2025-11-06 17:37:20,555 - INFO - Step 148018: {'train_loss_batch': 2.4343013763427734, 'train_lr': 0.031086339553134702, 'batch_time': 1.666272460671137, 'data_time': 0.009987431571370645}
2025-11-06 17:37:20 Train Epoch 059:  20%|█▉        | 500/2502 [13:53<55:17,  1.66s/it, Loss=3.1415, Top1=N/A, LR=0.031086]   2025-11-06 17:40:06,144 - INFO - Step 148118: {'train_loss_batch': 4.211832523345947, 'train_lr': 0.031086339553134702, 'batch_time': 1.66419990571911, 'data_time': 0.008206595917661747}
2025-11-06 17:40:06 Train Epoch 059:  24%|██▍       | 600/2502 [16:38<52:34,  1.66s/it, Loss=3.1261, Top1=N/A, LR=0.031086]2025-11-06 17:42:51,221 - INFO - Step 148218: {'train_loss_batch': 4.137232780456543, 'train_lr': 0.031086339553134702, 'batch_time': 1.6619646053345944, 'data_time': 0.0070107471129660206}
2025-11-06 17:42:51 Train Epoch 059:  28%|██▊       | 700/2502 [19:24<49:18,  1.64s/it, Loss=3.1297, Top1=N/A, LR=0.031086]2025-11-06 17:45:36,409 - INFO - Step 148318: {'train_loss_batch': 2.442711591720581, 'train_lr': 0.031086339553134702, 'batch_time': 1.6605262640709544, 'data_time': 0.0061525444841588955}
2025-11-06 17:45:36 Train Epoch 059:  32%|███▏      | 800/2502 [22:08<46:15,  1.63s/it, Loss=3.1145, Top1=64.38%, LR=0.031086]2025-11-06 17:48:20,610 - INFO - Step 148418: {'train_loss_batch': 2.5187010765075684, 'train_lr': 0.031086339553134702, 'batch_time': 1.658213251687763, 'data_time': 0.005505231138174602}
2025-11-06 17:48:20 Train Epoch 059:  36%|███▌      | 900/2502 [24:53<44:21,  1.66s/it, Loss=3.1118, Top1=64.36%, LR=0.031086]2025-11-06 17:51:05,550 - INFO - Step 148518: {'train_loss_batch': 2.3938493728637695, 'train_lr': 0.031086339553134702, 'batch_time': 1.6572345160485373, 'data_time': 0.004999143037362051}
2025-11-06 17:51:05 Train Epoch 059:  40%|███▉      | 1000/2502 [27:38<41:18,  1.65s/it, Loss=3.1247, Top1=N/A, LR=0.031086]   2025-11-06 17:53:50,964 - INFO - Step 148618: {'train_loss_batch': 2.6176295280456543, 'train_lr': 0.031086339553134702, 'batch_time': 1.6569262120154473, 'data_time': 0.004602320306189172}
2025-11-06 17:53:50 Train Epoch 059:  44%|████▍     | 1100/2502 [30:24<38:28,  1.65s/it, Loss=3.1194, Top1=N/A, LR=0.031086]2025-11-06 17:56:36,535 - INFO - Step 148718: {'train_loss_batch': 3.6468567848205566, 'train_lr': 0.031086339553134702, 'batch_time': 1.6568153419893077, 'data_time': 0.004275495631385131}
2025-11-06 17:56:36 Train Epoch 059:  48%|████▊     | 1200/2502 [33:08<35:36,  1.64s/it, Loss=3.1221, Top1=N/A, LR=0.031086]2025-11-06 17:59:21,379 - INFO - Step 148818: {'train_loss_batch': 3.9350085258483887, 'train_lr': 0.031086339553134702, 'batch_time': 1.656117321152572, 'data_time': 0.004002522072327524}
2025-11-06 17:59:21 Train Epoch 059:  52%|█████▏    | 1300/2502 [35:54<33:05,  1.65s/it, Loss=3.1224, Top1=N/A, LR=0.031086]2025-11-06 18:02:06,611 - INFO - Step 148918: {'train_loss_batch': 3.2148845195770264, 'train_lr': 0.031086339553134702, 'batch_time': 1.6558255223839398, 'data_time': 0.00377339679034832}
2025-11-06 18:02:06 Train Epoch 059:  56%|█████▌    | 1400/2502 [38:39<30:23,  1.66s/it, Loss=3.1282, Top1=N/A, LR=0.031086]2025-11-06 18:04:51,817 - INFO - Step 149018: {'train_loss_batch': 2.6429378986358643, 'train_lr': 0.031086339553134702, 'batch_time': 1.6555565370141736, 'data_time': 0.0035743184127099677}
2025-11-06 18:04:51 Train Epoch 059:  60%|█████▉    | 1500/2502 [41:24<27:33,  1.65s/it, Loss=3.1296, Top1=N/A, LR=0.031086]2025-11-06 18:07:37,267 - INFO - Step 149118: {'train_loss_batch': 2.699398994445801, 'train_lr': 0.031086339553134702, 'batch_time': 1.655485803011971, 'data_time': 0.003403504318908244}
2025-11-06 18:07:37 Train Epoch 059:  64%|██████▍   | 1600/2502 [44:10<24:47,  1.65s/it, Loss=3.1263, Top1=N/A, LR=0.031086]2025-11-06 18:10:22,796 - INFO - Step 149218: {'train_loss_batch': 3.1552999019622803, 'train_lr': 0.031086339553134702, 'batch_time': 1.6554735712078792, 'data_time': 0.003253714730038187}
2025-11-06 18:10:22 Train Epoch 059:  68%|██████▊   | 1700/2502 [46:56<21:57,  1.64s/it, Loss=3.1293, Top1=N/A, LR=0.031086]2025-11-06 18:13:08,572 - INFO - Step 149318: {'train_loss_batch': 3.3560895919799805, 'train_lr': 0.031086339553134702, 'batch_time': 1.655607709879037, 'data_time': 0.0031228177621741636}
2025-11-06 18:13:08 Train Epoch 059:  72%|███████▏  | 1800/2502 [49:41<19:25,  1.66s/it, Loss=3.1294, Top1=N/A, LR=0.031086]2025-11-06 18:15:53,626 - INFO - Step 149418: {'train_loss_batch': 3.6511077880859375, 'train_lr': 0.031086339553134702, 'batch_time': 1.6553267499329578, 'data_time': 0.003007498534105673}
2025-11-06 18:15:53 Train Epoch 059:  76%|███████▌  | 1900/2502 [52:27<16:38,  1.66s/it, Loss=3.1274, Top1=64.16%, LR=0.031086]2025-11-06 18:18:39,632 - INFO - Step 149518: {'train_loss_batch': 2.513833522796631, 'train_lr': 0.031086339553134702, 'batch_time': 1.6555753777617845, 'data_time': 0.0029041413443142462}
2025-11-06 18:18:39 Train Epoch 059:  80%|███████▉  | 2000/2502 [55:12<13:51,  1.66s/it, Loss=3.1284, Top1=64.21%, LR=0.031086]2025-11-06 18:21:25,160 - INFO - Step 149618: {'train_loss_batch': 2.413151264190674, 'train_lr': 0.031086339553134702, 'batch_time': 1.6555603693152356, 'data_time': 0.0028126694928521457}
2025-11-06 18:21:25 Train Epoch 059:  84%|████████▍ | 2100/2502 [57:58<11:08,  1.66s/it, Loss=3.1268, Top1=N/A, LR=0.031086]   2025-11-06 18:24:11,183 - INFO - Step 149718: {'train_loss_batch': 3.8640689849853516, 'train_lr': 0.031086339553134702, 'batch_time': 1.6557826788183736, 'data_time': 0.0027273189221490627}
2025-11-06 18:24:11 Train Epoch 059:  88%|████████▊ | 2200/2502 [1:00:44<08:20,  1.66s/it, Loss=3.1264, Top1=N/A, LR=0.031086]2025-11-06 18:26:56,670 - INFO - Step 149818: {'train_loss_batch': 3.2052102088928223, 'train_lr': 0.031086339553134702, 'batch_time': 1.6557412799625493, 'data_time': 0.0026508606005559883}
2025-11-06 18:26:56 Train Epoch 059:  92%|█████████▏| 2300/2502 [1:03:30<05:35,  1.66s/it, Loss=3.1325, Top1=N/A, LR=0.031086]2025-11-06 18:29:42,396 - INFO - Step 149918: {'train_loss_batch': 4.870861053466797, 'train_lr': 0.031086339553134702, 'batch_time': 1.6558069175660532, 'data_time': 0.0025829647168653107}
2025-11-06 18:29:42 Train Epoch 059:  96%|█████████▌| 2400/2502 [1:06:15<02:48,  1.65s/it, Loss=3.1351, Top1=N/A, LR=0.031086]2025-11-06 18:32:27,450 - INFO - Step 150018: {'train_loss_batch': 2.403980016708374, 'train_lr': 0.031086339553134702, 'batch_time': 1.6555875647519838, 'data_time': 0.0025166031322296535}
2025-11-06 18:32:27 Train Epoch 059: 100%|█████████▉| 2500/2502 [1:09:00<00:03,  1.65s/it, Loss=3.1412, Top1=N/A, LR=0.031086]2025-11-06 18:35:12,886 - INFO - Step 150118: {'train_loss_batch': 2.6288530826568604, 'train_lr': 0.031086339553134702, 'batch_time': 1.6555384459947404, 'data_time': 0.0024937554770877294}
2025-11-06 18:35:12 Train Epoch 059: 100%|██████████| 2502/2502 [1:09:07<00:00,  1.66s/it, Loss=3.1412, Top1=N/A, LR=0.031086]
2025-11-06 18:35:20 Val Epoch 059:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 18:35:24   with torch.cuda.amp.autocast():
2025-11-06 18:35:25 Val Epoch 059: 100%|██████████| 98/98 [01:49<00:00,  1.12s/it, Loss=2.2264, Top1=68.57%, Top5=88.93%]
2025-11-06 18:37:10 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 18:37:10   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 18:37:10 2025-11-06 18:37:10,029 - INFO - Step 59: {'epoch': 59, 'learning_rate': 0.029389245907892204, 'train_loss': 3.1415451605924125, 'train_top1': 64.19913668592437, 'train_top5': 84.74018513655462, 'train_precision': 63.987229525346024, 'train_recall': 64.08183171486594, 'train_f1': 63.851927480297554, 'val_loss': 2.226410167007446, 'val_top1': 68.57, 'val_top5': 88.93399998535156, 'val_precision': 70.27951446960326, 'val_recall': 68.56800000000001, 'val_f1': 68.05542347793042}
2025-11-06 18:37:10 2025-11-06 18:37:10,030 - INFO - Epoch 059 Summary - LR: 0.029389, Train Loss: 3.1415, Val Loss: 2.2264, Val F1: 68.06%, Val Precision: 70.28%, Val Recall: 68.57%
2025-11-06 18:37:13 2025-11-06 18:37:13,037 - INFO - New best model saved with validation accuracy: 68.570%
2025-11-06 18:37:13 2025-11-06 18:37:13,038 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_060.pth
2025-11-06 18:37:13 Train Epoch 060:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 59 that is less than the current step 150118. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 18:37:16 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 18:37:16   with torch.cuda.amp.autocast():
2025-11-06 18:37:18 Train Epoch 060:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.1278, Top1=N/A, LR=0.029389]2025-11-06 18:37:18,217 - INFO - Step 150120: {'train_loss_batch': 4.1278076171875, 'train_lr': 0.029389245907892204, 'batch_time': 5.1770546436309814, 'data_time': 3.5075128078460693}
2025-11-06 18:37:18 Train Epoch 060:   4%|▍         | 100/2502 [02:50<1:06:23,  1.66s/it, Loss=3.2212, Top1=N/A, LR=0.029389]2025-11-06 18:40:03,568 - INFO - Step 150220: {'train_loss_batch': 4.6843109130859375, 'train_lr': 0.029389245907892204, 'batch_time': 1.6884064768800642, 'data_time': 0.03577141242452187}
2025-11-06 18:40:03 Train Epoch 060:   8%|▊         | 200/2502 [05:36<1:03:49,  1.66s/it, Loss=3.1666, Top1=N/A, LR=0.029389]2025-11-06 18:42:49,288 - INFO - Step 150320: {'train_loss_batch': 3.429919719696045, 'train_lr': 0.029389245907892204, 'batch_time': 1.672878579713812, 'data_time': 0.018492627499708487}
2025-11-06 18:42:49 Train Epoch 060:  12%|█▏        | 300/2502 [08:22<1:00:52,  1.66s/it, Loss=3.1587, Top1=N/A, LR=0.029389]2025-11-06 18:45:35,089 - INFO - Step 150420: {'train_loss_batch': 4.089395523071289, 'train_lr': 0.029389245907892204, 'batch_time': 1.6679372993418544, 'data_time': 0.012684933766970207}
2025-11-06 18:45:35 Train Epoch 060:  16%|█▌        | 400/2502 [11:07<58:11,  1.66s/it, Loss=3.1654, Top1=N/A, LR=0.029389]2025-11-06 18:48:20,143 - INFO - Step 150520: {'train_loss_batch': 2.491999626159668, 'train_lr': 0.029389245907892204, 'batch_time': 1.6635988264012516, 'data_time': 0.00977812859780176}
2025-11-06 18:48:20 Train Epoch 060:  20%|█▉        | 500/2502 [13:52<55:19,  1.66s/it, Loss=3.1561, Top1=N/A, LR=0.029389]2025-11-06 18:51:05,761 - INFO - Step 150620: {'train_loss_batch': 3.9214417934417725, 'train_lr': 0.029389245907892204, 'batch_time': 1.662117343224927, 'data_time': 0.008029129690752772}
2025-11-06 18:51:05 Train Epoch 060:  24%|██▍       | 600/2502 [16:37<52:25,  1.65s/it, Loss=3.1385, Top1=N/A, LR=0.029389]2025-11-06 18:53:50,662 - INFO - Step 150720: {'train_loss_batch': 2.461047649383545, 'train_lr': 0.029389245907892204, 'batch_time': 1.6599365573952876, 'data_time': 0.006858074724575049}
2025-11-06 18:53:50 Train Epoch 060:  28%|██▊       | 700/2502 [19:22<49:48,  1.66s/it, Loss=3.1385, Top1=N/A, LR=0.029389]2025-11-06 18:56:35,923 - INFO - Step 150820: {'train_loss_batch': 2.4319472312927246, 'train_lr': 0.029389245907892204, 'batch_time': 1.658890930970283, 'data_time': 0.006021788729070427}
2025-11-06 18:56:35 Train Epoch 060:  32%|███▏      | 800/2502 [22:08<46:50,  1.65s/it, Loss=3.1369, Top1=N/A, LR=0.029389]2025-11-06 18:59:21,646 - INFO - Step 150920: {'train_loss_batch': 2.3725428581237793, 'train_lr': 0.029389245907892204, 'batch_time': 1.6586831404772888, 'data_time': 0.005392943726347924}
2025-11-06 18:59:21 Train Epoch 060:  36%|███▌      | 900/2502 [24:53<44:09,  1.65s/it, Loss=3.1435, Top1=N/A, LR=0.029389]2025-11-06 19:02:06,693 - INFO - Step 151020: {'train_loss_batch': 4.053879261016846, 'train_lr': 0.029389245907892204, 'batch_time': 1.657772058916674, 'data_time': 0.004901758706265364}
2025-11-06 19:02:06 Train Epoch 060:  40%|███▉      | 1000/2502 [27:39<41:14,  1.65s/it, Loss=3.1438, Top1=64.63%, LR=0.029389]2025-11-06 19:04:52,044 - INFO - Step 151120: {'train_loss_batch': 2.4476966857910156, 'train_lr': 0.029389245907892204, 'batch_time': 1.6573458877834049, 'data_time': 0.004506925007441899}
2025-11-06 19:04:52 Train Epoch 060:  44%|████▍     | 1100/2502 [30:24<38:41,  1.66s/it, Loss=3.1485, Top1=N/A, LR=0.029389]   2025-11-06 19:07:37,552 - INFO - Step 151220: {'train_loss_batch': 2.2412686347961426, 'train_lr': 0.029389245907892204, 'batch_time': 1.6571398220963092, 'data_time': 0.004187711686247809}
2025-11-06 19:07:37 Train Epoch 060:  48%|████▊     | 1200/2502 [33:10<35:47,  1.65s/it, Loss=3.1329, Top1=N/A, LR=0.029389]2025-11-06 19:10:23,087 - INFO - Step 151320: {'train_loss_batch': 2.4340837001800537, 'train_lr': 0.029389245907892204, 'batch_time': 1.6569907071687697, 'data_time': 0.003917814392928379}
2025-11-06 19:10:23 Train Epoch 060:  52%|█████▏    | 1300/2502 [35:55<33:13,  1.66s/it, Loss=3.1325, Top1=N/A, LR=0.029389]2025-11-06 19:13:08,693 - INFO - Step 151420: {'train_loss_batch': 2.522911310195923, 'train_lr': 0.029389245907892204, 'batch_time': 1.6569186011980719, 'data_time': 0.0036936015188464926}
2025-11-06 19:13:08 Train Epoch 060:  56%|█████▌    | 1400/2502 [38:41<30:27,  1.66s/it, Loss=3.1331, Top1=64.51%, LR=0.029389]2025-11-06 19:15:54,248 - INFO - Step 151520: {'train_loss_batch': 2.4591147899627686, 'train_lr': 0.029389245907892204, 'batch_time': 1.6568213518307433, 'data_time': 0.0035016058514068843}
2025-11-06 19:15:54 Train Epoch 060:  60%|█████▉    | 1500/2502 [41:27<27:41,  1.66s/it, Loss=3.1281, Top1=N/A, LR=0.029389]   2025-11-06 19:18:40,326 - INFO - Step 151620: {'train_loss_batch': 3.0493452548980713, 'train_lr': 0.029389245907892204, 'batch_time': 1.657084823528343, 'data_time': 0.0033369010325513784}
2025-11-06 19:18:40 Train Epoch 060:  64%|██████▍   | 1600/2502 [44:12<24:42,  1.64s/it, Loss=3.1333, Top1=N/A, LR=0.029389]2025-11-06 19:21:25,541 - INFO - Step 151720: {'train_loss_batch': 3.9929161071777344, 'train_lr': 0.029389245907892204, 'batch_time': 1.6567761704446673, 'data_time': 0.0031939488064863025}
2025-11-06 19:21:25 Train Epoch 060:  68%|██████▊   | 1700/2502 [46:57<22:03,  1.65s/it, Loss=3.1292, Top1=N/A, LR=0.029389]2025-11-06 19:24:10,360 - INFO - Step 151820: {'train_loss_batch': 4.075320243835449, 'train_lr': 0.029389245907892204, 'batch_time': 1.6562712963436157, 'data_time': 0.0030636097808784068}
2025-11-06 19:24:10 Train Epoch 060:  72%|███████▏  | 1800/2502 [49:43<19:28,  1.66s/it, Loss=3.1382, Top1=N/A, LR=0.029389]2025-11-06 19:26:56,314 - INFO - Step 151920: {'train_loss_batch': 3.208357810974121, 'train_lr': 0.029389245907892204, 'batch_time': 1.6564528989765395, 'data_time': 0.0029522757077468627}
2025-11-06 19:26:56 Train Epoch 060:  76%|███████▌  | 1900/2502 [52:28<16:37,  1.66s/it, Loss=3.1380, Top1=N/A, LR=0.029389]2025-11-06 19:29:41,900 - INFO - Step 152020: {'train_loss_batch': 4.8267340660095215, 'train_lr': 0.029389245907892204, 'batch_time': 1.6564216398051512, 'data_time': 0.002849885753930587}
2025-11-06 19:29:41 Train Epoch 060:  80%|███████▉  | 2000/2502 [55:14<13:52,  1.66s/it, Loss=3.1370, Top1=64.46%, LR=0.029389]2025-11-06 19:32:27,674 - INFO - Step 152120: {'train_loss_batch': 2.2598860263824463, 'train_lr': 0.029389245907892204, 'batch_time': 1.6564870379913574, 'data_time': 0.002758622705668345}
2025-11-06 19:32:27 Train Epoch 060:  84%|████████▍ | 2100/2502 [57:59<11:09,  1.67s/it, Loss=3.1377, Top1=N/A, LR=0.029389]   2025-11-06 19:35:12,967 - INFO - Step 152220: {'train_loss_batch': 2.542495012283325, 'train_lr': 0.029389245907892204, 'batch_time': 1.6563175909295642, 'data_time': 0.002676373376896244}
2025-11-06 19:35:12 Train Epoch 060:  88%|████████▊ | 2200/2502 [1:00:45<08:21,  1.66s/it, Loss=3.1390, Top1=N/A, LR=0.029389]2025-11-06 19:37:58,173 - INFO - Step 152320: {'train_loss_batch': 3.8618252277374268, 'train_lr': 0.029389245907892204, 'batch_time': 1.6561241887580909, 'data_time': 0.0026013736343557105}
2025-11-06 19:37:58 Train Epoch 060:  92%|█████████▏| 2300/2502 [1:03:30<05:34,  1.66s/it, Loss=3.1426, Top1=64.40%, LR=0.029389]2025-11-06 19:40:43,062 - INFO - Step 152420: {'train_loss_batch': 2.355076313018799, 'train_lr': 0.029389245907892204, 'batch_time': 1.6558095854502666, 'data_time': 0.002534162267712291}
2025-11-06 19:40:43 Train Epoch 060:  96%|█████████▌| 2400/2502 [1:06:15<02:49,  1.66s/it, Loss=3.1428, Top1=64.40%, LR=0.029389]2025-11-06 19:43:28,217 - INFO - Step 152520: {'train_loss_batch': 2.471283435821533, 'train_lr': 0.029389245907892204, 'batch_time': 1.6556324056166998, 'data_time': 0.0024692858323014212}
2025-11-06 19:43:28 Train Epoch 060: 100%|█████████▉| 2500/2502 [1:09:00<00:03,  1.66s/it, Loss=3.1398, Top1=N/A, LR=0.029389]   2025-11-06 19:46:13,261 - INFO - Step 152620: {'train_loss_batch': 2.3094820976257324, 'train_lr': 0.029389245907892204, 'batch_time': 1.6554247928781063, 'data_time': 0.0024314249863104077}
2025-11-06 19:46:13 Train Epoch 060: 100%|██████████| 2502/2502 [1:09:02<00:00,  1.66s/it, Loss=3.1398, Top1=N/A, LR=0.029389]
2025-11-06 19:46:15 Val Epoch 060:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 19:46:19   with torch.cuda.amp.autocast():
2025-11-06 19:46:20 Val Epoch 060: 100%|██████████| 98/98 [01:50<00:00,  1.12s/it, Loss=2.1720, Top1=68.86%, Top5=89.14%]
2025-11-06 19:48:05 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 19:48:05   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 19:48:05 2025-11-06 19:48:05,824 - INFO - Step 60: {'epoch': 60, 'learning_rate': 0.027720310902951975, 'train_loss': 3.1395103194826035, 'train_top1': 64.38542213760505, 'train_top5': 84.9408318014706, 'train_precision': 64.21187481756682, 'train_recall': 64.24740723876361, 'train_f1': 64.04829559926203, 'val_loss': 2.17202343170166, 'val_top1': 68.86000000732422, 'val_top5': 89.13599999023438, 'val_precision': 70.44352794524517, 'val_recall': 68.854, 'val_f1': 68.45105664693115}
2025-11-06 19:48:05 2025-11-06 19:48:05,826 - INFO - Epoch 060 Summary - LR: 0.027720, Train Loss: 3.1395, Val Loss: 2.1720, Val F1: 68.45%, Val Precision: 70.44%, Val Recall: 68.85%
2025-11-06 19:48:08 2025-11-06 19:48:08,881 - INFO - New best model saved with validation accuracy: 68.860%
2025-11-06 19:48:08 2025-11-06 19:48:08,881 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_061.pth
2025-11-06 19:48:08 Train Epoch 061:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 19:48:12   with torch.cuda.amp.autocast():
2025-11-06 19:48:13 wandb: WARNING Tried to log to step 60 that is less than the current step 152620. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 19:48:14 Train Epoch 061:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.9833, Top1=N/A, LR=0.027720]2025-11-06 19:48:14,586 - INFO - Step 152622: {'train_loss_batch': 3.983272075653076, 'train_lr': 0.027720310902951975, 'batch_time': 5.70253586769104, 'data_time': 4.052137136459351}
2025-11-06 19:48:14 Train Epoch 061:   4%|▍         | 100/2502 [02:51<1:06:25,  1.66s/it, Loss=2.9795, Top1=N/A, LR=0.027720]2025-11-06 19:51:00,125 - INFO - Step 152722: {'train_loss_batch': 2.488433361053467, 'train_lr': 0.027720310902951975, 'batch_time': 1.6954678332451547, 'data_time': 0.04115347578974053}
2025-11-06 19:51:00 Train Epoch 061:   8%|▊         | 200/2502 [05:36<1:03:14,  1.65s/it, Loss=3.0417, Top1=65.35%, LR=0.027720]2025-11-06 19:53:45,784 - INFO - Step 152822: {'train_loss_batch': 2.3909945487976074, 'train_lr': 0.027720310902951975, 'batch_time': 1.6761265666923713, 'data_time': 0.02117481397752145}
2025-11-06 19:53:45 Train Epoch 061:  12%|█▏        | 300/2502 [08:22<1:00:39,  1.65s/it, Loss=3.0849, Top1=65.36%, LR=0.027720]2025-11-06 19:56:31,238 - INFO - Step 152922: {'train_loss_batch': 2.391070604324341, 'train_lr': 0.027720310902951975, 'batch_time': 1.6689545165660769, 'data_time': 0.014491142228592274}
2025-11-06 19:56:31 Train Epoch 061:  16%|█▌        | 400/2502 [11:07<57:20,  1.64s/it, Loss=3.0808, Top1=N/A, LR=0.027720]   2025-11-06 19:59:16,266 - INFO - Step 153022: {'train_loss_batch': 2.3963029384613037, 'train_lr': 0.027720310902951975, 'batch_time': 1.664294869525177, 'data_time': 0.011128477324868675}
2025-11-06 19:59:16 Train Epoch 061:  20%|█▉        | 500/2502 [13:50<54:52,  1.64s/it, Loss=3.0946, Top1=N/A, LR=0.027720]2025-11-06 20:01:59,857 - INFO - Step 153122: {'train_loss_batch': 3.8127236366271973, 'train_lr': 0.027720310902951975, 'batch_time': 1.658630040829291, 'data_time': 0.009092681184262335}
2025-11-06 20:01:59 Train Epoch 061:  24%|██▍       | 600/2502 [16:36<52:43,  1.66s/it, Loss=3.0952, Top1=64.95%, LR=0.027720]2025-11-06 20:04:45,330 - INFO - Step 153222: {'train_loss_batch': 2.5346193313598633, 'train_lr': 0.027720310902951975, 'batch_time': 1.6579806165964948, 'data_time': 0.007744448355549385}
2025-11-06 20:04:45 Train Epoch 061:  28%|██▊       | 700/2502 [19:22<49:53,  1.66s/it, Loss=3.1031, Top1=N/A, LR=0.027720]   2025-11-06 20:07:31,107 - INFO - Step 153322: {'train_loss_batch': 4.170570373535156, 'train_lr': 0.027720310902951975, 'batch_time': 1.6579506638726902, 'data_time': 0.006778049741083817}
2025-11-06 20:07:31 Train Epoch 061:  32%|███▏      | 800/2502 [22:08<46:54,  1.65s/it, Loss=3.0997, Top1=N/A, LR=0.027720]2025-11-06 20:10:17,166 - INFO - Step 153422: {'train_loss_batch': 2.635920524597168, 'train_lr': 0.027720310902951975, 'batch_time': 1.6582795118124745, 'data_time': 0.006055655104390691}
2025-11-06 20:10:17 Train Epoch 061:  36%|███▌      | 900/2502 [24:52<43:42,  1.64s/it, Loss=3.0994, Top1=N/A, LR=0.027720]2025-11-06 20:13:01,814 - INFO - Step 153522: {'train_loss_batch': 2.503420829772949, 'train_lr': 0.027720310902951975, 'batch_time': 1.6569702545890004, 'data_time': 0.0054966077688134605}
2025-11-06 20:13:01 Train Epoch 061:  40%|███▉      | 1000/2502 [27:38<41:36,  1.66s/it, Loss=3.1091, Top1=N/A, LR=0.027720]2025-11-06 20:15:47,043 - INFO - Step 153622: {'train_loss_batch': 2.620293140411377, 'train_lr': 0.027720310902951975, 'batch_time': 1.6565025174296224, 'data_time': 0.005046146613853676}
2025-11-06 20:15:47 Train Epoch 061:  44%|████▍     | 1100/2502 [30:23<38:34,  1.65s/it, Loss=3.1166, Top1=N/A, LR=0.027720]2025-11-06 20:18:32,814 - INFO - Step 153722: {'train_loss_batch': 4.677011489868164, 'train_lr': 0.027720310902951975, 'batch_time': 1.6566115196567575, 'data_time': 0.004679195237744406}
2025-11-06 20:18:32 Train Epoch 061:  48%|████▊     | 1200/2502 [33:08<35:47,  1.65s/it, Loss=3.1105, Top1=N/A, LR=0.027720]2025-11-06 20:21:17,755 - INFO - Step 153822: {'train_loss_batch': 2.8619658946990967, 'train_lr': 0.027720310902951975, 'batch_time': 1.656011815273593, 'data_time': 0.004363743887654352}
2025-11-06 20:21:17 Train Epoch 061:  52%|█████▏    | 1300/2502 [35:54<33:07,  1.65s/it, Loss=3.1030, Top1=64.76%, LR=0.027720]2025-11-06 20:24:03,295 - INFO - Step 153922: {'train_loss_batch': 2.4664125442504883, 'train_lr': 0.027720310902951975, 'batch_time': 1.6559644163250464, 'data_time': 0.004104499904858342}
2025-11-06 20:24:08 Train Epoch 061:  56%|█████▌    | 1400/2502 [38:40<30:28,  1.66s/it, Loss=3.0991, Top1=N/A, LR=0.027720]   2025-11-06 20:26:49,032 - INFO - Step 154022: {'train_loss_batch': 4.049264907836914, 'train_lr': 0.027720310902951975, 'batch_time': 1.6560650589974926, 'data_time': 0.0038824864237075357}
2025-11-06 20:26:49 Train Epoch 061:  60%|█████▉    | 1500/2502 [41:25<27:38,  1.65s/it, Loss=3.0935, Top1=64.70%, LR=0.027720]2025-11-06 20:29:34,671 - INFO - Step 154122: {'train_loss_batch': 2.390739917755127, 'train_lr': 0.027720310902951975, 'batch_time': 1.656086200241404, 'data_time': 0.003695068638933094}
2025-11-06 20:29:34 Train Epoch 061:  64%|██████▍   | 1600/2502 [44:11<24:48,  1.65s/it, Loss=3.0969, Top1=N/A, LR=0.027720]   2025-11-06 20:32:20,404 - INFO - Step 154222: {'train_loss_batch': 2.345432996749878, 'train_lr': 0.027720310902951975, 'batch_time': 1.656164100808996, 'data_time': 0.003526964312713642}
2025-11-06 20:32:20 Train Epoch 061:  68%|██████▊   | 1700/2502 [46:57<22:08,  1.66s/it, Loss=3.1031, Top1=N/A, LR=0.027720]2025-11-06 20:35:06,396 - INFO - Step 154322: {'train_loss_batch': 2.4193453788757324, 'train_lr': 0.027720310902951975, 'batch_time': 1.656384600813707, 'data_time': 0.003379749452836509}
2025-11-06 20:35:06 Train Epoch 061:  72%|███████▏  | 1800/2502 [49:43<19:23,  1.66s/it, Loss=3.1041, Top1=N/A, LR=0.027720]2025-11-06 20:37:52,220 - INFO - Step 154422: {'train_loss_batch': 2.410043954849243, 'train_lr': 0.027720310902951975, 'batch_time': 1.6564873538369407, 'data_time': 0.0032470864365327233}
2025-11-06 20:38:00 Train Epoch 061:  76%|███████▌  | 1900/2502 [52:29<16:38,  1.66s/it, Loss=3.1029, Top1=64.72%, LR=0.027720]2025-11-06 20:40:38,208 - INFO - Step 154522: {'train_loss_batch': 2.469883441925049, 'train_lr': 0.027720310902951975, 'batch_time': 1.6566657481978655, 'data_time': 0.0031273747543985124}
2025-11-06 20:41:00 Train Epoch 061:  80%|███████▉  | 2000/2502 [55:15<13:52,  1.66s/it, Loss=3.0986, Top1=N/A, LR=0.027720]   2025-11-06 20:43:24,096 - INFO - Step 154622: {'train_loss_batch': 2.605236053466797, 'train_lr': 0.027720310902951975, 'batch_time': 1.6567763153878765, 'data_time': 0.00302217496388677}
2025-11-06 20:43:33 Train Epoch 061:  84%|████████▍ | 2100/2502 [58:00<11:07,  1.66s/it, Loss=3.1009, Top1=N/A, LR=0.027720]2025-11-06 20:46:09,116 - INFO - Step 154722: {'train_loss_batch': 3.768369436264038, 'train_lr': 0.027720310902951975, 'batch_time': 1.6564632562839774, 'data_time': 0.0029235888185641812}
2025-11-06 20:46:12 Train Epoch 061:  88%|████████▊ | 2200/2502 [1:00:45<08:19,  1.65s/it, Loss=3.1014, Top1=N/A, LR=0.027720]2025-11-06 20:48:54,255 - INFO - Step 154822: {'train_loss_batch': 2.4490206241607666, 'train_lr': 0.027720310902951975, 'batch_time': 1.6562324796466057, 'data_time': 0.0028386717219614863}
2025-11-06 20:48:58 Train Epoch 061:  92%|█████████▏| 2300/2502 [1:03:30<05:35,  1.66s/it, Loss=3.1054, Top1=N/A, LR=0.027720]2025-11-06 20:51:39,578 - INFO - Step 154922: {'train_loss_batch': 3.0152394771575928, 'train_lr': 0.027720310902951975, 'batch_time': 1.6561023368362964, 'data_time': 0.0027579730720636068}
2025-11-06 20:51:39 Train Epoch 061:  96%|█████████▌| 2400/2502 [1:06:16<02:49,  1.66s/it, Loss=3.1077, Top1=64.70%, LR=0.027720]2025-11-06 20:54:24,898 - INFO - Step 155022: {'train_loss_batch': 2.4178552627563477, 'train_lr': 0.027720310902951975, 'batch_time': 1.6559811171667123, 'data_time': 0.002681987576562134}
2025-11-06 20:54:34 Train Epoch 061: 100%|█████████▉| 2500/2502 [1:09:02<00:03,  1.66s/it, Loss=3.1082, Top1=N/A, LR=0.027720]   2025-11-06 20:57:10,943 - INFO - Step 155122: {'train_loss_batch': 4.186107635498047, 'train_lr': 0.027720310902951975, 'batch_time': 1.6561596348780434, 'data_time': 0.0026531408234435716}
2025-11-06 20:57:10 Train Epoch 061: 100%|██████████| 2502/2502 [1:09:03<00:00,  1.66s/it, Loss=3.1082, Top1=N/A, LR=0.027720]
2025-11-06 20:57:13 Val Epoch 061:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 20:57:23   with torch.cuda.amp.autocast():
2025-11-06 20:57:23 Val Epoch 061: 100%|██████████| 98/98 [01:47<00:00,  1.10s/it, Loss=2.1719, Top1=69.23%, Top5=89.65%]
2025-11-06 20:59:16 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 20:59:16   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 20:59:16 2025-11-06 20:59:01,276 - INFO - Step 61: {'epoch': 61, 'learning_rate': 0.026081814102781185, 'train_loss': 3.1080311280455617, 'train_top1': 64.7007604805726, 'train_top5': 85.08954818507158, 'train_precision': 64.54882212127015, 'train_recall': 64.59301604341246, 'train_f1': 64.39225230232509, 'val_loss': 2.1719283025741576, 'val_top1': 69.232, 'val_top5': 89.64600001464844, 'val_precision': 70.78988024350863, 'val_recall': 69.228, 'val_f1': 68.83449351746205}
2025-11-06 20:59:16 2025-11-06 20:59:01,278 - INFO - Epoch 061 Summary - LR: 0.026082, Train Loss: 3.1080, Val Loss: 2.1719, Val F1: 68.83%, Val Precision: 70.79%, Val Recall: 69.23%
2025-11-06 20:59:16 wandb: WARNING Tried to log to step 61 that is less than the current step 155122. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 20:59:16 2025-11-06 20:59:04,639 - INFO - New best model saved with validation accuracy: 69.232%
2025-11-06 20:59:16 2025-11-06 20:59:04,639 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_062.pth
2025-11-06 20:59:16 Train Epoch 062:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 20:59:16   with torch.cuda.amp.autocast():
2025-11-06 20:59:16 Train Epoch 062:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.5897, Top1=N/A, LR=0.026082]2025-11-06 20:59:09,959 - INFO - Step 155124: {'train_loss_batch': 3.58974027633667, 'train_lr': 0.026081814102781185, 'batch_time': 5.317564487457275, 'data_time': 3.671567916870117}
2025-11-06 20:59:16 Train Epoch 062:   4%|▍         | 100/2502 [02:50<1:06:01,  1.65s/it, Loss=3.0768, Top1=N/A, LR=0.026082]2025-11-06 21:01:55,024 - INFO - Step 155224: {'train_loss_batch': 2.3948843479156494, 'train_lr': 0.026081814102781185, 'batch_time': 1.6869653286320148, 'data_time': 0.03733596471276614}
2025-11-06 21:01:55 Train Epoch 062:   8%|▊         | 200/2502 [05:36<1:03:34,  1.66s/it, Loss=3.0553, Top1=N/A, LR=0.026082]2025-11-06 21:04:40,698 - INFO - Step 155324: {'train_loss_batch': 2.5792925357818604, 'train_lr': 0.026081814102781185, 'batch_time': 1.6719270594677522, 'data_time': 0.019251698878274034}
2025-11-06 21:04:44 Train Epoch 062:  12%|█▏        | 300/2502 [08:21<1:00:50,  1.66s/it, Loss=3.0393, Top1=65.22%, LR=0.026082]2025-11-06 21:07:26,271 - INFO - Step 155424: {'train_loss_batch': 2.3983652591705322, 'train_lr': 0.026081814102781185, 'batch_time': 1.6665436635381756, 'data_time': 0.013190160954117378}
2025-11-06 21:08:00 Train Epoch 062:  16%|█▌        | 400/2502 [11:04<57:24,  1.64s/it, Loss=3.0318, Top1=65.23%, LR=0.026082]2025-11-06 21:10:09,551 - INFO - Step 155524: {'train_loss_batch': 2.5645742416381836, 'train_lr': 0.026081814102781185, 'batch_time': 1.6581284809588197, 'data_time': 0.010142154527126702}
2025-11-06 21:10:45 Train Epoch 062:  20%|█▉        | 500/2502 [13:50<55:12,  1.65s/it, Loss=3.0404, Top1=N/A, LR=0.026082]   2025-11-06 21:12:54,973 - INFO - Step 155624: {'train_loss_batch': 2.8732104301452637, 'train_lr': 0.026081814102781185, 'batch_time': 1.6573477947783328, 'data_time': 0.008318093008623866}
2025-11-06 21:12:56 Train Epoch 062:  24%|██▍       | 600/2502 [16:36<52:43,  1.66s/it, Loss=3.0371, Top1=N/A, LR=0.026082]2025-11-06 21:15:40,933 - INFO - Step 155724: {'train_loss_batch': 2.4353065490722656, 'train_lr': 0.026081814102781185, 'batch_time': 1.6577225933455786, 'data_time': 0.007106790526734414}
2025-11-06 21:16:09 Train Epoch 062:  28%|██▊       | 700/2502 [19:21<49:25,  1.65s/it, Loss=3.0410, Top1=N/A, LR=0.026082]2025-11-06 21:18:25,789 - INFO - Step 155824: {'train_loss_batch': 4.748658180236816, 'train_lr': 0.026081814102781185, 'batch_time': 1.6564160774165655, 'data_time': 0.006233959837408787}
2025-11-06 21:18:25 Train Epoch 062:  32%|███▏      | 800/2502 [22:06<47:10,  1.66s/it, Loss=3.0515, Top1=65.24%, LR=0.026082]2025-11-06 21:21:11,490 - INFO - Step 155924: {'train_loss_batch': 2.318239212036133, 'train_lr': 0.026081814102781185, 'batch_time': 1.6564888579122137, 'data_time': 0.005577780036592901}
2025-11-06 21:21:11 Train Epoch 062:  36%|███▌      | 900/2502 [24:51<44:09,  1.65s/it, Loss=3.0639, Top1=65.21%, LR=0.026082]2025-11-06 21:23:56,348 - INFO - Step 156024: {'train_loss_batch': 2.4708240032196045, 'train_lr': 0.026081814102781185, 'batch_time': 1.6556108699125403, 'data_time': 0.005070742703436747}
2025-11-06 21:23:56 Train Epoch 062:  40%|███▉      | 1000/2502 [27:37<41:34,  1.66s/it, Loss=3.0625, Top1=N/A, LR=0.026082]   2025-11-06 21:26:41,712 - INFO - Step 156124: {'train_loss_batch': 2.48903489112854, 'train_lr': 0.026081814102781185, 'batch_time': 1.6554147554086995, 'data_time': 0.004659674146196821}
2025-11-06 21:26:41 Train Epoch 062:  44%|████▍     | 1100/2502 [30:22<38:28,  1.65s/it, Loss=3.0601, Top1=N/A, LR=0.026082]2025-11-06 21:29:27,028 - INFO - Step 156224: {'train_loss_batch': 2.571753978729248, 'train_lr': 0.026081814102781185, 'batch_time': 1.6552091404917455, 'data_time': 0.0043270966878920444}
2025-11-06 21:29:27 Train Epoch 062:  48%|████▊     | 1200/2502 [33:07<35:48,  1.65s/it, Loss=3.0599, Top1=N/A, LR=0.026082]2025-11-06 21:32:12,219 - INFO - Step 156324: {'train_loss_batch': 4.849578857421875, 'train_lr': 0.026081814102781185, 'batch_time': 1.654934537103035, 'data_time': 0.004054280145281459}
2025-11-06 21:32:12 Train Epoch 062:  52%|█████▏    | 1300/2502 [35:53<33:15,  1.66s/it, Loss=3.0601, Top1=N/A, LR=0.026082]2025-11-06 21:34:58,224 - INFO - Step 156424: {'train_loss_batch': 2.3660354614257812, 'train_lr': 0.026081814102781185, 'batch_time': 1.6553276572934854, 'data_time': 0.003823795106024306}
2025-11-06 21:34:58 Train Epoch 062:  56%|█████▌    | 1400/2502 [38:38<30:33,  1.66s/it, Loss=3.0678, Top1=65.17%, LR=0.026082]2025-11-06 21:37:43,434 - INFO - Step 156524: {'train_loss_batch': 2.3394384384155273, 'train_lr': 0.026081814102781185, 'batch_time': 1.6550971462758928, 'data_time': 0.0036251282879150738}
2025-11-06 21:37:43 Train Epoch 062:  60%|█████▉    | 1500/2502 [41:24<27:32,  1.65s/it, Loss=3.0706, Top1=N/A, LR=0.026082]   2025-11-06 21:40:29,316 - INFO - Step 156624: {'train_loss_batch': 4.077603340148926, 'train_lr': 0.026081814102781185, 'batch_time': 1.6553449903941806, 'data_time': 0.003451546536216253}
2025-11-06 21:40:29 Train Epoch 062:  64%|██████▍   | 1600/2502 [44:10<24:48,  1.65s/it, Loss=3.0668, Top1=N/A, LR=0.026082]2025-11-06 21:43:14,754 - INFO - Step 156724: {'train_loss_batch': 3.8455238342285156, 'train_lr': 0.026081814102781185, 'batch_time': 1.655284605050072, 'data_time': 0.003305038461679224}
2025-11-06 21:43:14 Train Epoch 062:  68%|██████▊   | 1700/2502 [46:55<22:09,  1.66s/it, Loss=3.0700, Top1=65.19%, LR=0.026082]2025-11-06 21:46:00,033 - INFO - Step 156824: {'train_loss_batch': 2.494682550430298, 'train_lr': 0.026081814102781185, 'batch_time': 1.6551376751491003, 'data_time': 0.0031707460637796212}
2025-11-06 21:46:00 Train Epoch 062:  72%|███████▏  | 1800/2502 [49:41<19:28,  1.66s/it, Loss=3.0736, Top1=N/A, LR=0.026082]   2025-11-06 21:48:45,829 - INFO - Step 156924: {'train_loss_batch': 4.017128944396973, 'train_lr': 0.026081814102781185, 'batch_time': 1.6552941677373094, 'data_time': 0.003056173652890389}
2025-11-06 21:48:45 Train Epoch 062:  76%|███████▌  | 1900/2502 [52:26<16:31,  1.65s/it, Loss=3.0801, Top1=N/A, LR=0.026082]2025-11-06 21:51:31,326 - INFO - Step 157024: {'train_loss_batch': 3.6224184036254883, 'train_lr': 0.026081814102781185, 'batch_time': 1.6552770414959437, 'data_time': 0.0029496001545094865}
2025-11-06 21:51:31 Train Epoch 062:  80%|███████▉  | 2000/2502 [55:11<13:54,  1.66s/it, Loss=3.0880, Top1=N/A, LR=0.026082]2025-11-06 21:54:16,522 - INFO - Step 157124: {'train_loss_batch': 3.9614498615264893, 'train_lr': 0.026081814102781185, 'batch_time': 1.6551109852044956, 'data_time': 0.0028515257637599656}
2025-11-06 21:54:16 Train Epoch 062:  84%|████████▍ | 2100/2502 [57:55<11:02,  1.65s/it, Loss=3.0943, Top1=65.16%, LR=0.026082]2025-11-06 21:57:00,514 - INFO - Step 157224: {'train_loss_batch': 2.2122645378112793, 'train_lr': 0.026081814102781185, 'batch_time': 1.6543880203461319, 'data_time': 0.0027611175075932266}
2025-11-06 21:57:00 Train Epoch 062:  88%|████████▊ | 2200/2502 [1:00:40<08:19,  1.65s/it, Loss=3.0952, Top1=N/A, LR=0.026082]   2025-11-06 21:59:45,448 - INFO - Step 157324: {'train_loss_batch': 3.88136887550354, 'train_lr': 0.026081814102781185, 'batch_time': 1.6541587296858098, 'data_time': 0.002684797497566913}
2025-11-06 21:59:45 Train Epoch 062:  92%|█████████▏| 2300/2502 [1:03:25<05:32,  1.65s/it, Loss=3.0872, Top1=N/A, LR=0.026082]2025-11-06 22:02:29,959 - INFO - Step 157424: {'train_loss_batch': 3.629413604736328, 'train_lr': 0.026081814102781185, 'batch_time': 1.6537652137951973, 'data_time': 0.002611697315910701}
2025-11-06 22:02:29 Train Epoch 062:  96%|█████████▌| 2400/2502 [1:06:10<02:47,  1.65s/it, Loss=3.0875, Top1=65.18%, LR=0.026082]2025-11-06 22:05:15,144 - INFO - Step 157524: {'train_loss_batch': 2.395752191543579, 'train_lr': 0.026081814102781185, 'batch_time': 1.6536855595353146, 'data_time': 0.0025453258682816586}
2025-11-06 22:05:15 Train Epoch 062: 100%|█████████▉| 2500/2502 [1:08:55<00:03,  1.65s/it, Loss=3.0909, Top1=N/A, LR=0.026082]   2025-11-06 22:08:00,327 - INFO - Step 157624: {'train_loss_batch': 4.4109296798706055, 'train_lr': 0.026081814102781185, 'batch_time': 1.6536112119559525, 'data_time': 0.002517510394676358}
2025-11-06 22:08:00 Train Epoch 062: 100%|██████████| 2502/2502 [1:08:57<00:00,  1.65s/it, Loss=3.0909, Top1=N/A, LR=0.026082]
2025-11-06 22:08:02 Val Epoch 062:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 22:08:07   with torch.cuda.amp.autocast():
2025-11-06 22:08:07 Val Epoch 062: 100%|██████████| 98/98 [01:48<00:00,  1.11s/it, Loss=2.2223, Top1=69.13%, Top5=89.60%]
2025-11-06 22:09:51 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 22:09:51   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 22:09:51 2025-11-06 22:09:51,502 - INFO - Step 62: {'epoch': 62, 'learning_rate': 0.02447599349692057, 'train_loss': 3.091610206593331, 'train_top1': 65.20760436776062, 'train_top5': 85.49710424710425, 'train_precision': 65.05679170243177, 'train_recall': 65.07639281047257, 'train_f1': 64.88968859928741, 'val_loss': 2.2222805963134764, 'val_top1': 69.12999999023438, 'val_top5': 89.60399999023437, 'val_precision': 70.85083742679575, 'val_recall': 69.13, 'val_f1': 68.71684192303117}
2025-11-06 22:09:51 2025-11-06 22:09:51,504 - INFO - Epoch 062 Summary - LR: 0.024476, Train Loss: 3.0916, Val Loss: 2.2223, Val F1: 68.72%, Val Precision: 70.85%, Val Recall: 69.13%
2025-11-06 22:09:52 Train Epoch 063:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 62 that is less than the current step 157624. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 22:09:56 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 22:09:56   with torch.cuda.amp.autocast():
2025-11-06 22:09:57 Train Epoch 063:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.5959, Top1=N/A, LR=0.024476]2025-11-06 22:09:57,850 - INFO - Step 157626: {'train_loss_batch': 3.595932960510254, 'train_lr': 0.02447599349692057, 'batch_time': 5.643421173095703, 'data_time': 4.005238771438599}
2025-11-06 22:09:57 Train Epoch 063:   4%|▍         | 100/2502 [02:51<1:06:31,  1.66s/it, Loss=3.0411, Top1=N/A, LR=0.024476]2025-11-06 22:12:43,371 - INFO - Step 157726: {'train_loss_batch': 3.452935218811035, 'train_lr': 0.02447599349692057, 'batch_time': 1.694695475077865, 'data_time': 0.04069952209397118}
2025-11-06 22:12:43 Train Epoch 063:   8%|▊         | 200/2502 [05:35<1:02:33,  1.63s/it, Loss=3.0135, Top1=65.85%, LR=0.024476]2025-11-06 22:15:27,636 - INFO - Step 157826: {'train_loss_batch': 2.24859619140625, 'train_lr': 0.02447599349692057, 'batch_time': 1.6688029540711968, 'data_time': 0.020978516014061165}
2025-11-06 22:15:27 Train Epoch 063:  12%|█▏        | 300/2502 [08:20<1:00:57,  1.66s/it, Loss=3.0440, Top1=N/A, LR=0.024476]   2025-11-06 22:18:12,535 - INFO - Step 157926: {'train_loss_batch': 4.05301570892334, 'train_lr': 0.02447599349692057, 'batch_time': 1.6622215624267476, 'data_time': 0.014354994130689044}
2025-11-06 22:18:12 Train Epoch 063:  16%|█▌        | 400/2502 [11:05<57:57,  1.65s/it, Loss=3.0651, Top1=N/A, LR=0.024476]2025-11-06 22:20:58,086 - INFO - Step 158026: {'train_loss_batch': 2.951526165008545, 'train_lr': 0.02447599349692057, 'batch_time': 1.660546663098799, 'data_time': 0.011021763309278988}
2025-11-06 22:21:14 Train Epoch 063:  20%|█▉        | 500/2502 [13:51<55:25,  1.66s/it, Loss=3.0923, Top1=N/A, LR=0.024476]2025-11-06 22:23:43,423 - INFO - Step 158126: {'train_loss_batch': 2.935145378112793, 'train_lr': 0.02447599349692057, 'batch_time': 1.6591141576062658, 'data_time': 0.009024690487189683}
2025-11-06 22:23:43 Train Epoch 063:  24%|██▍       | 600/2502 [16:36<52:08,  1.64s/it, Loss=3.0863, Top1=65.58%, LR=0.024476]2025-11-06 22:26:28,686 - INFO - Step 158226: {'train_loss_batch': 2.2506542205810547, 'train_lr': 0.02447599349692057, 'batch_time': 1.6580349046259673, 'data_time': 0.007693581493841035}
2025-11-06 22:26:28 Train Epoch 063:  28%|██▊       | 700/2502 [19:21<49:36,  1.65s/it, Loss=3.0632, Top1=N/A, LR=0.024476]   2025-11-06 22:29:13,635 - INFO - Step 158326: {'train_loss_batch': 2.481975555419922, 'train_lr': 0.02447599349692057, 'batch_time': 1.6568157451809218, 'data_time': 0.006746497541963629}
2025-11-06 22:29:18 Train Epoch 063:  32%|███▏      | 800/2502 [22:07<47:15,  1.67s/it, Loss=3.0688, Top1=65.60%, LR=0.024476]2025-11-06 22:31:59,433 - INFO - Step 158426: {'train_loss_batch': 2.392310857772827, 'train_lr': 0.02447599349692057, 'batch_time': 1.6569604257519326, 'data_time': 0.006025611684563455}
2025-11-06 22:31:59 Train Epoch 063:  36%|███▌      | 900/2502 [24:52<44:17,  1.66s/it, Loss=3.0670, Top1=N/A, LR=0.024476]   2025-11-06 22:34:44,279 - INFO - Step 158526: {'train_loss_batch': 3.3314805030822754, 'train_lr': 0.02447599349692057, 'batch_time': 1.656017014242039, 'data_time': 0.0054707778545383876}
2025-11-06 22:34:44 Train Epoch 063:  40%|███▉      | 1000/2502 [27:36<41:31,  1.66s/it, Loss=3.0664, Top1=65.61%, LR=0.024476]2025-11-06 22:37:28,777 - INFO - Step 158626: {'train_loss_batch': 2.356189250946045, 'train_lr': 0.02447599349692057, 'batch_time': 1.654913980644066, 'data_time': 0.005019595930269072}
2025-11-06 22:37:28 Train Epoch 063:  44%|████▍     | 1100/2502 [30:21<38:17,  1.64s/it, Loss=3.0678, Top1=N/A, LR=0.024476]   2025-11-06 22:40:13,605 - INFO - Step 158726: {'train_loss_batch': 2.789524793624878, 'train_lr': 0.02447599349692057, 'batch_time': 1.6543116978793444, 'data_time': 0.004658667419738492}
2025-11-06 22:40:13 Train Epoch 063:  48%|████▊     | 1200/2502 [33:06<35:57,  1.66s/it, Loss=3.0772, Top1=N/A, LR=0.024476]2025-11-06 22:42:59,182 - INFO - Step 158826: {'train_loss_batch': 3.7537426948547363, 'train_lr': 0.02447599349692057, 'batch_time': 1.6544327712078872, 'data_time': 0.004355809571443252}
2025-11-06 22:42:59 Train Epoch 063:  52%|█████▏    | 1300/2502 [35:52<33:08,  1.65s/it, Loss=3.0772, Top1=N/A, LR=0.024476]2025-11-06 22:45:44,999 - INFO - Step 158926: {'train_loss_batch': 4.59678316116333, 'train_lr': 0.02447599349692057, 'batch_time': 1.654719760837599, 'data_time': 0.004099081150849538}
2025-11-06 22:45:45 Train Epoch 063:  56%|█████▌    | 1400/2502 [38:37<30:00,  1.63s/it, Loss=3.0709, Top1=N/A, LR=0.024476]2025-11-06 22:48:29,557 - INFO - Step 159026: {'train_loss_batch': 3.7081503868103027, 'train_lr': 0.02447599349692057, 'batch_time': 1.6540675311323407, 'data_time': 0.00387608094525116}
2025-11-06 22:49:23 Train Epoch 063:  60%|█████▉    | 1500/2502 [41:22<27:39,  1.66s/it, Loss=3.0763, Top1=N/A, LR=0.024476]2025-11-06 22:51:14,550 - INFO - Step 159126: {'train_loss_batch': 3.973539352416992, 'train_lr': 0.02447599349692057, 'batch_time': 1.6537913507338289, 'data_time': 0.003681703220598702}
2025-11-06 23:02:31 Train Epoch 063:  64%|██████▍   | 1600/2502 [44:07<24:57,  1.66s/it, Loss=3.0718, Top1=N/A, LR=0.024476]2025-11-06 22:53:59,969 - INFO - Step 159226: {'train_loss_batch': 3.4842848777770996, 'train_lr': 0.02447599349692057, 'batch_time': 1.653816015403171, 'data_time': 0.003512370296004115}
2025-11-06 23:02:31 Train Epoch 063:  68%|██████▊   | 1700/2502 [46:52<22:04,  1.65s/it, Loss=3.0706, Top1=N/A, LR=0.024476]2025-11-06 22:56:44,369 - INFO - Step 159326: {'train_loss_batch': 4.085127353668213, 'train_lr': 0.02447599349692057, 'batch_time': 1.6532391729528662, 'data_time': 0.0033640170503826017}
2025-11-06 23:02:31 Train Epoch 063:  72%|███████▏  | 1800/2502 [49:37<19:15,  1.65s/it, Loss=3.0655, Top1=N/A, LR=0.024476]2025-11-06 22:59:29,543 - INFO - Step 159426: {'train_loss_batch': 4.717080116271973, 'train_lr': 0.02447599349692057, 'batch_time': 1.6531559119682588, 'data_time': 0.0032285877494663746}
2025-11-06 23:02:31 Train Epoch 063:  76%|███████▌  | 1900/2502 [52:23<16:37,  1.66s/it, Loss=3.0608, Top1=N/A, LR=0.024476]2025-11-06 23:02:15,359 - INFO - Step 159526: {'train_loss_batch': 2.3548901081085205, 'train_lr': 0.02447599349692057, 'batch_time': 1.6534187044989492, 'data_time': 0.0031108177692748444}
2025-11-06 23:02:31 Train Epoch 063:  80%|███████▉  | 2000/2502 [55:08<13:50,  1.65s/it, Loss=3.0629, Top1=N/A, LR=0.024476]2025-11-06 23:05:01,199 - INFO - Step 159626: {'train_loss_batch': 2.7103922367095947, 'train_lr': 0.02447599349692057, 'batch_time': 1.6536677829746245, 'data_time': 0.0030055085400948816}
2025-11-06 23:05:02 Train Epoch 063:  84%|████████▍ | 2100/2502 [57:53<11:02,  1.65s/it, Loss=3.0680, Top1=N/A, LR=0.024476]2025-11-06 23:07:46,075 - INFO - Step 159726: {'train_loss_batch': 3.116427421569824, 'train_lr': 0.02447599349692057, 'batch_time': 1.6534339494219512, 'data_time': 0.002911261976587948}
2025-11-06 23:07:46 Train Epoch 063:  88%|████████▊ | 2200/2502 [1:00:38<08:17,  1.65s/it, Loss=3.0671, Top1=65.51%, LR=0.024476]2025-11-06 23:10:31,145 - INFO - Step 159826: {'train_loss_batch': 2.35994291305542, 'train_lr': 0.02447599349692057, 'batch_time': 1.653309662956262, 'data_time': 0.002822855395222187}
2025-11-06 23:10:46 Train Epoch 063:  92%|█████████▏| 2300/2502 [1:03:23<05:34,  1.66s/it, Loss=3.0680, Top1=N/A, LR=0.024476]   2025-11-06 23:13:15,724 - INFO - Step 159926: {'train_loss_batch': 3.933414936065674, 'train_lr': 0.02447599349692057, 'batch_time': 1.6529827590405242, 'data_time': 0.002743590349531443}
2025-11-06 23:13:25 Train Epoch 063:  96%|█████████▌| 2400/2502 [1:06:09<02:48,  1.65s/it, Loss=3.0697, Top1=N/A, LR=0.024476]2025-11-06 23:16:01,568 - INFO - Step 160026: {'train_loss_batch': 3.323335647583008, 'train_lr': 0.02447599349692057, 'batch_time': 1.653210088443081, 'data_time': 0.0026723249213787873}
2025-11-06 23:16:01 Train Epoch 063: 100%|█████████▉| 2500/2502 [1:08:54<00:03,  1.67s/it, Loss=3.0684, Top1=N/A, LR=0.024476]2025-11-06 23:18:46,970 - INFO - Step 160126: {'train_loss_batch': 3.9525303840637207, 'train_lr': 0.02447599349692057, 'batch_time': 1.6532423045338749, 'data_time': 0.002640363551387306}
2025-11-06 23:19:13 Train Epoch 063: 100%|██████████| 2502/2502 [1:08:56<00:00,  1.65s/it, Loss=3.0684, Top1=N/A, LR=0.024476]
2025-11-06 23:19:13 Val Epoch 063:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 23:19:13   with torch.cuda.amp.autocast():
2025-11-06 23:19:13 Val Epoch 063: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=2.1282, Top1=69.67%, Top5=89.74%]
2025-11-06 23:21:33 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-06 23:21:33   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-06 23:21:33 2025-11-06 23:20:40,128 - INFO - Step 63: {'epoch': 63, 'learning_rate': 0.02290504244315989, 'train_loss': 3.0681484775672807, 'train_top1': 65.47433035714286, 'train_top5': 85.67920918367346, 'train_precision': 65.28537488046608, 'train_recall': 65.3350618276568, 'train_f1': 65.14616519621579, 'val_loss': 2.1282071207427977, 'val_top1': 69.66599998779297, 'val_top5': 89.73599999023438, 'val_precision': 71.12827792874555, 'val_recall': 69.674, 'val_f1': 69.1902943851768}
2025-11-06 23:21:33 2025-11-06 23:20:40,130 - INFO - Epoch 063 Summary - LR: 0.022905, Train Loss: 3.0681, Val Loss: 2.1282, Val F1: 69.19%, Val Precision: 71.13%, Val Recall: 69.67%
2025-11-06 23:21:33 2025-11-06 23:20:43,114 - INFO - New best model saved with validation accuracy: 69.666%
2025-11-06 23:21:33 2025-11-06 23:20:43,114 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_064.pth
2025-11-06 23:21:33 Train Epoch 064:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 63 that is less than the current step 160126. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-06 23:21:33 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-06 23:21:33   with torch.cuda.amp.autocast():
2025-11-06 23:21:33 Train Epoch 064:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.3123, Top1=66.21%, LR=0.022905]2025-11-06 23:20:48,392 - INFO - Step 160128: {'train_loss_batch': 2.312297821044922, 'train_lr': 0.02290504244315989, 'batch_time': 5.275818347930908, 'data_time': 3.6321678161621094}
2025-11-06 23:21:33 Train Epoch 064:   4%|▍         | 100/2502 [02:51<1:06:19,  1.66s/it, Loss=3.1630, Top1=66.33%, LR=0.022905]2025-11-06 23:23:34,271 - INFO - Step 160228: {'train_loss_batch': 2.2795848846435547, 'train_lr': 0.02290504244315989, 'batch_time': 1.6946109474295437, 'data_time': 0.03699829318735859}
2025-11-06 23:23:38 Train Epoch 064:   8%|▊         | 200/2502 [05:36<1:02:54,  1.64s/it, Loss=3.0518, Top1=N/A, LR=0.022905]   2025-11-06 23:26:19,752 - INFO - Step 160328: {'train_loss_batch': 2.3306877613067627, 'train_lr': 0.02290504244315989, 'batch_time': 1.6748048808444198, 'data_time': 0.019112509874562127}
2025-11-06 23:26:26 Train Epoch 064:  12%|█▏        | 300/2502 [08:21<1:00:39,  1.65s/it, Loss=3.0787, Top1=N/A, LR=0.022905]2025-11-06 23:29:04,537 - INFO - Step 160428: {'train_loss_batch': 3.231522798538208, 'train_lr': 0.02290504244315989, 'batch_time': 1.665850967267819, 'data_time': 0.013107554065032655}
2025-11-06 23:31:58 Train Epoch 064:  16%|█▌        | 400/2502 [11:06<57:49,  1.65s/it, Loss=3.0833, Top1=N/A, LR=0.022905]2025-11-06 23:31:49,528 - INFO - Step 160528: {'train_loss_batch': 2.397280216217041, 'train_lr': 0.02290504244315989, 'batch_time': 1.661873672371196, 'data_time': 0.010086304529052126}
2025-11-06 23:31:58 Train Epoch 064:  20%|█▉        | 494/2502 [13:39<55:13,  1.65s/it, Loss=3.0833, Top1=N/A, LR=0.022905]
2025-11-06 23:34:34 Train Epoch 064:  24%|██▍       | 600/2502 [16:36<52:45,  1.66s/it, Loss=3.0640, Top1=N/A, LR=0.022905]   2025-11-06 23:37:19,754 - INFO - Step 160728: {'train_loss_batch': 4.960076332092285, 'train_lr': 0.02290504244315989, 'batch_time': 1.6582987074447353, 'data_time': 0.007063228159696608}
2025-11-06 23:37:19 Train Epoch 064:  28%|██▊       | 700/2502 [19:22<49:51,  1.66s/it, Loss=3.0737, Top1=N/A, LR=0.022905]2025-11-06 23:40:05,340 - INFO - Step 160828: {'train_loss_batch': 3.7198972702026367, 'train_lr': 0.02290504244315989, 'batch_time': 1.6579503094758865, 'data_time': 0.006200518655708955}
2025-11-06 23:40:05 Train Epoch 064:  32%|███▏      | 800/2502 [22:07<46:59,  1.66s/it, Loss=3.0893, Top1=66.30%, LR=0.022905]2025-11-06 23:42:50,635 - INFO - Step 160928: {'train_loss_batch': 2.4349112510681152, 'train_lr': 0.02290504244315989, 'batch_time': 1.6573255779442566, 'data_time': 0.005547292521235053}
2025-11-06 23:42:50 Train Epoch 064:  36%|███▌      | 900/2502 [24:53<44:26,  1.66s/it, Loss=3.0766, Top1=N/A, LR=0.022905]   2025-11-06 23:45:36,349 - INFO - Step 161028: {'train_loss_batch': 4.647870063781738, 'train_lr': 0.02290504244315989, 'batch_time': 1.6573046227538757, 'data_time': 0.005044142228781714}
2025-11-06 23:45:36 Train Epoch 064:  40%|███▉      | 1000/2502 [27:38<41:33,  1.66s/it, Loss=3.0721, Top1=N/A, LR=0.022905]2025-11-06 23:48:21,510 - INFO - Step 161128: {'train_loss_batch': 3.935018539428711, 'train_lr': 0.02290504244315989, 'batch_time': 1.6567353371020916, 'data_time': 0.004638425596467741}
2025-11-06 23:48:21 Train Epoch 064:  44%|████▍     | 1100/2502 [30:23<38:20,  1.64s/it, Loss=3.0714, Top1=N/A, LR=0.022905]2025-11-06 23:51:06,735 - INFO - Step 161228: {'train_loss_batch': 2.753995895385742, 'train_lr': 0.02290504244315989, 'batch_time': 1.6563278431247084, 'data_time': 0.004310189540769489}
2025-11-06 23:51:06 Train Epoch 064:  48%|████▊     | 1200/2502 [33:09<35:50,  1.65s/it, Loss=3.0807, Top1=N/A, LR=0.022905]2025-11-06 23:53:52,276 - INFO - Step 161328: {'train_loss_batch': 3.4941492080688477, 'train_lr': 0.02290504244315989, 'batch_time': 1.656251690568376, 'data_time': 0.004036368974341044}
2025-11-06 23:53:52 Train Epoch 064:  52%|█████▏    | 1300/2502 [35:54<33:15,  1.66s/it, Loss=3.0764, Top1=N/A, LR=0.022905]2025-11-06 23:56:37,466 - INFO - Step 161428: {'train_loss_batch': 4.010237693786621, 'train_lr': 0.02290504244315989, 'batch_time': 1.6559172175097336, 'data_time': 0.003803332524515866}
2025-11-06 23:56:37 Train Epoch 064:  56%|█████▌    | 1400/2502 [38:39<30:27,  1.66s/it, Loss=3.0705, Top1=N/A, LR=0.022905]2025-11-06 23:59:22,655 - INFO - Step 161528: {'train_loss_batch': 2.657510280609131, 'train_lr': 0.02290504244315989, 'batch_time': 1.6556294442584565, 'data_time': 0.003607822264372494}
2025-11-06 23:59:22 Train Epoch 064:  60%|█████▉    | 1500/2502 [41:25<27:40,  1.66s/it, Loss=3.0657, Top1=N/A, LR=0.022905]2025-11-07 00:02:08,458 - INFO - Step 161628: {'train_loss_batch': 3.786099910736084, 'train_lr': 0.02290504244315989, 'batch_time': 1.6557888673354118, 'data_time': 0.0034386711387456377}
2025-11-07 00:02:08 Train Epoch 064:  64%|██████▍   | 1600/2502 [44:10<24:50,  1.65s/it, Loss=3.0705, Top1=N/A, LR=0.022905]2025-11-07 00:04:53,693 - INFO - Step 161728: {'train_loss_batch': 3.7551231384277344, 'train_lr': 0.02290504244315989, 'batch_time': 1.6555742268857176, 'data_time': 0.003286334144406435}
2025-11-07 00:04:53 Train Epoch 064:  68%|██████▊   | 1700/2502 [46:56<21:57,  1.64s/it, Loss=3.0766, Top1=N/A, LR=0.022905]2025-11-07 00:07:39,119 - INFO - Step 161828: {'train_loss_batch': 3.579484701156616, 'train_lr': 0.02290504244315989, 'batch_time': 1.6554962502445354, 'data_time': 0.0031556313630765075}
2025-11-07 00:07:39 Train Epoch 064:  72%|███████▏  | 1800/2502 [49:41<19:19,  1.65s/it, Loss=3.0764, Top1=66.09%, LR=0.022905]2025-11-07 00:10:24,498 - INFO - Step 161928: {'train_loss_batch': 2.361419200897217, 'train_lr': 0.02290504244315989, 'batch_time': 1.6554018545653806, 'data_time': 0.003038378307251451}
2025-11-07 00:10:24 Train Epoch 064:  76%|███████▌  | 1900/2502 [52:26<16:38,  1.66s/it, Loss=3.0823, Top1=N/A, LR=0.022905]   2025-11-07 00:13:09,554 - INFO - Step 162028: {'train_loss_batch': 3.140040397644043, 'train_lr': 0.02290504244315989, 'batch_time': 1.6551469149933182, 'data_time': 0.002928619821218363}
2025-11-07 00:13:09 Train Epoch 064:  80%|███████▉  | 2000/2502 [55:11<13:44,  1.64s/it, Loss=3.0785, Top1=N/A, LR=0.022905]2025-11-07 00:15:54,473 - INFO - Step 162128: {'train_loss_batch': 2.330573081970215, 'train_lr': 0.02290504244315989, 'batch_time': 1.6548493598354632, 'data_time': 0.002832027866148103}
2025-11-07 00:15:54 Train Epoch 064:  84%|████████▍ | 2100/2502 [57:56<11:03,  1.65s/it, Loss=3.0788, Top1=N/A, LR=0.022905]2025-11-07 00:18:39,918 - INFO - Step 162228: {'train_loss_batch': 2.315699815750122, 'train_lr': 0.02290504244315989, 'batch_time': 1.6548299374096964, 'data_time': 0.002745442251997979}
2025-11-07 00:18:39 Train Epoch 064:  88%|████████▊ | 2200/2502 [1:00:41<08:14,  1.64s/it, Loss=3.0756, Top1=N/A, LR=0.022905]2025-11-07 00:21:24,424 - INFO - Step 162328: {'train_loss_batch': 3.180638313293457, 'train_lr': 0.02290504244315989, 'batch_time': 1.6543861116836527, 'data_time': 0.0026646146769959077}
2025-11-07 00:21:24 Train Epoch 064:  92%|█████████▏| 2300/2502 [1:03:26<05:34,  1.66s/it, Loss=3.0798, Top1=N/A, LR=0.022905]2025-11-07 00:24:09,726 - INFO - Step 162428: {'train_loss_batch': 3.8018674850463867, 'train_lr': 0.02290504244315989, 'batch_time': 1.654326480246274, 'data_time': 0.0025909471283887377}
2025-11-07 00:24:09 Train Epoch 064:  96%|█████████▌| 2400/2502 [1:06:12<02:49,  1.66s/it, Loss=3.0788, Top1=66.13%, LR=0.022905]2025-11-07 00:26:55,711 - INFO - Step 162528: {'train_loss_batch': 2.3052072525024414, 'train_lr': 0.02290504244315989, 'batch_time': 1.6545568264210935, 'data_time': 0.002523700876168438}
2025-11-07 00:26:55 Train Epoch 064: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.64s/it, Loss=3.0809, Top1=N/A, LR=0.022905]   2025-11-07 00:29:39,791 - INFO - Step 162628: {'train_loss_batch': 3.392714023590088, 'train_lr': 0.02290504244315989, 'batch_time': 1.654006414344815, 'data_time': 0.0024929882668819677}
2025-11-07 00:29:39 Train Epoch 064: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=3.0809, Top1=N/A, LR=0.022905]
2025-11-07 00:29:42 Val Epoch 064:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 00:29:46   with torch.cuda.amp.autocast():
2025-11-07 00:29:47 Val Epoch 064: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=2.1276, Top1=69.92%, Top5=90.03%]
2025-11-07 00:31:33 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 00:31:33   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 00:31:33 2025-11-07 00:31:33,871 - INFO - Step 64: {'epoch': 64, 'learning_rate': 0.021371106671675143, 'train_loss': 3.08069446980715, 'train_top1': 66.11052738611714, 'train_top5': 85.99596664859003, 'train_precision': 65.91064183555164, 'train_recall': 65.98013350032696, 'train_f1': 65.77176578931268, 'val_loss': 2.127567134246826, 'val_top1': 69.91600000976563, 'val_top5': 90.02599998535156, 'val_precision': 71.25973221413078, 'val_recall': 69.908, 'val_f1': 69.33395257951042}
2025-11-07 00:31:33 2025-11-07 00:31:33,873 - INFO - Epoch 064 Summary - LR: 0.021371, Train Loss: 3.0807, Val Loss: 2.1276, Val F1: 69.33%, Val Precision: 71.26%, Val Recall: 69.91%
2025-11-07 00:31:37 2025-11-07 00:31:37,241 - INFO - New best model saved with validation accuracy: 69.916%
2025-11-07 00:31:37 2025-11-07 00:31:37,242 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_065.pth
2025-11-07 00:31:37 Train Epoch 065:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 00:31:40   with torch.cuda.amp.autocast():
2025-11-07 00:31:42 Train Epoch 065:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.7756, Top1=N/A, LR=0.021371]2025-11-07 00:31:42,537 - INFO - Step 162630: {'train_loss_batch': 2.7756268978118896, 'train_lr': 0.021371106671675143, 'batch_time': 5.292219877243042, 'data_time': 3.6252553462982178}
2025-11-07 00:31:42 Train Epoch 065:   0%|          | 1/2502 [00:05<3:40:45,  5.30s/it, Loss=2.7756, Top1=N/A, LR=0.021371]wandb: WARNING Tried to log to step 64 that is less than the current step 162628. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 00:31:44 Train Epoch 065:   4%|▍         | 100/2502 [02:50<1:05:59,  1.65s/it, Loss=3.0699, Top1=N/A, LR=0.021371]2025-11-07 00:34:27,424 - INFO - Step 162730: {'train_loss_batch': 3.3259620666503906, 'train_lr': 0.021371106671675143, 'batch_time': 1.6849598459678121, 'data_time': 0.03692140673646833}
2025-11-07 00:34:27 Train Epoch 065:   8%|▊         | 200/2502 [05:36<1:03:44,  1.66s/it, Loss=3.0270, Top1=N/A, LR=0.021371]2025-11-07 00:37:13,258 - INFO - Step 162830: {'train_loss_batch': 2.93194317817688, 'train_lr': 0.021371106671675143, 'batch_time': 1.6717153081846474, 'data_time': 0.019065708663333115}
2025-11-07 00:37:13 Train Epoch 065:  12%|█▏        | 300/2502 [08:21<1:00:51,  1.66s/it, Loss=3.0501, Top1=66.95%, LR=0.021371]2025-11-07 00:39:59,038 - INFO - Step 162930: {'train_loss_batch': 2.275787830352783, 'train_lr': 0.021371106671675143, 'batch_time': 1.6670885513787255, 'data_time': 0.013079716121635564}
2025-11-07 00:39:59 Train Epoch 065:  16%|█▌        | 400/2502 [11:07<57:58,  1.66s/it, Loss=3.0576, Top1=N/A, LR=0.021371]   2025-11-07 00:42:44,430 - INFO - Step 163030: {'train_loss_batch': 3.872835874557495, 'train_lr': 0.021371106671675143, 'batch_time': 1.6638050311224122, 'data_time': 0.010065293966088806}
2025-11-07 00:42:44 Train Epoch 065:  20%|█▉        | 500/2502 [13:52<55:30,  1.66s/it, Loss=3.0627, Top1=N/A, LR=0.021371]2025-11-07 00:45:29,567 - INFO - Step 163130: {'train_loss_batch': 4.05582332611084, 'train_lr': 0.021371106671675143, 'batch_time': 1.6613224423574116, 'data_time': 0.008259567671907162}
2025-11-07 00:45:29 Train Epoch 065:  24%|██▍       | 600/2502 [16:37<52:25,  1.65s/it, Loss=3.0454, Top1=N/A, LR=0.021371]2025-11-07 00:48:14,550 - INFO - Step 163230: {'train_loss_batch': 4.7014617919921875, 'train_lr': 0.021371106671675143, 'batch_time': 1.6594108368910092, 'data_time': 0.007055832026603813}
2025-11-07 00:48:14 Train Epoch 065:  28%|██▊       | 700/2502 [19:21<49:07,  1.64s/it, Loss=3.0546, Top1=N/A, LR=0.021371]2025-11-07 00:50:58,390 - INFO - Step 163330: {'train_loss_batch': 3.851980686187744, 'train_lr': 0.021371106671675143, 'batch_time': 1.6564126524877616, 'data_time': 0.006189215030207614}
2025-11-07 00:50:58 Train Epoch 065:  32%|███▏      | 800/2502 [22:06<46:57,  1.66s/it, Loss=3.0560, Top1=N/A, LR=0.021371]2025-11-07 00:53:43,689 - INFO - Step 163430: {'train_loss_batch': 2.4641170501708984, 'train_lr': 0.021371106671675143, 'batch_time': 1.6559851648804549, 'data_time': 0.005543798096617509}
2025-11-07 00:53:43 Train Epoch 065:  36%|███▌      | 900/2502 [24:52<44:19,  1.66s/it, Loss=3.0499, Top1=66.51%, LR=0.021371]2025-11-07 00:56:29,463 - INFO - Step 163530: {'train_loss_batch': 2.3025527000427246, 'train_lr': 0.021371106671675143, 'batch_time': 1.6561794868452302, 'data_time': 0.005045048007689888}
2025-11-07 00:56:29 Train Epoch 065:  40%|███▉      | 1000/2502 [27:38<41:29,  1.66s/it, Loss=3.0520, Top1=66.45%, LR=0.021371]2025-11-07 00:59:15,397 - INFO - Step 163630: {'train_loss_batch': 2.3014907836914062, 'train_lr': 0.021371106671675143, 'batch_time': 1.6564946993962153, 'data_time': 0.004645705342173696}
2025-11-07 00:59:15 Train Epoch 065:  44%|████▍     | 1100/2502 [30:24<38:46,  1.66s/it, Loss=3.0388, Top1=N/A, LR=0.021371]   2025-11-07 01:02:01,329 - INFO - Step 163730: {'train_loss_batch': 2.5265843868255615, 'train_lr': 0.021371106671675143, 'batch_time': 1.6567510380515393, 'data_time': 0.004311323815535459}
2025-11-07 01:02:01 Train Epoch 065:  48%|████▊     | 1200/2502 [33:10<36:04,  1.66s/it, Loss=3.0308, Top1=N/A, LR=0.021371]2025-11-07 01:04:47,324 - INFO - Step 163830: {'train_loss_batch': 4.032723903656006, 'train_lr': 0.021371106671675143, 'batch_time': 1.6570174650387601, 'data_time': 0.00403679380011896}
2025-11-07 01:04:47 Train Epoch 065:  52%|█████▏    | 1300/2502 [35:55<33:09,  1.66s/it, Loss=3.0390, Top1=N/A, LR=0.021371]2025-11-07 01:07:33,170 - INFO - Step 163930: {'train_loss_batch': 4.678668022155762, 'train_lr': 0.021371106671675143, 'batch_time': 1.657128083898323, 'data_time': 0.0038093113147873404}
2025-11-07 01:07:33 Train Epoch 065:  56%|█████▌    | 1400/2502 [38:41<30:28,  1.66s/it, Loss=3.0322, Top1=66.35%, LR=0.021371]2025-11-07 01:10:18,676 - INFO - Step 164030: {'train_loss_batch': 2.2839579582214355, 'train_lr': 0.021371106671675143, 'batch_time': 1.6569803804265526, 'data_time': 0.0036098938342250305}
2025-11-07 01:10:18 Train Epoch 065:  60%|█████▉    | 1500/2502 [41:27<27:47,  1.66s/it, Loss=3.0342, Top1=66.38%, LR=0.021371]2025-11-07 01:13:04,269 - INFO - Step 164130: {'train_loss_batch': 2.188307762145996, 'train_lr': 0.021371106671675143, 'batch_time': 1.6569103025579992, 'data_time': 0.0034362321532145886}
2025-11-07 01:13:04 Train Epoch 065:  64%|██████▍   | 1600/2502 [44:12<24:51,  1.65s/it, Loss=3.0365, Top1=N/A, LR=0.021371]   2025-11-07 01:15:49,639 - INFO - Step 164230: {'train_loss_batch': 3.8787736892700195, 'train_lr': 0.021371106671675143, 'batch_time': 1.656709552332433, 'data_time': 0.003284224415480681}
2025-11-07 01:15:49 Train Epoch 065:  68%|██████▊   | 1700/2502 [46:58<22:10,  1.66s/it, Loss=3.0385, Top1=N/A, LR=0.021371]2025-11-07 01:18:35,352 - INFO - Step 164330: {'train_loss_batch': 2.420316696166992, 'train_lr': 0.021371106671675143, 'batch_time': 1.6567342893857244, 'data_time': 0.0031531762544159884}
2025-11-07 01:18:35 Train Epoch 065:  72%|███████▏  | 1800/2502 [49:43<19:13,  1.64s/it, Loss=3.0401, Top1=N/A, LR=0.021371]2025-11-07 01:21:20,744 - INFO - Step 164430: {'train_loss_batch': 3.138251304626465, 'train_lr': 0.021371106671675143, 'batch_time': 1.6565776379620745, 'data_time': 0.003032852979847486}
2025-11-07 01:21:20 Train Epoch 065:  76%|███████▌  | 1900/2502 [52:28<16:29,  1.64s/it, Loss=3.0391, Top1=N/A, LR=0.021371]2025-11-07 01:24:05,945 - INFO - Step 164530: {'train_loss_batch': 4.085083961486816, 'train_lr': 0.021371106671675143, 'batch_time': 1.6563377402946236, 'data_time': 0.002925188901360445}
2025-11-07 01:24:05 Train Epoch 065:  80%|███████▉  | 2000/2502 [55:13<13:50,  1.66s/it, Loss=3.0332, Top1=N/A, LR=0.021371]2025-11-07 01:26:50,904 - INFO - Step 164630: {'train_loss_batch': 2.499150037765503, 'train_lr': 0.021371106671675143, 'batch_time': 1.6560001547249599, 'data_time': 0.0028296746592829073}
2025-11-07 01:26:50 Train Epoch 065:  84%|████████▍ | 2100/2502 [57:59<11:07,  1.66s/it, Loss=3.0257, Top1=N/A, LR=0.021371]2025-11-07 01:29:36,287 - INFO - Step 164730: {'train_loss_batch': 2.741058826446533, 'train_lr': 0.021371106671675143, 'batch_time': 1.6558964712060331, 'data_time': 0.0027413043676246525}
2025-11-07 01:29:36 Train Epoch 065:  88%|████████▊ | 2200/2502 [1:00:44<08:16,  1.64s/it, Loss=3.0350, Top1=N/A, LR=0.021371]2025-11-07 01:32:21,825 - INFO - Step 164830: {'train_loss_batch': 3.388235092163086, 'train_lr': 0.021371106671675143, 'batch_time': 1.6558731169876106, 'data_time': 0.0026626612911545}
2025-11-07 01:32:21 Train Epoch 065:  92%|█████████▏| 2300/2502 [1:03:29<05:34,  1.66s/it, Loss=3.0369, Top1=N/A, LR=0.021371]2025-11-07 01:35:06,847 - INFO - Step 164930: {'train_loss_batch': 3.048908233642578, 'train_lr': 0.021371106671675143, 'batch_time': 1.6556272612401373, 'data_time': 0.002589915949901463}
2025-11-07 01:35:06 Train Epoch 065:  96%|█████████▌| 2400/2502 [1:06:14<02:47,  1.65s/it, Loss=3.0411, Top1=66.35%, LR=0.021371]2025-11-07 01:37:51,759 - INFO - Step 165030: {'train_loss_batch': 2.3437564373016357, 'train_lr': 0.021371106671675143, 'batch_time': 1.6553560270462766, 'data_time': 0.0025249762815120367}
2025-11-07 01:37:51 Train Epoch 065: 100%|█████████▉| 2500/2502 [1:08:58<00:03,  1.66s/it, Loss=3.0413, Top1=N/A, LR=0.021371]   2025-11-07 01:40:36,100 - INFO - Step 165130: {'train_loss_batch': 3.7609140872955322, 'train_lr': 0.021371106671675143, 'batch_time': 1.6548787951707744, 'data_time': 0.0025033436027444493}
2025-11-07 01:40:36 Train Epoch 065: 100%|██████████| 2502/2502 [1:09:00<00:00,  1.65s/it, Loss=3.0413, Top1=N/A, LR=0.021371]
2025-11-07 01:40:38 Val Epoch 065:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 01:40:42   with torch.cuda.amp.autocast():
2025-11-07 01:40:43 Val Epoch 065: 100%|██████████| 98/98 [01:47<00:00,  1.10s/it, Loss=2.1189, Top1=70.56%, Top5=90.37%]
2025-11-07 01:42:26 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 01:42:26   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 01:42:26 2025-11-07 01:42:26,434 - INFO - Step 65: {'epoch': 65, 'learning_rate': 0.019876281354219084, 'train_loss': 3.0412071162848164, 'train_top1': 66.32857313575526, 'train_top5': 86.14177521510516, 'train_precision': 66.15983776752589, 'train_recall': 66.15488174284307, 'train_f1': 65.99218052605795, 'val_loss': 2.1188904177474974, 'val_top1': 70.56400001953125, 'val_top5': 90.36800000976562, 'val_precision': 71.80635392603556, 'val_recall': 70.564, 'val_f1': 70.18173956985345}
2025-11-07 01:42:26 2025-11-07 01:42:26,436 - INFO - Epoch 065 Summary - LR: 0.019876, Train Loss: 3.0412, Val Loss: 2.1189, Val F1: 70.18%, Val Precision: 71.81%, Val Recall: 70.56%
2025-11-07 01:42:29 2025-11-07 01:42:29,709 - INFO - New best model saved with validation accuracy: 70.564%
2025-11-07 01:42:29 2025-11-07 01:42:29,710 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_066.pth
2025-11-07 01:42:29 Train Epoch 066:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 01:42:33   with torch.cuda.amp.autocast():
2025-11-07 01:42:33 wandb: WARNING Tried to log to step 65 that is less than the current step 165130. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 01:42:35 Train Epoch 066:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.8166, Top1=N/A, LR=0.019876]2025-11-07 01:42:35,003 - INFO - Step 165132: {'train_loss_batch': 3.8166451454162598, 'train_lr': 0.019876281354219084, 'batch_time': 5.291258335113525, 'data_time': 3.654381275177002}
2025-11-07 01:42:35 Train Epoch 066:   4%|▍         | 100/2502 [02:50<1:05:56,  1.65s/it, Loss=3.0274, Top1=N/A, LR=0.019876]2025-11-07 01:45:20,409 - INFO - Step 165232: {'train_loss_batch': 2.2414138317108154, 'train_lr': 0.019876281354219084, 'batch_time': 1.6900737309219813, 'data_time': 0.03721028743403973}
2025-11-07 01:45:20 Train Epoch 066:   8%|▊         | 200/2502 [05:36<1:03:40,  1.66s/it, Loss=2.9988, Top1=N/A, LR=0.019876]2025-11-07 01:48:06,015 - INFO - Step 165332: {'train_loss_batch': 3.845024824142456, 'train_lr': 0.019876281354219084, 'batch_time': 1.6731477056569721, 'data_time': 0.019194906623802376}
2025-11-07 01:48:06 Train Epoch 066:  12%|█▏        | 300/2502 [08:22<1:00:41,  1.65s/it, Loss=2.9954, Top1=N/A, LR=0.019876]2025-11-07 01:50:52,000 - INFO - Step 165432: {'train_loss_batch': 2.3265538215637207, 'train_lr': 0.019876281354219084, 'batch_time': 1.668732184508314, 'data_time': 0.0131495664286059}
2025-11-07 01:50:52 Train Epoch 066:  16%|█▌        | 400/2502 [11:07<57:54,  1.65s/it, Loss=3.0303, Top1=67.17%, LR=0.019876]2025-11-07 01:53:37,637 - INFO - Step 165532: {'train_loss_batch': 2.3905344009399414, 'train_lr': 0.019876281354219084, 'batch_time': 1.6656467034632427, 'data_time': 0.010118247862171354}
2025-11-07 01:53:37 Train Epoch 066:  20%|█▉        | 500/2502 [13:53<55:10,  1.65s/it, Loss=3.0295, Top1=67.09%, LR=0.019876]2025-11-07 01:56:22,738 - INFO - Step 165632: {'train_loss_batch': 2.3047564029693604, 'train_lr': 0.019876281354219084, 'batch_time': 1.6627255999399517, 'data_time': 0.008305701429020622}
2025-11-07 01:56:22 Train Epoch 066:  24%|██▍       | 600/2502 [16:37<51:58,  1.64s/it, Loss=3.0320, Top1=N/A, LR=0.019876]   2025-11-07 01:59:07,667 - INFO - Step 165732: {'train_loss_batch': 2.9847099781036377, 'train_lr': 0.019876281354219084, 'batch_time': 1.6604889173079251, 'data_time': 0.0070918308518293895}
2025-11-07 01:59:07 Train Epoch 066:  28%|██▊       | 700/2502 [19:23<49:55,  1.66s/it, Loss=3.0196, Top1=N/A, LR=0.019876]2025-11-07 02:01:52,818 - INFO - Step 165832: {'train_loss_batch': 2.7590293884277344, 'train_lr': 0.019876281354219084, 'batch_time': 1.659207846060629, 'data_time': 0.006233432323548321}
2025-11-07 02:01:52 Train Epoch 066:  32%|███▏      | 800/2502 [22:06<46:29,  1.64s/it, Loss=2.9956, Top1=66.87%, LR=0.019876]2025-11-07 02:04:36,380 - INFO - Step 165932: {'train_loss_batch': 2.1912715435028076, 'train_lr': 0.019876281354219084, 'batch_time': 1.6562634880623122, 'data_time': 0.005570639682917411}
2025-11-07 02:04:36 Train Epoch 066:  36%|███▌      | 900/2502 [24:52<44:11,  1.66s/it, Loss=3.0126, Top1=N/A, LR=0.019876]   2025-11-07 02:07:21,749 - INFO - Step 166032: {'train_loss_batch': 3.682645320892334, 'train_lr': 0.019876281354219084, 'batch_time': 1.6559768710099367, 'data_time': 0.005067151077050348}
2025-11-07 02:07:21 Train Epoch 066:  40%|███▉      | 1000/2502 [27:38<41:36,  1.66s/it, Loss=3.0155, Top1=66.86%, LR=0.019876]2025-11-07 02:10:07,754 - INFO - Step 166132: {'train_loss_batch': 2.3820879459381104, 'train_lr': 0.019876281354219084, 'batch_time': 1.656384231327297, 'data_time': 0.004666124309574093}
2025-11-07 02:10:07 Train Epoch 066:  44%|████▍     | 1100/2502 [30:23<38:32,  1.65s/it, Loss=3.0170, Top1=N/A, LR=0.019876]   2025-11-07 02:12:52,852 - INFO - Step 166232: {'train_loss_batch': 2.2969706058502197, 'train_lr': 0.019876281354219084, 'batch_time': 1.655893071579132, 'data_time': 0.004330903372907508}
2025-11-07 02:12:52 Train Epoch 066:  48%|████▊     | 1200/2502 [33:08<35:41,  1.64s/it, Loss=3.0141, Top1=N/A, LR=0.019876]2025-11-07 02:15:38,276 - INFO - Step 166332: {'train_loss_batch': 3.7612338066101074, 'train_lr': 0.019876281354219084, 'batch_time': 1.6557548742905743, 'data_time': 0.004064800737303163}
2025-11-07 02:15:38 Train Epoch 066:  52%|█████▏    | 1300/2502 [35:53<32:54,  1.64s/it, Loss=3.0059, Top1=N/A, LR=0.019876]2025-11-07 02:18:23,145 - INFO - Step 166432: {'train_loss_batch': 2.477020502090454, 'train_lr': 0.019876281354219084, 'batch_time': 1.6552117330857554, 'data_time': 0.003834353145684397}
2025-11-07 02:18:23 Train Epoch 066:  56%|█████▌    | 1400/2502 [38:38<30:14,  1.65s/it, Loss=3.0099, Top1=N/A, LR=0.019876]2025-11-07 02:21:08,041 - INFO - Step 166532: {'train_loss_batch': 4.086982727050781, 'train_lr': 0.019876281354219084, 'batch_time': 1.6547653176459476, 'data_time': 0.0036350031736320804}
2025-11-07 02:21:08 Train Epoch 066:  60%|█████▉    | 1500/2502 [41:23<27:28,  1.65s/it, Loss=3.0105, Top1=N/A, LR=0.019876]2025-11-07 02:23:52,959 - INFO - Step 166632: {'train_loss_batch': 3.9568328857421875, 'train_lr': 0.019876281354219084, 'batch_time': 1.6543934156225968, 'data_time': 0.0034647183288025267}
2025-11-07 02:23:52 Train Epoch 066:  64%|██████▍   | 1600/2502 [44:08<24:52,  1.65s/it, Loss=3.0158, Top1=N/A, LR=0.019876]2025-11-07 02:26:38,560 - INFO - Step 166732: {'train_loss_batch': 2.250124216079712, 'train_lr': 0.019876281354219084, 'batch_time': 1.654493677251269, 'data_time': 0.00331297462839845}
2025-11-07 02:26:38 Train Epoch 066:  68%|██████▊   | 1700/2502 [46:53<22:15,  1.67s/it, Loss=3.0149, Top1=N/A, LR=0.019876]2025-11-07 02:29:23,532 - INFO - Step 166832: {'train_loss_batch': 3.2382383346557617, 'train_lr': 0.019876281354219084, 'batch_time': 1.6542132374540908, 'data_time': 0.003183358560794245}
2025-11-07 02:29:23 Train Epoch 066:  72%|███████▏  | 1800/2502 [49:39<19:21,  1.65s/it, Loss=3.0141, Top1=N/A, LR=0.019876]2025-11-07 02:32:09,233 - INFO - Step 166932: {'train_loss_batch': 4.371191501617432, 'train_lr': 0.019876281354219084, 'batch_time': 1.6543682644858353, 'data_time': 0.003067617347543601}
2025-11-07 02:32:09 Train Epoch 066:  76%|███████▌  | 1900/2502 [52:25<16:40,  1.66s/it, Loss=3.0122, Top1=N/A, LR=0.019876]2025-11-07 02:34:54,996 - INFO - Step 167032: {'train_loss_batch': 2.436598777770996, 'train_lr': 0.019876281354219084, 'batch_time': 1.6545395211506242, 'data_time': 0.002962287760357804}
2025-11-07 02:34:55 Train Epoch 066:  80%|███████▉  | 2000/2502 [55:11<13:54,  1.66s/it, Loss=3.0146, Top1=N/A, LR=0.019876]2025-11-07 02:37:40,789 - INFO - Step 167132: {'train_loss_batch': 3.30991268157959, 'train_lr': 0.019876281354219084, 'batch_time': 1.654709291839409, 'data_time': 0.0028677725422567037}
2025-11-07 02:37:40 Train Epoch 066:  84%|████████▍ | 2100/2502 [57:56<11:07,  1.66s/it, Loss=3.0146, Top1=N/A, LR=0.019876]2025-11-07 02:40:25,773 - INFO - Step 167232: {'train_loss_batch': 3.847052574157715, 'train_lr': 0.019876281354219084, 'batch_time': 1.6544771280701758, 'data_time': 0.0027792419722283356}
2025-11-07 02:40:25 Train Epoch 066:  88%|████████▊ | 2200/2502 [1:00:41<08:18,  1.65s/it, Loss=3.0186, Top1=N/A, LR=0.019876]2025-11-07 02:43:10,775 - INFO - Step 167332: {'train_loss_batch': 2.3180477619171143, 'train_lr': 0.019876281354219084, 'batch_time': 1.6542746380533428, 'data_time': 0.002701317272853548}
2025-11-07 02:43:10 Train Epoch 066:  92%|█████████▏| 2300/2502 [1:03:25<05:29,  1.63s/it, Loss=3.0168, Top1=N/A, LR=0.019876]2025-11-07 02:45:55,254 - INFO - Step 167432: {'train_loss_batch': 3.6398308277130127, 'train_lr': 0.019876281354219084, 'batch_time': 1.6538622604147966, 'data_time': 0.0026287687493116636}
2025-11-07 02:45:55 Train Epoch 066:  96%|█████████▌| 2400/2502 [1:06:10<02:48,  1.66s/it, Loss=3.0180, Top1=N/A, LR=0.019876]2025-11-07 02:48:40,297 - INFO - Step 167532: {'train_loss_batch': 4.592866897583008, 'train_lr': 0.019876281354219084, 'batch_time': 1.6537195025558822, 'data_time': 0.0025609245602164054}
2025-11-07 02:48:40 Train Epoch 066: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.67s/it, Loss=3.0142, Top1=N/A, LR=0.019876]2025-11-07 02:51:26,374 - INFO - Step 167632: {'train_loss_batch': 2.3995461463928223, 'train_lr': 0.019876281354219084, 'batch_time': 1.6540012507379556, 'data_time': 0.002558867486178136}
2025-11-07 02:51:26 Train Epoch 066: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=3.0142, Top1=N/A, LR=0.019876]
2025-11-07 02:51:28 Val Epoch 066:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 02:51:33   with torch.cuda.amp.autocast():
2025-11-07 02:51:33 Val Epoch 066: 100%|██████████| 98/98 [01:48<00:00,  1.11s/it, Loss=2.0881, Top1=71.03%, Top5=90.64%]
2025-11-07 02:53:17 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 02:53:17   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 02:53:17 2025-11-07 02:53:17,156 - INFO - Step 66: {'epoch': 66, 'learning_rate': 0.018422608242368733, 'train_loss': 3.01403033342674, 'train_top1': 66.65076335877863, 'train_top5': 86.34713442270993, 'train_precision': 66.47425972563958, 'train_recall': 66.52549968064406, 'train_f1': 66.33980266689716, 'val_loss': 2.0880781508255004, 'val_top1': 71.03199998291015, 'val_top5': 90.63600000976562, 'val_precision': 72.27268424776157, 'val_recall': 71.036, 'val_f1': 70.68604168557691}
2025-11-07 02:53:17 2025-11-07 02:53:17,157 - INFO - Epoch 066 Summary - LR: 0.018423, Train Loss: 3.0140, Val Loss: 2.0881, Val F1: 70.69%, Val Precision: 72.27%, Val Recall: 71.04%
2025-11-07 02:53:20 2025-11-07 02:53:20,400 - INFO - New best model saved with validation accuracy: 71.032%
2025-11-07 02:53:20 2025-11-07 02:53:20,400 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_067.pth
2025-11-07 02:53:20 Train Epoch 067:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 66 that is less than the current step 167632. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 02:53:24 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 02:53:24   with torch.cuda.amp.autocast():
2025-11-07 02:53:25 Train Epoch 067:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.9266, Top1=N/A, LR=0.018423]2025-11-07 02:53:25,630 - INFO - Step 167634: {'train_loss_batch': 3.9265520572662354, 'train_lr': 0.018422608242368733, 'batch_time': 5.2281575202941895, 'data_time': 3.5831515789031982}
2025-11-07 02:53:25 Train Epoch 067:   4%|▍         | 100/2502 [02:50<1:06:09,  1.65s/it, Loss=3.0413, Top1=N/A, LR=0.018423]2025-11-07 02:56:11,375 - INFO - Step 167734: {'train_loss_batch': 2.950000047683716, 'train_lr': 0.018422608242368733, 'batch_time': 1.6928095982806517, 'data_time': 0.03651148021811306}
2025-11-07 02:56:11 Train Epoch 067:   8%|▊         | 200/2502 [05:36<1:03:52,  1.66s/it, Loss=3.0040, Top1=N/A, LR=0.018423]2025-11-07 02:58:57,228 - INFO - Step 167834: {'train_loss_batch': 3.3429651260375977, 'train_lr': 0.018422608242368733, 'batch_time': 1.6757543229345064, 'data_time': 0.01892643425595108}
2025-11-07 02:58:57 Train Epoch 067:  12%|█▏        | 300/2502 [08:22<1:00:51,  1.66s/it, Loss=2.9717, Top1=N/A, LR=0.018423]2025-11-07 03:01:42,990 - INFO - Step 167934: {'train_loss_batch': 2.3229243755340576, 'train_lr': 0.018422608242368733, 'batch_time': 1.6697264485977021, 'data_time': 0.012982481737865563}
2025-11-07 03:01:42 Train Epoch 067:  16%|█▌        | 400/2502 [11:08<58:01,  1.66s/it, Loss=3.0187, Top1=67.29%, LR=0.018423]2025-11-07 03:04:29,082 - INFO - Step 168034: {'train_loss_batch': 2.296982765197754, 'train_lr': 0.018422608242368733, 'batch_time': 1.6675304147668015, 'data_time': 0.010023029070542637}
2025-11-07 03:04:29 Train Epoch 067:  20%|█▉        | 500/2502 [13:54<55:19,  1.66s/it, Loss=2.9946, Top1=67.30%, LR=0.018423]2025-11-07 03:07:14,609 - INFO - Step 168134: {'train_loss_batch': 2.330667734146118, 'train_lr': 0.018422608242368733, 'batch_time': 1.6650829905283426, 'data_time': 0.008217631699796208}
2025-11-07 03:07:14 Train Epoch 067:  24%|██▍       | 600/2502 [16:39<52:12,  1.65s/it, Loss=2.9858, Top1=67.26%, LR=0.018423]2025-11-07 03:10:00,140 - INFO - Step 168234: {'train_loss_batch': 2.4466605186462402, 'train_lr': 0.018422608242368733, 'batch_time': 1.6634574872681782, 'data_time': 0.007025062938696533}
2025-11-07 03:10:00 Train Epoch 067:  28%|██▊       | 700/2502 [19:24<49:20,  1.64s/it, Loss=3.0073, Top1=N/A, LR=0.018423]   2025-11-07 03:12:44,992 - INFO - Step 168334: {'train_loss_batch': 3.956899881362915, 'train_lr': 0.018422608242368733, 'batch_time': 1.661325643134015, 'data_time': 0.006170439482075341}
2025-11-07 03:12:44 Train Epoch 067:  32%|███▏      | 800/2502 [22:10<47:06,  1.66s/it, Loss=3.0091, Top1=N/A, LR=0.018423]2025-11-07 03:15:30,520 - INFO - Step 168434: {'train_loss_batch': 2.4507064819335938, 'train_lr': 0.018422608242368733, 'batch_time': 1.6605699985066007, 'data_time': 0.00552437367957183}
2025-11-07 03:15:30 Train Epoch 067:  36%|███▌      | 900/2502 [24:55<44:06,  1.65s/it, Loss=3.0168, Top1=N/A, LR=0.018423]2025-11-07 03:18:16,260 - INFO - Step 168534: {'train_loss_batch': 3.053156614303589, 'train_lr': 0.018422608242368733, 'batch_time': 1.6602179506642705, 'data_time': 0.005027303420478575}
2025-11-07 03:18:16 Train Epoch 067:  40%|███▉      | 1000/2502 [27:41<41:28,  1.66s/it, Loss=3.0082, Top1=67.11%, LR=0.018423]2025-11-07 03:21:01,656 - INFO - Step 168634: {'train_loss_batch': 2.366580009460449, 'train_lr': 0.018422608242368733, 'batch_time': 1.659592467230874, 'data_time': 0.004622137391722047}
2025-11-07 03:21:01 Train Epoch 067:  44%|████▍     | 1100/2502 [30:26<38:30,  1.65s/it, Loss=3.0159, Top1=N/A, LR=0.018423]   2025-11-07 03:23:47,224 - INFO - Step 168734: {'train_loss_batch': 2.962932586669922, 'train_lr': 0.018422608242368733, 'batch_time': 1.6592369662102084, 'data_time': 0.004295039891547014}
2025-11-07 03:23:47 Train Epoch 067:  48%|████▊     | 1200/2502 [33:11<35:45,  1.65s/it, Loss=3.0123, Top1=N/A, LR=0.018423]2025-11-07 03:26:31,988 - INFO - Step 168834: {'train_loss_batch': 3.9358878135681152, 'train_lr': 0.018422608242368733, 'batch_time': 1.6582713885470095, 'data_time': 0.004024942550532129}
2025-11-07 03:26:32 Train Epoch 067:  52%|█████▏    | 1300/2502 [35:56<33:07,  1.65s/it, Loss=3.0218, Top1=67.05%, LR=0.018423]2025-11-07 03:29:17,387 - INFO - Step 168934: {'train_loss_batch': 2.365990161895752, 'train_lr': 0.018422608242368733, 'batch_time': 1.6579424867622674, 'data_time': 0.003792375532321798}
2025-11-07 03:29:17 Train Epoch 067:  56%|█████▌    | 1400/2502 [38:42<30:25,  1.66s/it, Loss=3.0261, Top1=N/A, LR=0.018423]   2025-11-07 03:32:03,245 - INFO - Step 169034: {'train_loss_batch': 2.379976987838745, 'train_lr': 0.018422608242368733, 'batch_time': 1.6579877969454562, 'data_time': 0.0035909088401603834}
2025-11-07 03:32:03 Train Epoch 067:  60%|█████▉    | 1500/2502 [41:27<27:42,  1.66s/it, Loss=3.0248, Top1=N/A, LR=0.018423]2025-11-07 03:34:48,247 - INFO - Step 169134: {'train_loss_batch': 2.2979319095611572, 'train_lr': 0.018422608242368733, 'batch_time': 1.6574568608695075, 'data_time': 0.003416867354645243}
2025-11-07 03:34:48 Train Epoch 067:  64%|██████▍   | 1600/2502 [44:13<24:54,  1.66s/it, Loss=3.0217, Top1=N/A, LR=0.018423]2025-11-07 03:37:33,990 - INFO - Step 169234: {'train_loss_batch': 3.9294891357421875, 'train_lr': 0.018422608242368733, 'batch_time': 1.6574551852176815, 'data_time': 0.0032645223440042814}
2025-11-07 03:37:33 Train Epoch 067:  68%|██████▊   | 1700/2502 [46:59<22:01,  1.65s/it, Loss=3.0182, Top1=N/A, LR=0.018423]2025-11-07 03:40:19,488 - INFO - Step 169334: {'train_loss_batch': 2.855999231338501, 'train_lr': 0.018422608242368733, 'batch_time': 1.6573090390693714, 'data_time': 0.0031304128165808515}
2025-11-07 03:40:19 Train Epoch 067:  72%|███████▏  | 1800/2502 [49:44<19:28,  1.66s/it, Loss=3.0254, Top1=N/A, LR=0.018423]2025-11-07 03:43:04,883 - INFO - Step 169434: {'train_loss_batch': 2.3491079807281494, 'train_lr': 0.018422608242368733, 'batch_time': 1.657122585920411, 'data_time': 0.003013453835715591}
2025-11-07 03:43:04 Train Epoch 067:  76%|███████▌  | 1900/2502 [52:29<16:32,  1.65s/it, Loss=3.0202, Top1=67.00%, LR=0.018423]2025-11-07 03:45:50,298 - INFO - Step 169534: {'train_loss_batch': 2.291902780532837, 'train_lr': 0.018422608242368733, 'batch_time': 1.6569661405574392, 'data_time': 0.002908273473154677}
2025-11-07 03:45:50 Train Epoch 067:  80%|███████▉  | 2000/2502 [55:15<13:46,  1.65s/it, Loss=3.0241, Top1=N/A, LR=0.018423]   2025-11-07 03:48:35,598 - INFO - Step 169634: {'train_loss_batch': 2.765155792236328, 'train_lr': 0.018422608242368733, 'batch_time': 1.6567679790780878, 'data_time': 0.002812483142698365}
2025-11-07 03:48:35 Train Epoch 067:  84%|████████▍ | 2100/2502 [58:00<11:00,  1.64s/it, Loss=3.0258, Top1=N/A, LR=0.018423]2025-11-07 03:51:20,610 - INFO - Step 169734: {'train_loss_batch': 2.5932140350341797, 'train_lr': 0.018422608242368733, 'batch_time': 1.6564512376499312, 'data_time': 0.002727686479396902}
2025-11-07 03:51:20 Train Epoch 067:  88%|████████▊ | 2200/2502 [1:00:45<08:15,  1.64s/it, Loss=3.0175, Top1=N/A, LR=0.018423]2025-11-07 03:54:05,696 - INFO - Step 169834: {'train_loss_batch': 3.263181209564209, 'train_lr': 0.018422608242368733, 'batch_time': 1.6561973738377878, 'data_time': 0.002648861285828829}
2025-11-07 03:54:05 Train Epoch 067:  92%|█████████▏| 2300/2502 [1:03:30<05:34,  1.65s/it, Loss=3.0185, Top1=N/A, LR=0.018423]2025-11-07 03:56:50,415 - INFO - Step 169934: {'train_loss_batch': 2.4287900924682617, 'train_lr': 0.018422608242368733, 'batch_time': 1.655805500110094, 'data_time': 0.0025740668650349034}
2025-11-07 03:56:50 Train Epoch 067:  96%|█████████▌| 2400/2502 [1:06:15<02:48,  1.66s/it, Loss=3.0149, Top1=67.00%, LR=0.018423]2025-11-07 03:59:36,141 - INFO - Step 170034: {'train_loss_batch': 2.3139641284942627, 'train_lr': 0.018422608242368733, 'batch_time': 1.6558659463959502, 'data_time': 0.002510216274841385}
2025-11-07 03:59:36 Train Epoch 067: 100%|█████████▉| 2500/2502 [1:09:00<00:03,  1.66s/it, Loss=3.0137, Top1=N/A, LR=0.018423]   2025-11-07 04:02:20,948 - INFO - Step 170134: {'train_loss_batch': 4.432648181915283, 'train_lr': 0.018422608242368733, 'batch_time': 1.655554390678116, 'data_time': 0.0024861570645789534}
2025-11-07 04:02:20 Train Epoch 067: 100%|██████████| 2502/2502 [1:09:02<00:00,  1.66s/it, Loss=3.0137, Top1=N/A, LR=0.018423]
2025-11-07 04:02:23 Val Epoch 067:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 04:02:27   with torch.cuda.amp.autocast():
2025-11-07 04:02:28 Val Epoch 067: 100%|██████████| 98/98 [01:49<00:00,  1.12s/it, Loss=2.0994, Top1=70.92%, Top5=90.50%]
2025-11-07 04:04:12 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 04:04:12   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 04:04:12 2025-11-07 04:04:12,640 - INFO - Step 67: {'epoch': 67, 'learning_rate': 0.017012072878738276, 'train_loss': 3.013871241054184, 'train_top1': 66.99670339595376, 'train_top5': 86.56257526493256, 'train_precision': 66.87933372764712, 'train_recall': 66.90410854274414, 'train_f1': 66.72319415983043, 'val_loss': 2.099363149719238, 'val_top1': 70.91600001708984, 'val_top5': 90.49599999023438, 'val_precision': 72.24047065648165, 'val_recall': 70.92, 'val_f1': 70.55286800303917}
2025-11-07 04:04:12 2025-11-07 04:04:12,642 - INFO - Epoch 067 Summary - LR: 0.017012, Train Loss: 3.0139, Val Loss: 2.0994, Val F1: 70.55%, Val Precision: 72.24%, Val Recall: 70.92%
2025-11-07 04:04:13 Train Epoch 068:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 67 that is less than the current step 170134. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 04:04:16 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 04:04:16   with torch.cuda.amp.autocast():
2025-11-07 04:04:18 Train Epoch 068:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.8249, Top1=N/A, LR=0.017012]2025-11-07 04:04:18,447 - INFO - Step 170136: {'train_loss_batch': 3.824941396713257, 'train_lr': 0.017012072878738276, 'batch_time': 5.27374792098999, 'data_time': 3.6418352127075195}
2025-11-07 04:04:18 Train Epoch 068:   4%|▍         | 100/2502 [02:50<1:06:13,  1.65s/it, Loss=3.1378, Top1=67.99%, LR=0.017012]2025-11-07 04:07:04,114 - INFO - Step 170236: {'train_loss_batch': 2.241560935974121, 'train_lr': 0.017012072878738276, 'batch_time': 1.692487622251605, 'data_time': 0.03708429383759451}
2025-11-07 04:07:04 Train Epoch 068:   8%|▊         | 200/2502 [05:36<1:03:34,  1.66s/it, Loss=3.0304, Top1=N/A, LR=0.017012]   2025-11-07 04:09:50,031 - INFO - Step 170336: {'train_loss_batch': 3.192214012145996, 'train_lr': 0.017012072878738276, 'batch_time': 1.6759102818977774, 'data_time': 0.019136368338741473}
2025-11-07 04:09:50 Train Epoch 068:  12%|█▏        | 300/2502 [08:22<1:00:53,  1.66s/it, Loss=3.0031, Top1=67.80%, LR=0.017012]2025-11-07 04:12:35,873 - INFO - Step 170436: {'train_loss_batch': 2.2430458068847656, 'train_lr': 0.017012072878738276, 'batch_time': 1.6700960457126959, 'data_time': 0.013100704877479528}
2025-11-07 04:12:35 Train Epoch 068:  16%|█▌        | 400/2502 [11:08<57:50,  1.65s/it, Loss=2.9825, Top1=N/A, LR=0.017012]   2025-11-07 04:15:21,415 - INFO - Step 170536: {'train_loss_batch': 3.1045703887939453, 'train_lr': 0.017012072878738276, 'batch_time': 1.666436019383761, 'data_time': 0.010091424285622309}
2025-11-07 04:15:21 Train Epoch 068:  20%|█▉        | 500/2502 [13:54<55:20,  1.66s/it, Loss=2.9773, Top1=N/A, LR=0.017012]2025-11-07 04:18:07,329 - INFO - Step 170636: {'train_loss_batch': 2.5451104640960693, 'train_lr': 0.017012072878738276, 'batch_time': 1.6649799456377468, 'data_time': 0.008285366370530423}
2025-11-07 04:18:07 Train Epoch 068:  24%|██▍       | 600/2502 [16:40<52:40,  1.66s/it, Loss=2.9772, Top1=N/A, LR=0.017012]2025-11-07 04:20:53,289 - INFO - Step 170736: {'train_loss_batch': 3.835880756378174, 'train_lr': 0.017012072878738276, 'batch_time': 1.6640840501039476, 'data_time': 0.007066523969272607}
2025-11-07 04:20:53 Train Epoch 068:  28%|██▊       | 700/2502 [19:25<49:18,  1.64s/it, Loss=2.9695, Top1=N/A, LR=0.017012]2025-11-07 04:23:38,858 - INFO - Step 170836: {'train_loss_batch': 4.202072620391846, 'train_lr': 0.017012072878738276, 'batch_time': 1.6628862229971675, 'data_time': 0.006202691631888527}
2025-11-07 04:23:38 Train Epoch 068:  32%|███▏      | 800/2502 [22:10<47:02,  1.66s/it, Loss=2.9628, Top1=67.78%, LR=0.017012]2025-11-07 04:26:23,429 - INFO - Step 170936: {'train_loss_batch': 2.2437705993652344, 'train_lr': 0.017012072878738276, 'batch_time': 1.6607417191161347, 'data_time': 0.005548778395825409}
2025-11-07 04:26:23 Train Epoch 068:  36%|███▌      | 900/2502 [24:56<44:20,  1.66s/it, Loss=2.9547, Top1=N/A, LR=0.017012]   2025-11-07 04:29:09,457 - INFO - Step 171036: {'train_loss_batch': 3.8534088134765625, 'train_lr': 0.017012072878738276, 'batch_time': 1.6606911165468172, 'data_time': 0.005042007046190403}
2025-11-07 04:29:09 Train Epoch 068:  40%|███▉      | 1000/2502 [27:42<41:36,  1.66s/it, Loss=2.9513, Top1=N/A, LR=0.017012]2025-11-07 04:31:55,381 - INFO - Step 171136: {'train_loss_batch': 2.311218738555908, 'train_lr': 0.017012072878738276, 'batch_time': 1.660545930519447, 'data_time': 0.004640535160259053}
2025-11-07 04:31:55 Train Epoch 068:  44%|████▍     | 1100/2502 [30:27<38:42,  1.66s/it, Loss=2.9587, Top1=N/A, LR=0.017012]2025-11-07 04:34:40,883 - INFO - Step 171236: {'train_loss_batch': 2.7206032276153564, 'train_lr': 0.017012072878738276, 'batch_time': 1.6600432718590539, 'data_time': 0.00431060920944006}
2025-11-07 04:34:40 Train Epoch 068:  48%|████▊     | 1200/2502 [33:12<35:44,  1.65s/it, Loss=2.9598, Top1=N/A, LR=0.017012]2025-11-07 04:37:25,869 - INFO - Step 171336: {'train_loss_batch': 3.401762008666992, 'train_lr': 0.017012072878738276, 'batch_time': 1.6591953202945604, 'data_time': 0.004028764592916344}
2025-11-07 04:37:25 Train Epoch 068:  52%|█████▏    | 1300/2502 [35:58<33:15,  1.66s/it, Loss=2.9636, Top1=N/A, LR=0.017012]2025-11-07 04:40:11,786 - INFO - Step 171436: {'train_loss_batch': 2.3580727577209473, 'train_lr': 0.017012072878738276, 'batch_time': 1.6591932264133016, 'data_time': 0.0037956204806539667}
2025-11-07 04:40:11 Train Epoch 068:  56%|█████▌    | 1400/2502 [38:44<30:26,  1.66s/it, Loss=2.9748, Top1=N/A, LR=0.017012]2025-11-07 04:42:57,693 - INFO - Step 171536: {'train_loss_batch': 3.8353443145751953, 'train_lr': 0.017012072878738276, 'batch_time': 1.6591841612944511, 'data_time': 0.00359862060056764}
2025-11-07 04:42:57 Train Epoch 068:  60%|█████▉    | 1500/2502 [41:30<27:43,  1.66s/it, Loss=2.9760, Top1=N/A, LR=0.017012]2025-11-07 04:45:43,345 - INFO - Step 171636: {'train_loss_batch': 2.2814323902130127, 'train_lr': 0.017012072878738276, 'batch_time': 1.6590067015577363, 'data_time': 0.0034243734894713745}
2025-11-07 04:45:43 Train Epoch 068:  64%|██████▍   | 1600/2502 [44:14<24:37,  1.64s/it, Loss=2.9740, Top1=N/A, LR=0.017012]2025-11-07 04:48:28,066 - INFO - Step 171736: {'train_loss_batch': 3.138031005859375, 'train_lr': 0.017012072878738276, 'batch_time': 1.658269933579044, 'data_time': 0.003269468077564895}
2025-11-07 04:48:28 Train Epoch 068:  68%|██████▊   | 1700/2502 [46:59<21:50,  1.63s/it, Loss=2.9677, Top1=67.58%, LR=0.017012]2025-11-07 04:51:12,864 - INFO - Step 171836: {'train_loss_batch': 2.375479221343994, 'train_lr': 0.017012072878738276, 'batch_time': 1.6576648712999187, 'data_time': 0.0031371237179028994}
2025-11-07 04:51:12 Train Epoch 068:  72%|███████▏  | 1800/2502 [49:43<19:26,  1.66s/it, Loss=2.9649, Top1=N/A, LR=0.017012]   2025-11-07 04:53:56,997 - INFO - Step 171936: {'train_loss_batch': 2.5952279567718506, 'train_lr': 0.017012072878738276, 'batch_time': 1.6567577157399178, 'data_time': 0.0030192911856575582}
2025-11-07 04:53:57 Train Epoch 068:  76%|███████▌  | 1900/2502 [52:29<16:35,  1.65s/it, Loss=2.9659, Top1=67.59%, LR=0.017012]2025-11-07 04:56:42,711 - INFO - Step 172036: {'train_loss_batch': 2.352100133895874, 'train_lr': 0.017012072878738276, 'batch_time': 1.6567772696483518, 'data_time': 0.002912228637968221}
2025-11-07 04:56:42 Train Epoch 068:  80%|███████▉  | 2000/2502 [55:15<13:53,  1.66s/it, Loss=2.9629, Top1=N/A, LR=0.017012]   2025-11-07 04:59:28,437 - INFO - Step 172136: {'train_loss_batch': 3.491598129272461, 'train_lr': 0.017012072878738276, 'batch_time': 1.6568013300364284, 'data_time': 0.0028183553172373165}
2025-11-07 04:59:28 Train Epoch 068:  84%|████████▍ | 2100/2502 [58:00<10:55,  1.63s/it, Loss=2.9640, Top1=N/A, LR=0.017012]2025-11-07 05:02:13,522 - INFO - Step 172236: {'train_loss_batch': 2.813079357147217, 'train_lr': 0.017012072878738276, 'batch_time': 1.656518035590904, 'data_time': 0.002730643051343552}
2025-11-07 05:02:13 Train Epoch 068:  88%|████████▊ | 2200/2502 [1:00:44<08:21,  1.66s/it, Loss=2.9662, Top1=N/A, LR=0.017012]2025-11-07 05:04:57,491 - INFO - Step 172336: {'train_loss_batch': 4.486185073852539, 'train_lr': 0.017012072878738276, 'batch_time': 1.6557535190356965, 'data_time': 0.002652122670008561}
2025-11-07 05:04:57 Train Epoch 068:  92%|█████████▏| 2300/2502 [1:03:29<05:35,  1.66s/it, Loss=2.9790, Top1=67.53%, LR=0.017012]2025-11-07 05:07:42,771 - INFO - Step 172436: {'train_loss_batch': 2.37636137008667, 'train_lr': 0.017012072878738276, 'batch_time': 1.6556249933106026, 'data_time': 0.0025824954436168314}
2025-11-07 05:07:42 Train Epoch 068:  96%|█████████▌| 2400/2502 [1:06:15<02:48,  1.66s/it, Loss=2.9874, Top1=N/A, LR=0.017012]   2025-11-07 05:10:28,620 - INFO - Step 172536: {'train_loss_batch': 3.7493884563446045, 'train_lr': 0.017012072878738276, 'batch_time': 1.6557440553194083, 'data_time': 0.0025170190986719097}
2025-11-07 05:10:28 Train Epoch 068: 100%|█████████▉| 2500/2502 [1:09:00<00:03,  1.66s/it, Loss=2.9885, Top1=N/A, LR=0.017012]2025-11-07 05:13:13,766 - INFO - Step 172636: {'train_loss_batch': 2.3738865852355957, 'train_lr': 0.017012072878738276, 'batch_time': 1.65557297588777, 'data_time': 0.002477680383230009}
2025-11-07 05:13:13 Train Epoch 068: 100%|██████████| 2502/2502 [1:09:02<00:00,  1.66s/it, Loss=2.9885, Top1=N/A, LR=0.017012]
2025-11-07 05:13:16 Val Epoch 068:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 05:13:20   with torch.cuda.amp.autocast():
2025-11-07 05:13:20 Val Epoch 068: 100%|██████████| 98/98 [01:48<00:00,  1.11s/it, Loss=2.0946, Top1=70.92%, Top5=90.61%]
2025-11-07 05:15:04 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 05:15:04   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 05:15:04 2025-11-07 05:15:04,497 - INFO - Step 68: {'epoch': 68, 'learning_rate': 0.015646601884966714, 'train_loss': 2.98885071163269, 'train_top1': 67.53352933403805, 'train_top5': 86.8620342230444, 'train_precision': 67.41357926682574, 'train_recall': 67.41821270489451, 'train_f1': 67.24694753601734, 'val_loss': 2.0946260205078127, 'val_top1': 70.91799997070312, 'val_top5': 90.60800000732422, 'val_precision': 72.13023163884445, 'val_recall': 70.91600000000001, 'val_f1': 70.49802702761073}
2025-11-07 05:15:04 2025-11-07 05:15:04,499 - INFO - Epoch 068 Summary - LR: 0.015647, Train Loss: 2.9889, Val Loss: 2.0946, Val F1: 70.50%, Val Precision: 72.13%, Val Recall: 70.92%
2025-11-07 05:15:04 Train Epoch 069:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 05:15:08   with torch.cuda.amp.autocast():
2025-11-07 05:15:10 Train Epoch 069:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.2107, Top1=N/A, LR=0.015647]2025-11-07 05:15:10,136 - INFO - Step 172638: {'train_loss_batch': 3.210710048675537, 'train_lr': 0.015646601884966714, 'batch_time': 5.189982891082764, 'data_time': 3.544908046722412}
2025-11-07 05:15:10 Train Epoch 069:   0%|          | 3/2502 [00:08<1:41:42,  2.44s/it, Loss=3.2107, Top1=N/A, LR=0.015647]wandb: WARNING Tried to log to step 68 that is less than the current step 172636. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 05:15:15 Train Epoch 069:   4%|▍         | 100/2502 [02:50<1:06:31,  1.66s/it, Loss=3.0678, Top1=N/A, LR=0.015647]2025-11-07 05:17:55,763 - INFO - Step 172738: {'train_loss_batch': 3.0628325939178467, 'train_lr': 0.015646601884966714, 'batch_time': 1.6912533670368761, 'data_time': 0.036148196399802025}
2025-11-07 05:17:55 Train Epoch 069:   8%|▊         | 200/2502 [05:36<1:03:25,  1.65s/it, Loss=3.0000, Top1=N/A, LR=0.015647]2025-11-07 05:20:41,182 - INFO - Step 172838: {'train_loss_batch': 2.9592056274414062, 'train_lr': 0.015646601884966714, 'batch_time': 1.6728171199115354, 'data_time': 0.018681727831636494}
2025-11-07 05:20:41 Train Epoch 069:  12%|█▏        | 300/2502 [08:21<1:00:41,  1.65s/it, Loss=2.9707, Top1=68.42%, LR=0.015647]2025-11-07 05:23:26,635 - INFO - Step 172938: {'train_loss_batch': 2.3333029747009277, 'train_lr': 0.015646601884966714, 'batch_time': 1.6667395691538966, 'data_time': 0.012802756109903024}
2025-11-07 05:23:26 Train Epoch 069:  16%|█▌        | 400/2502 [11:07<58:01,  1.66s/it, Loss=2.9205, Top1=68.34%, LR=0.015647]2025-11-07 05:26:12,260 - INFO - Step 173038: {'train_loss_batch': 2.2028322219848633, 'train_lr': 0.015646601884966714, 'batch_time': 1.6641228793565175, 'data_time': 0.009850911071472929}
2025-11-07 05:26:12 Train Epoch 069:  20%|█▉        | 500/2502 [13:52<55:04,  1.65s/it, Loss=2.9165, Top1=N/A, LR=0.015647]   2025-11-07 05:28:57,922 - INFO - Step 173138: {'train_loss_batch': 3.293762683868408, 'train_lr': 0.015646601884966714, 'batch_time': 1.6626259895141966, 'data_time': 0.008087862037613006}
2025-11-07 05:28:57 Train Epoch 069:  24%|██▍       | 600/2502 [16:38<52:15,  1.65s/it, Loss=2.9243, Top1=N/A, LR=0.015647]2025-11-07 05:31:43,320 - INFO - Step 173238: {'train_loss_batch': 2.37703537940979, 'train_lr': 0.015646601884966714, 'batch_time': 1.6611858132278265, 'data_time': 0.006911550306044085}
2025-11-07 05:31:43 Train Epoch 069:  28%|██▊       | 700/2502 [19:23<49:45,  1.66s/it, Loss=2.9179, Top1=N/A, LR=0.015647]2025-11-07 05:34:28,245 - INFO - Step 173338: {'train_loss_batch': 3.834861993789673, 'train_lr': 0.015646601884966714, 'batch_time': 1.6594831848280576, 'data_time': 0.006062807608262959}
2025-11-07 05:34:28 Train Epoch 069:  32%|███▏      | 800/2502 [22:08<46:32,  1.64s/it, Loss=2.9259, Top1=68.37%, LR=0.015647]2025-11-07 05:37:13,735 - INFO - Step 173438: {'train_loss_batch': 2.1930148601531982, 'train_lr': 0.015646601884966714, 'batch_time': 1.6589113710524883, 'data_time': 0.005428878853234757}
2025-11-07 05:37:13 Train Epoch 069:  36%|███▌      | 900/2502 [24:53<43:53,  1.64s/it, Loss=2.9426, Top1=N/A, LR=0.015647]   2025-11-07 05:39:58,504 - INFO - Step 173538: {'train_loss_batch': 2.325023889541626, 'train_lr': 0.015646601884966714, 'batch_time': 1.657665343713284, 'data_time': 0.004933034937072144}
2025-11-07 05:39:58 Train Epoch 069:  40%|███▉      | 1000/2502 [27:38<41:09,  1.64s/it, Loss=2.9436, Top1=N/A, LR=0.015647]2025-11-07 05:42:43,892 - INFO - Step 173638: {'train_loss_batch': 2.467374563217163, 'train_lr': 0.015646601884966714, 'batch_time': 1.6572866904271113, 'data_time': 0.004544063524290041}
2025-11-07 05:42:43 Train Epoch 069:  44%|████▍     | 1100/2502 [30:24<38:45,  1.66s/it, Loss=2.9559, Top1=N/A, LR=0.015647]2025-11-07 05:45:29,126 - INFO - Step 173738: {'train_loss_batch': 3.8614373207092285, 'train_lr': 0.015646601884966714, 'batch_time': 1.6568373388642077, 'data_time': 0.004216624865414986}
2025-11-07 05:45:29 Train Epoch 069:  48%|████▊     | 1200/2502 [33:10<35:47,  1.65s/it, Loss=2.9520, Top1=N/A, LR=0.015647]2025-11-07 05:48:15,098 - INFO - Step 173838: {'train_loss_batch': 2.2404720783233643, 'train_lr': 0.015646601884966714, 'batch_time': 1.6570773287478533, 'data_time': 0.003949582824897607}
2025-11-07 05:48:15 Train Epoch 069:  52%|█████▏    | 1300/2502 [35:55<33:02,  1.65s/it, Loss=2.9568, Top1=N/A, LR=0.015647]2025-11-07 05:51:00,460 - INFO - Step 173938: {'train_loss_batch': 3.975311756134033, 'train_lr': 0.015646601884966714, 'batch_time': 1.6568115677492696, 'data_time': 0.003720147164393901}
2025-11-07 05:51:00 Train Epoch 069:  56%|█████▌    | 1400/2502 [38:41<30:26,  1.66s/it, Loss=2.9586, Top1=68.13%, LR=0.015647]2025-11-07 05:53:46,292 - INFO - Step 174038: {'train_loss_batch': 2.4324028491973877, 'train_lr': 0.015646601884966714, 'batch_time': 1.656918585598256, 'data_time': 0.00352538304189373}
2025-11-07 05:53:46 Train Epoch 069:  60%|█████▉    | 1500/2502 [41:27<27:41,  1.66s/it, Loss=2.9571, Top1=N/A, LR=0.015647]   2025-11-07 05:56:32,208 - INFO - Step 174138: {'train_loss_batch': 3.5921335220336914, 'train_lr': 0.015646601884966714, 'batch_time': 1.6570681655192518, 'data_time': 0.0033593721027615703}
2025-11-07 05:56:32 Train Epoch 069:  64%|██████▍   | 1600/2502 [44:12<24:44,  1.65s/it, Loss=2.9539, Top1=N/A, LR=0.015647]2025-11-07 05:59:17,466 - INFO - Step 174238: {'train_loss_batch': 2.5975399017333984, 'train_lr': 0.015646601884966714, 'batch_time': 1.656787779240367, 'data_time': 0.003214297333335519}
2025-11-07 05:59:17 Train Epoch 069:  68%|██████▊   | 1700/2502 [46:57<22:11,  1.66s/it, Loss=2.9580, Top1=N/A, LR=0.015647]2025-11-07 06:02:02,859 - INFO - Step 174338: {'train_loss_batch': 2.3266007900238037, 'train_lr': 0.015646601884966714, 'batch_time': 1.6566192114234042, 'data_time': 0.003083918110053025}
2025-11-07 06:02:02 Train Epoch 069:  72%|███████▏  | 1800/2502 [49:43<19:20,  1.65s/it, Loss=2.9661, Top1=N/A, LR=0.015647]2025-11-07 06:04:48,410 - INFO - Step 174438: {'train_loss_batch': 3.9256386756896973, 'train_lr': 0.015646601884966714, 'batch_time': 1.6565578820505518, 'data_time': 0.002966408726905598}
2025-11-07 06:04:48 Train Epoch 069:  76%|███████▌  | 1900/2502 [52:27<16:22,  1.63s/it, Loss=2.9632, Top1=N/A, LR=0.015647]2025-11-07 06:07:32,510 - INFO - Step 174538: {'train_loss_batch': 2.235065460205078, 'train_lr': 0.015646601884966714, 'batch_time': 1.655739192521177, 'data_time': 0.0028598825283140837}
2025-11-07 06:07:32 Train Epoch 069:  80%|███████▉  | 2000/2502 [55:11<13:54,  1.66s/it, Loss=2.9644, Top1=N/A, LR=0.015647]2025-11-07 06:10:16,929 - INFO - Step 174638: {'train_loss_batch': 3.42549729347229, 'train_lr': 0.015646601884966714, 'batch_time': 1.6551619056223155, 'data_time': 0.0027659000127926757}
2025-11-07 06:10:16 Train Epoch 069:  84%|████████▍ | 2100/2502 [57:57<11:02,  1.65s/it, Loss=2.9673, Top1=N/A, LR=0.015647]2025-11-07 06:13:02,142 - INFO - Step 174738: {'train_loss_batch': 2.3968594074249268, 'train_lr': 0.015646601884966714, 'batch_time': 1.6550175080351577, 'data_time': 0.0026819112469955945}
2025-11-07 06:13:02 Train Epoch 069:  88%|████████▊ | 2200/2502 [1:00:42<08:20,  1.66s/it, Loss=2.9724, Top1=N/A, LR=0.015647]2025-11-07 06:15:47,703 - INFO - Step 174838: {'train_loss_batch': 2.276792526245117, 'train_lr': 0.015646601884966714, 'batch_time': 1.6550444615315112, 'data_time': 0.00260440494081531}
2025-11-07 06:15:47 Train Epoch 069:  92%|█████████▏| 2300/2502 [1:03:28<05:33,  1.65s/it, Loss=2.9743, Top1=N/A, LR=0.015647]2025-11-07 06:18:33,582 - INFO - Step 174938: {'train_loss_batch': 3.0606369972229004, 'train_lr': 0.015646601884966714, 'batch_time': 1.655206795103495, 'data_time': 0.0025351141805702686}
2025-11-07 06:18:33 Train Epoch 069:  96%|█████████▌| 2400/2502 [1:06:13<02:48,  1.65s/it, Loss=2.9790, Top1=N/A, LR=0.015647]2025-11-07 06:21:18,726 - INFO - Step 175038: {'train_loss_batch': 4.654147148132324, 'train_lr': 0.015646601884966714, 'batch_time': 1.6550500143671572, 'data_time': 0.002471037677604424}
2025-11-07 06:21:18 Train Epoch 069: 100%|█████████▉| 2500/2502 [1:08:59<00:03,  1.65s/it, Loss=2.9827, Top1=N/A, LR=0.015647]2025-11-07 06:24:04,663 - INFO - Step 175138: {'train_loss_batch': 3.3189492225646973, 'train_lr': 0.015646601884966714, 'batch_time': 1.655222773408947, 'data_time': 0.002437363239061065}
2025-11-07 06:24:04 Train Epoch 069: 100%|██████████| 2502/2502 [1:09:01<00:00,  1.66s/it, Loss=2.9827, Top1=N/A, LR=0.015647]
2025-11-07 06:24:06 Val Epoch 069:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 06:24:11   with torch.cuda.amp.autocast():
2025-11-07 06:24:11 Val Epoch 069: 100%|██████████| 98/98 [01:49<00:00,  1.12s/it, Loss=2.0509, Top1=72.18%, Top5=91.10%]
2025-11-07 06:25:56 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 06:25:56   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 06:25:56 2025-11-07 06:25:56,537 - INFO - Step 69: {'epoch': 69, 'learning_rate': 0.014328060330184448, 'train_loss': 2.982395789510817, 'train_top1': 67.976171875, 'train_top5': 87.133203125, 'train_precision': 67.84981722469558, 'train_recall': 67.84337577199466, 'train_f1': 67.68457703864073, 'val_loss': 2.050931356391907, 'val_top1': 72.17600001708985, 'val_top5': 91.10399998535156, 'val_precision': 73.07567615484626, 'val_recall': 72.172, 'val_f1': 71.75355132078217}
2025-11-07 06:25:56 2025-11-07 06:25:56,539 - INFO - Epoch 069 Summary - LR: 0.014328, Train Loss: 2.9824, Val Loss: 2.0509, Val F1: 71.75%, Val Precision: 73.08%, Val Recall: 72.17%
2025-11-07 06:25:59 2025-11-07 06:25:59,781 - INFO - New best model saved with validation accuracy: 72.176%
2025-11-07 06:25:59 2025-11-07 06:25:59,781 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_070.pth
2025-11-07 06:25:59 Train Epoch 070:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 06:26:03   with torch.cuda.amp.autocast():
2025-11-07 06:26:04 wandb: WARNING Tried to log to step 69 that is less than the current step 175138. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 06:26:05 Train Epoch 070:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=4.2689, Top1=N/A, LR=0.014328]2025-11-07 06:26:05,293 - INFO - Step 175140: {'train_loss_batch': 4.268896579742432, 'train_lr': 0.014328060330184448, 'batch_time': 5.509941101074219, 'data_time': 3.87686824798584}
2025-11-07 06:26:05 Train Epoch 070:   4%|▍         | 100/2502 [02:50<1:06:34,  1.66s/it, Loss=2.9932, Top1=N/A, LR=0.014328]2025-11-07 06:28:50,598 - INFO - Step 175240: {'train_loss_batch': 3.569178581237793, 'train_lr': 0.014328060330184448, 'batch_time': 1.691235610754183, 'data_time': 0.03946330995842962}
2025-11-07 06:28:50 Train Epoch 070:   8%|▊         | 200/2502 [05:36<1:03:35,  1.66s/it, Loss=2.9336, Top1=N/A, LR=0.014328]2025-11-07 06:31:36,177 - INFO - Step 175340: {'train_loss_batch': 2.387667655944824, 'train_lr': 0.014328060330184448, 'batch_time': 1.6736033176308247, 'data_time': 0.020314083763616}
2025-11-07 06:31:36 Train Epoch 070:  12%|█▏        | 300/2502 [08:22<1:00:50,  1.66s/it, Loss=2.9483, Top1=N/A, LR=0.014328]2025-11-07 06:34:22,177 - INFO - Step 175440: {'train_loss_batch': 4.729297161102295, 'train_lr': 0.014328060330184448, 'batch_time': 1.6690823817965992, 'data_time': 0.013937820231795708}
2025-11-07 06:34:22 Train Epoch 070:  16%|█▌        | 400/2502 [11:06<57:25,  1.64s/it, Loss=2.9228, Top1=N/A, LR=0.014328]2025-11-07 06:37:06,357 - INFO - Step 175540: {'train_loss_batch': 2.3482887744903564, 'train_lr': 0.014328060330184448, 'batch_time': 1.6622781587063227, 'data_time': 0.01069747717897791}
2025-11-07 06:37:06 Train Epoch 070:  20%|█▉        | 500/2502 [13:51<55:00,  1.65s/it, Loss=2.9304, Top1=N/A, LR=0.014328]2025-11-07 06:39:51,203 - INFO - Step 175640: {'train_loss_batch': 2.2898547649383545, 'train_lr': 0.014328060330184448, 'batch_time': 1.659520800242167, 'data_time': 0.008755607757263792}
2025-11-07 06:39:51 Train Epoch 070:  24%|██▍       | 600/2502 [16:37<52:20,  1.65s/it, Loss=2.9237, Top1=N/A, LR=0.014328]2025-11-07 06:42:36,903 - INFO - Step 175740: {'train_loss_batch': 3.8987197875976562, 'train_lr': 0.014328060330184448, 'batch_time': 1.6591002734845965, 'data_time': 0.007469537055829599}
2025-11-07 06:42:36 Train Epoch 070:  28%|██▊       | 700/2502 [19:21<49:19,  1.64s/it, Loss=2.9355, Top1=N/A, LR=0.014328]2025-11-07 06:45:21,494 - INFO - Step 175840: {'train_loss_batch': 3.9040415287017822, 'train_lr': 0.014328060330184448, 'batch_time': 1.657218763389533, 'data_time': 0.006551764320885064}
2025-11-07 06:45:21 Train Epoch 070:  32%|███▏      | 800/2502 [22:06<46:30,  1.64s/it, Loss=2.9346, Top1=N/A, LR=0.014328]2025-11-07 06:48:05,935 - INFO - Step 175940: {'train_loss_batch': 2.270350217819214, 'train_lr': 0.014328060330184448, 'batch_time': 1.6556189569194666, 'data_time': 0.005859581867556149}
2025-11-07 06:48:05 Train Epoch 070:  36%|███▌      | 900/2502 [24:51<44:15,  1.66s/it, Loss=2.9387, Top1=N/A, LR=0.014328]2025-11-07 06:50:51,307 - INFO - Step 176040: {'train_loss_batch': 3.095982074737549, 'train_lr': 0.014328060330184448, 'batch_time': 1.6554077883539402, 'data_time': 0.0053170672002828346}
2025-11-07 06:50:51 Train Epoch 070:  40%|███▉      | 1000/2502 [27:35<41:34,  1.66s/it, Loss=2.9404, Top1=68.63%, LR=0.014328]2025-11-07 06:53:35,593 - INFO - Step 176140: {'train_loss_batch': 2.2565014362335205, 'train_lr': 0.014328060330184448, 'batch_time': 1.6541539053102354, 'data_time': 0.004886019599068534}
2025-11-07 06:53:35 Train Epoch 070:  44%|████▍     | 1100/2502 [30:21<38:56,  1.67s/it, Loss=2.9312, Top1=N/A, LR=0.014328]   2025-11-07 06:56:20,816 - INFO - Step 176240: {'train_loss_batch': 2.2263054847717285, 'train_lr': 0.014328060330184448, 'batch_time': 1.6539798298280528, 'data_time': 0.004536241536569206}
2025-11-07 06:56:20 Train Epoch 070:  48%|████▊     | 1200/2502 [33:05<35:21,  1.63s/it, Loss=2.9295, Top1=68.51%, LR=0.014328]2025-11-07 06:59:04,883 - INFO - Step 176340: {'train_loss_batch': 2.2581796646118164, 'train_lr': 0.014328060330184448, 'batch_time': 1.6528714820804644, 'data_time': 0.00424035681375953}
2025-11-07 06:59:04 Train Epoch 070:  52%|█████▏    | 1300/2502 [35:49<33:03,  1.65s/it, Loss=2.9167, Top1=N/A, LR=0.014328]   2025-11-07 07:01:49,769 - INFO - Step 176440: {'train_loss_batch': 2.1541075706481934, 'train_lr': 0.014328060330184448, 'batch_time': 1.6525628509931982, 'data_time': 0.003989019547857567}
2025-11-07 07:01:49 Train Epoch 070:  56%|█████▌    | 1400/2502 [38:35<30:33,  1.66s/it, Loss=2.9301, Top1=N/A, LR=0.014328]2025-11-07 07:04:35,594 - INFO - Step 176540: {'train_loss_batch': 2.3446247577667236, 'train_lr': 0.014328060330184448, 'batch_time': 1.652968527333044, 'data_time': 0.0037780861442723843}
2025-11-07 07:04:35 Train Epoch 070:  60%|█████▉    | 1500/2502 [41:21<27:41,  1.66s/it, Loss=2.9294, Top1=68.43%, LR=0.014328]2025-11-07 07:07:21,572 - INFO - Step 176640: {'train_loss_batch': 2.2161056995391846, 'train_lr': 0.014328060330184448, 'batch_time': 1.6534222018949036, 'data_time': 0.0035944852250802523}
2025-11-07 07:07:21 Train Epoch 070:  64%|██████▍   | 1600/2502 [44:07<24:58,  1.66s/it, Loss=2.9406, Top1=N/A, LR=0.014328]   2025-11-07 07:10:07,337 - INFO - Step 176740: {'train_loss_batch': 2.3300578594207764, 'train_lr': 0.014328060330184448, 'batch_time': 1.6536859188282362, 'data_time': 0.003436645219506211}
2025-11-07 07:10:07 Train Epoch 070:  68%|██████▊   | 1700/2502 [46:53<22:12,  1.66s/it, Loss=2.9381, Top1=68.41%, LR=0.014328]2025-11-07 07:12:52,833 - INFO - Step 176840: {'train_loss_batch': 2.302072286605835, 'train_lr': 0.014328060330184448, 'batch_time': 1.65376114144457, 'data_time': 0.003294462599801989}
2025-11-07 07:12:52 Train Epoch 070:  72%|███████▏  | 1800/2502 [49:38<19:22,  1.66s/it, Loss=2.9374, Top1=68.40%, LR=0.014328]2025-11-07 07:15:38,379 - INFO - Step 176940: {'train_loss_batch': 2.313892364501953, 'train_lr': 0.014328060330184448, 'batch_time': 1.6538550048851424, 'data_time': 0.003167368053794238}
2025-11-07 07:15:38 Train Epoch 070:  76%|███████▌  | 1900/2502 [52:24<16:39,  1.66s/it, Loss=2.9397, Top1=N/A, LR=0.014328]   2025-11-07 07:18:24,453 - INFO - Step 177040: {'train_loss_batch': 4.706208229064941, 'train_lr': 0.014328060330184448, 'batch_time': 1.654217046790848, 'data_time': 0.003057473462108309}
2025-11-07 07:18:24 Train Epoch 070:  80%|███████▉  | 2000/2502 [55:10<13:55,  1.66s/it, Loss=2.9375, Top1=68.41%, LR=0.014328]2025-11-07 07:21:10,094 - INFO - Step 177140: {'train_loss_batch': 2.2775049209594727, 'train_lr': 0.014328060330184448, 'batch_time': 1.654326612504943, 'data_time': 0.00295519912201187}
2025-11-07 07:21:10 Train Epoch 070:  84%|████████▍ | 2100/2502 [57:55<11:04,  1.65s/it, Loss=2.9326, Top1=68.40%, LR=0.014328]2025-11-07 07:23:55,222 - INFO - Step 177240: {'train_loss_batch': 2.1381947994232178, 'train_lr': 0.014328060330184448, 'batch_time': 1.654181740273753, 'data_time': 0.0028620663851230046}
2025-11-07 07:23:55 Train Epoch 070:  88%|████████▊ | 2200/2502 [1:00:40<08:17,  1.65s/it, Loss=2.9271, Top1=68.35%, LR=0.014328]2025-11-07 07:26:40,066 - INFO - Step 177340: {'train_loss_batch': 2.2492971420288086, 'train_lr': 0.014328060330184448, 'batch_time': 1.6539203585521138, 'data_time': 0.002777532034167264}
2025-11-07 07:26:40 Train Epoch 070:  92%|█████████▏| 2300/2502 [1:03:25<05:34,  1.66s/it, Loss=2.9294, Top1=N/A, LR=0.014328]   2025-11-07 07:29:25,310 - INFO - Step 177440: {'train_loss_batch': 2.3096115589141846, 'train_lr': 0.014328060330184448, 'batch_time': 1.6538563309312644, 'data_time': 0.0027001918476076553}
2025-11-07 07:29:25 Train Epoch 070:  96%|█████████▌| 2400/2502 [1:06:11<02:48,  1.65s/it, Loss=2.9304, Top1=N/A, LR=0.014328]2025-11-07 07:32:10,886 - INFO - Step 177540: {'train_loss_batch': 3.809976577758789, 'train_lr': 0.014328060330184448, 'batch_time': 1.6539353000675823, 'data_time': 0.0026299099483275503}
2025-11-07 07:32:10 Train Epoch 070: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.66s/it, Loss=2.9344, Top1=N/A, LR=0.014328]2025-11-07 07:34:56,690 - INFO - Step 177640: {'train_loss_batch': 3.7325663566589355, 'train_lr': 0.014328060330184448, 'batch_time': 1.6540994391542394, 'data_time': 0.002591040838913458}
2025-11-07 07:34:56 Train Epoch 070: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=2.9344, Top1=N/A, LR=0.014328]
2025-11-07 07:34:58 Val Epoch 070:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 07:35:03   with torch.cuda.amp.autocast():
2025-11-07 07:35:03 Val Epoch 070: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=2.0446, Top1=71.81%, Top5=90.96%]
2025-11-07 07:36:49 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 07:36:49   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 07:36:49 2025-11-07 07:36:49,963 - INFO - Step 70: {'epoch': 70, 'learning_rate': 0.013058249183553155, 'train_loss': 2.9341334938335955, 'train_top1': 68.33031720809792, 'train_top5': 87.33595221280602, 'train_precision': 68.19825942034561, 'train_recall': 68.19386675013932, 'train_f1': 68.05246417758248, 'val_loss': 2.044551737098694, 'val_top1': 71.80799998779297, 'val_top5': 90.96399998535156, 'val_precision': 72.74615279539253, 'val_recall': 71.812, 'val_f1': 71.38234375531978}
2025-11-07 07:36:49 2025-11-07 07:36:49,965 - INFO - Epoch 070 Summary - LR: 0.013058, Train Loss: 2.9341, Val Loss: 2.0446, Val F1: 71.38%, Val Precision: 72.75%, Val Recall: 71.81%
2025-11-07 07:36:50 Train Epoch 071:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 70 that is less than the current step 177640. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 07:36:54 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 07:36:54   with torch.cuda.amp.autocast():
2025-11-07 07:36:56 Train Epoch 071:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.2079, Top1=67.19%, LR=0.013058]2025-11-07 07:36:56,070 - INFO - Step 177642: {'train_loss_batch': 2.2079386711120605, 'train_lr': 0.013058249183553155, 'batch_time': 5.3079047203063965, 'data_time': 3.6647775173187256}
2025-11-07 07:36:56 Train Epoch 071:   4%|▍         | 100/2502 [02:51<1:06:13,  1.65s/it, Loss=2.9624, Top1=N/A, LR=0.013058]   2025-11-07 07:39:41,819 - INFO - Step 177742: {'train_loss_batch': 4.28049373626709, 'train_lr': 0.013058249183553155, 'batch_time': 1.6936365590237155, 'data_time': 0.0372941257930038}
2025-11-07 07:39:41 Train Epoch 071:   8%|▊         | 200/2502 [05:36<1:02:56,  1.64s/it, Loss=2.9203, Top1=N/A, LR=0.013058]2025-11-07 07:42:27,287 - INFO - Step 177842: {'train_loss_batch': 2.442920446395874, 'train_lr': 0.013058249183553155, 'batch_time': 1.674253990401083, 'data_time': 0.019228598371667054}
2025-11-07 07:42:27 Train Epoch 071:  12%|█▏        | 300/2502 [08:20<1:00:55,  1.66s/it, Loss=2.9293, Top1=N/A, LR=0.013058]2025-11-07 07:45:11,685 - INFO - Step 177942: {'train_loss_batch': 2.1231932640075684, 'train_lr': 0.013058249183553155, 'batch_time': 1.6641936888330402, 'data_time': 0.01317374571613299}
2025-11-07 07:45:11 Train Epoch 071:  16%|█▌        | 400/2502 [11:05<57:42,  1.65s/it, Loss=2.9039, Top1=N/A, LR=0.013058]2025-11-07 07:47:56,707 - INFO - Step 178042: {'train_loss_batch': 3.7142446041107178, 'train_lr': 0.013058249183553155, 'batch_time': 1.6607112628860663, 'data_time': 0.010142217550491751}
2025-11-07 07:47:56 Train Epoch 071:  20%|█▉        | 500/2502 [13:51<55:23,  1.66s/it, Loss=2.9037, Top1=N/A, LR=0.013058]2025-11-07 07:50:41,777 - INFO - Step 178142: {'train_loss_batch': 2.6010594367980957, 'train_lr': 0.013058249183553155, 'batch_time': 1.6587129676651335, 'data_time': 0.00831327371730538}
2025-11-07 07:50:41 Train Epoch 071:  24%|██▍       | 600/2502 [16:36<52:17,  1.65s/it, Loss=2.9222, Top1=68.90%, LR=0.013058]2025-11-07 07:53:27,247 - INFO - Step 178242: {'train_loss_batch': 2.356455087661743, 'train_lr': 0.013058249183553155, 'batch_time': 1.6580426018567331, 'data_time': 0.007096674597005479}
2025-11-07 07:53:27 Train Epoch 071:  28%|██▊       | 700/2502 [19:21<49:00,  1.63s/it, Loss=2.9223, Top1=N/A, LR=0.013058]   2025-11-07 07:56:12,456 - INFO - Step 178342: {'train_loss_batch': 2.348560094833374, 'train_lr': 0.013058249183553155, 'batch_time': 1.6571941991335315, 'data_time': 0.006231948414474683}
2025-11-07 07:56:12 Train Epoch 071:  32%|███▏      | 800/2502 [22:05<46:58,  1.66s/it, Loss=2.9420, Top1=68.92%, LR=0.013058]2025-11-07 07:58:56,178 - INFO - Step 178442: {'train_loss_batch': 2.3018877506256104, 'train_lr': 0.013058249183553155, 'batch_time': 1.6547001408280504, 'data_time': 0.005569919664761547}
2025-11-07 07:58:56 Train Epoch 071:  36%|███▌      | 900/2502 [24:51<44:08,  1.65s/it, Loss=2.9422, Top1=N/A, LR=0.013058]   2025-11-07 08:01:41,857 - INFO - Step 178542: {'train_loss_batch': 2.487332344055176, 'train_lr': 0.013058249183553155, 'batch_time': 1.654932571436536, 'data_time': 0.005070862045034055}
2025-11-07 08:01:41 Train Epoch 071:  40%|███▉      | 1000/2502 [27:36<41:35,  1.66s/it, Loss=2.9575, Top1=68.96%, LR=0.013058]2025-11-07 08:04:27,749 - INFO - Step 178642: {'train_loss_batch': 2.1471526622772217, 'train_lr': 0.013058249183553155, 'batch_time': 1.6553300115373824, 'data_time': 0.004669663432118418}
2025-11-07 08:04:27 Train Epoch 071:  44%|████▍     | 1100/2502 [30:22<38:38,  1.65s/it, Loss=2.9550, Top1=68.96%, LR=0.013058]2025-11-07 08:07:13,406 - INFO - Step 178742: {'train_loss_batch': 2.3624391555786133, 'train_lr': 0.013058249183553155, 'batch_time': 1.6554427651466834, 'data_time': 0.004338715316814038}
2025-11-07 08:07:13 Train Epoch 071:  48%|████▊     | 1200/2502 [33:08<36:07,  1.66s/it, Loss=2.9603, Top1=N/A, LR=0.013058]   2025-11-07 08:09:59,086 - INFO - Step 178842: {'train_loss_batch': 3.6940932273864746, 'train_lr': 0.013058249183553155, 'batch_time': 1.6555550638384664, 'data_time': 0.004060824248911836}
2025-11-07 08:09:59 Train Epoch 071:  52%|█████▏    | 1300/2502 [35:54<33:11,  1.66s/it, Loss=2.9629, Top1=N/A, LR=0.013058]2025-11-07 08:12:45,055 - INFO - Step 178942: {'train_loss_batch': 3.744476795196533, 'train_lr': 0.013058249183553155, 'batch_time': 1.655872913620089, 'data_time': 0.0038260541266427416}
2025-11-07 08:12:45 Train Epoch 071:  56%|█████▌    | 1400/2502 [38:39<30:25,  1.66s/it, Loss=2.9612, Top1=N/A, LR=0.013058]2025-11-07 08:15:30,437 - INFO - Step 179042: {'train_loss_batch': 2.233142614364624, 'train_lr': 0.013058249183553155, 'batch_time': 1.6557261472425657, 'data_time': 0.0036303848985431027}
2025-11-07 08:15:30 Train Epoch 071:  60%|█████▉    | 1500/2502 [41:25<27:41,  1.66s/it, Loss=2.9514, Top1=68.88%, LR=0.013058]2025-11-07 08:18:16,330 - INFO - Step 179142: {'train_loss_batch': 2.2999866008758545, 'train_lr': 0.013058249183553155, 'batch_time': 1.6559397402324334, 'data_time': 0.0034554967238536126}
2025-11-07 08:18:16 Train Epoch 071:  64%|██████▍   | 1600/2502 [44:10<24:43,  1.64s/it, Loss=2.9532, Top1=N/A, LR=0.013058]   2025-11-07 08:21:01,495 - INFO - Step 179242: {'train_loss_batch': 2.228364944458008, 'train_lr': 0.013058249183553155, 'batch_time': 1.6556716939198233, 'data_time': 0.0033021198072558564}
2025-11-07 08:21:01 Train Epoch 071:  68%|██████▊   | 1700/2502 [46:54<21:48,  1.63s/it, Loss=2.9580, Top1=N/A, LR=0.013058]2025-11-07 08:23:45,713 - INFO - Step 179342: {'train_loss_batch': 2.2286217212677, 'train_lr': 0.013058249183553155, 'batch_time': 1.6548782307424663, 'data_time': 0.003167463007707725}
2025-11-07 08:23:45 Train Epoch 071:  72%|███████▏  | 1800/2502 [49:39<19:14,  1.65s/it, Loss=2.9605, Top1=68.83%, LR=0.013058]2025-11-07 08:26:30,442 - INFO - Step 179442: {'train_loss_batch': 2.1682722568511963, 'train_lr': 0.013058249183553155, 'batch_time': 1.6544565354897405, 'data_time': 0.0030484686686819227}
2025-11-07 08:26:30 Train Epoch 071:  76%|███████▌  | 1900/2502 [52:25<16:36,  1.66s/it, Loss=2.9628, Top1=N/A, LR=0.013058]   2025-11-07 08:29:15,859 - INFO - Step 179542: {'train_loss_batch': 3.189016342163086, 'train_lr': 0.013058249183553155, 'batch_time': 1.6544416256793733, 'data_time': 0.0029430274020239657}
2025-11-07 08:29:15 Train Epoch 071:  80%|███████▉  | 2000/2502 [55:10<13:48,  1.65s/it, Loss=2.9623, Top1=N/A, LR=0.013058]2025-11-07 08:32:00,896 - INFO - Step 179642: {'train_loss_batch': 2.2695109844207764, 'train_lr': 0.013058249183553155, 'batch_time': 1.6542379005380656, 'data_time': 0.0028442295118309984}
2025-11-07 08:32:00 Train Epoch 071:  84%|████████▍ | 2100/2502 [57:55<11:04,  1.65s/it, Loss=2.9606, Top1=N/A, LR=0.013058]2025-11-07 08:34:46,622 - INFO - Step 179742: {'train_loss_batch': 2.409541606903076, 'train_lr': 0.013058249183553155, 'batch_time': 1.6543817685820386, 'data_time': 0.0027553744681502227}
2025-11-07 08:34:46 Train Epoch 071:  88%|████████▊ | 2200/2502 [1:00:41<08:17,  1.65s/it, Loss=2.9592, Top1=N/A, LR=0.013058]2025-11-07 08:37:31,928 - INFO - Step 179842: {'train_loss_batch': 3.546950340270996, 'train_lr': 0.013058249183553155, 'batch_time': 1.654321636843822, 'data_time': 0.0026759591117765298}
2025-11-07 08:37:31 Train Epoch 071:  92%|█████████▏| 2300/2502 [1:03:25<05:29,  1.63s/it, Loss=2.9618, Top1=N/A, LR=0.013058]2025-11-07 08:40:16,258 - INFO - Step 179942: {'train_loss_batch': 4.432116508483887, 'train_lr': 0.013058249183553155, 'batch_time': 1.6538424413134771, 'data_time': 0.002602864327196555}
2025-11-07 08:40:16 Train Epoch 071:  96%|█████████▌| 2400/2502 [1:06:10<02:48,  1.65s/it, Loss=2.9634, Top1=N/A, LR=0.013058]2025-11-07 08:43:00,834 - INFO - Step 180042: {'train_loss_batch': 2.3293304443359375, 'train_lr': 0.013058249183553155, 'batch_time': 1.6535056382107367, 'data_time': 0.0025361280349928458}
2025-11-07 08:43:00 Train Epoch 071: 100%|█████████▉| 2500/2502 [1:08:55<00:03,  1.64s/it, Loss=2.9642, Top1=68.77%, LR=0.013058]2025-11-07 08:45:46,338 - INFO - Step 180142: {'train_loss_batch': 2.286115884780884, 'train_lr': 0.013058249183553155, 'batch_time': 1.6535669311148222, 'data_time': 0.002500041014477998}
2025-11-07 08:45:46 Train Epoch 071: 100%|██████████| 2502/2502 [1:08:57<00:00,  1.65s/it, Loss=2.9642, Top1=68.77%, LR=0.013058]
2025-11-07 08:45:48 Val Epoch 071:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 08:45:53   with torch.cuda.amp.autocast():
2025-11-07 08:45:53 Val Epoch 071: 100%|██████████| 98/98 [01:49<00:00,  1.12s/it, Loss=2.0261, Top1=72.04%, Top5=91.15%]
2025-11-07 08:47:38 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 08:47:38   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 08:47:38 2025-11-07 08:47:38,174 - INFO - Step 71: {'epoch': 71, 'learning_rate': 0.011838902854358556, 'train_loss': 2.9639520421207286, 'train_top1': 68.76711902390439, 'train_top5': 87.53345991035856, 'train_precision': 68.64529274557988, 'train_recall': 68.68221244430158, 'train_f1': 68.50532899959708, 'val_loss': 2.026078134727478, 'val_top1': 72.03999998046875, 'val_top5': 91.15199998779296, 'val_precision': 72.98763438908726, 'val_recall': 72.048, 'val_f1': 71.58478036784092}
2025-11-07 08:47:38 2025-11-07 08:47:38,176 - INFO - Epoch 071 Summary - LR: 0.011839, Train Loss: 2.9640, Val Loss: 2.0261, Val F1: 71.58%, Val Precision: 72.99%, Val Recall: 72.05%
2025-11-07 08:47:39 Train Epoch 072:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 08:47:42   with torch.cuda.amp.autocast():
2025-11-07 08:47:44 wandb: WARNING Tried to log to step 71 that is less than the current step 180142. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 08:47:44 Train Epoch 072:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.8553, Top1=N/A, LR=0.011839]2025-11-07 08:47:44,445 - INFO - Step 180144: {'train_loss_batch': 2.8553080558776855, 'train_lr': 0.011838902854358556, 'batch_time': 5.252981424331665, 'data_time': 3.6022536754608154}
2025-11-07 08:47:44 Train Epoch 072:   4%|▍         | 100/2502 [02:50<1:06:16,  1.66s/it, Loss=2.8841, Top1=69.98%, LR=0.011839]2025-11-07 08:50:29,616 - INFO - Step 180244: {'train_loss_batch': 2.1514687538146973, 'train_lr': 0.011838902854358556, 'batch_time': 1.6873806962872495, 'data_time': 0.036789981445463575}
2025-11-07 08:50:29 Train Epoch 072:   8%|▊         | 200/2502 [05:35<1:03:05,  1.64s/it, Loss=2.8883, Top1=N/A, LR=0.011839]   2025-11-07 08:53:14,959 - INFO - Step 180344: {'train_loss_batch': 3.09196138381958, 'train_lr': 0.011838902854358556, 'batch_time': 1.6704863813979116, 'data_time': 0.019005419603034632}
2025-11-07 08:53:14 Train Epoch 072:  12%|█▏        | 300/2502 [08:21<1:00:39,  1.65s/it, Loss=2.9053, Top1=N/A, LR=0.011839]2025-11-07 08:56:00,437 - INFO - Step 180444: {'train_loss_batch': 3.6017560958862305, 'train_lr': 0.011838902854358556, 'batch_time': 1.6652662397619102, 'data_time': 0.013053105123019298}
2025-11-07 08:56:00 Train Epoch 072:  16%|█▌        | 400/2502 [11:06<57:43,  1.65s/it, Loss=2.9236, Top1=N/A, LR=0.011839]2025-11-07 08:58:46,046 - INFO - Step 180544: {'train_loss_batch': 3.0320651531219482, 'train_lr': 0.011838902854358556, 'batch_time': 1.6629779796647906, 'data_time': 0.010068282819448266}
2025-11-07 08:58:46 Train Epoch 072:  20%|█▉        | 500/2502 [13:51<54:45,  1.64s/it, Loss=2.8965, Top1=69.66%, LR=0.011839]2025-11-07 09:01:31,098 - INFO - Step 180644: {'train_loss_batch': 2.189072608947754, 'train_lr': 0.011838902854358556, 'batch_time': 1.6604918458028706, 'data_time': 0.008262424411887894}
2025-11-07 09:01:31 Train Epoch 072:  24%|██▍       | 600/2502 [16:36<52:04,  1.64s/it, Loss=2.8872, Top1=69.71%, LR=0.011839]2025-11-07 09:04:16,031 - INFO - Step 180744: {'train_loss_batch': 2.189943313598633, 'train_lr': 0.011838902854358556, 'batch_time': 1.6586336236626853, 'data_time': 0.007062522028131215}
2025-11-07 09:04:16 Train Epoch 072:  28%|██▊       | 700/2502 [19:22<49:47,  1.66s/it, Loss=2.8836, Top1=N/A, LR=0.011839]   2025-11-07 09:07:01,427 - INFO - Step 180844: {'train_loss_batch': 3.3353281021118164, 'train_lr': 0.011838902854358556, 'batch_time': 1.6579663335171644, 'data_time': 0.006207346746142682}
2025-11-07 09:07:01 Train Epoch 072:  32%|███▏      | 800/2502 [22:07<46:33,  1.64s/it, Loss=2.9031, Top1=N/A, LR=0.011839]2025-11-07 09:09:46,269 - INFO - Step 180944: {'train_loss_batch': 2.3659324645996094, 'train_lr': 0.011838902854358556, 'batch_time': 1.6567753173885273, 'data_time': 0.005563968129818806}
2025-11-07 09:09:46 Train Epoch 072:  36%|███▌      | 900/2502 [24:52<44:22,  1.66s/it, Loss=2.8946, Top1=N/A, LR=0.011839]2025-11-07 09:12:31,820 - INFO - Step 181044: {'train_loss_batch': 3.76322078704834, 'train_lr': 0.011838902854358556, 'batch_time': 1.6566342997894965, 'data_time': 0.005062550471704888}
2025-11-07 09:12:31 Train Epoch 072:  40%|███▉      | 1000/2502 [27:38<41:20,  1.65s/it, Loss=2.8862, Top1=N/A, LR=0.011839]2025-11-07 09:15:17,221 - INFO - Step 181144: {'train_loss_batch': 3.481250762939453, 'train_lr': 0.011838902854358556, 'batch_time': 1.6563719662276657, 'data_time': 0.004658294605327534}
2025-11-07 09:15:17 Train Epoch 072:  44%|████▍     | 1100/2502 [30:23<38:47,  1.66s/it, Loss=2.8828, Top1=69.63%, LR=0.011839]2025-11-07 09:18:02,441 - INFO - Step 181244: {'train_loss_batch': 2.1813626289367676, 'train_lr': 0.011838902854358556, 'batch_time': 1.6559921958899086, 'data_time': 0.004327289848085104}
2025-11-07 09:18:02 Train Epoch 072:  48%|████▊     | 1200/2502 [33:09<35:58,  1.66s/it, Loss=2.8986, Top1=N/A, LR=0.011839]   2025-11-07 09:20:48,270 - INFO - Step 181344: {'train_loss_batch': 2.8343849182128906, 'train_lr': 0.011838902854358556, 'batch_time': 1.6561833746129528, 'data_time': 0.004058171867033921}
2025-11-07 09:20:48 Train Epoch 072:  52%|█████▏    | 1300/2502 [35:54<33:08,  1.65s/it, Loss=2.8977, Top1=N/A, LR=0.011839]2025-11-07 09:23:34,183 - INFO - Step 181444: {'train_loss_batch': 2.5153205394744873, 'train_lr': 0.011838902854358556, 'batch_time': 1.656410139950306, 'data_time': 0.003825057569968526}
2025-11-07 09:23:34 Train Epoch 072:  56%|█████▌    | 1400/2502 [38:40<30:29,  1.66s/it, Loss=2.8981, Top1=69.42%, LR=0.011839]2025-11-07 09:26:19,638 - INFO - Step 181544: {'train_loss_batch': 2.3141372203826904, 'train_lr': 0.011838902854358556, 'batch_time': 1.6562768910970966, 'data_time': 0.003619765656748301}
2025-11-07 09:26:19 Train Epoch 072:  60%|█████▉    | 1500/2502 [41:26<27:39,  1.66s/it, Loss=2.9109, Top1=69.39%, LR=0.011839]2025-11-07 09:29:05,691 - INFO - Step 181644: {'train_loss_batch': 2.283285617828369, 'train_lr': 0.011838902854358556, 'batch_time': 1.656560234670874, 'data_time': 0.0034468038966860635}
2025-11-07 09:29:05 Train Epoch 072:  64%|██████▍   | 1600/2502 [44:11<24:47,  1.65s/it, Loss=2.9124, Top1=N/A, LR=0.011839]   2025-11-07 09:31:50,360 - INFO - Step 181744: {'train_loss_batch': 3.2937026023864746, 'train_lr': 0.011838902854358556, 'batch_time': 1.6559433688378795, 'data_time': 0.0032907972329858687}
2025-11-07 09:31:50 Train Epoch 072:  68%|██████▊   | 1700/2502 [46:56<22:03,  1.65s/it, Loss=2.9172, Top1=N/A, LR=0.011839]2025-11-07 09:34:35,660 - INFO - Step 181844: {'train_loss_batch': 2.2766194343566895, 'train_lr': 0.011838902854358556, 'batch_time': 1.6557701240911826, 'data_time': 0.003156658343046571}
2025-11-07 09:34:35 Train Epoch 072:  72%|███████▏  | 1800/2502 [49:41<19:06,  1.63s/it, Loss=2.9148, Top1=N/A, LR=0.011839]2025-11-07 09:37:20,475 - INFO - Step 181944: {'train_loss_batch': 3.7354955673217773, 'train_lr': 0.011838902854358556, 'batch_time': 1.6553471273743132, 'data_time': 0.003036510937747394}
2025-11-07 09:37:20 Train Epoch 072:  76%|███████▌  | 1900/2502 [52:25<16:41,  1.66s/it, Loss=2.9150, Top1=69.37%, LR=0.011839]2025-11-07 09:40:04,751 - INFO - Step 182044: {'train_loss_batch': 2.162630081176758, 'train_lr': 0.011838902854358556, 'batch_time': 1.654684856902418, 'data_time': 0.0029314953925921876}
2025-11-07 09:40:04 Train Epoch 072:  80%|███████▉  | 2000/2502 [55:09<13:49,  1.65s/it, Loss=2.9095, Top1=N/A, LR=0.011839]   2025-11-07 09:42:48,828 - INFO - Step 182144: {'train_loss_batch': 3.065269947052002, 'train_lr': 0.011838902854358556, 'batch_time': 1.653989539272722, 'data_time': 0.0028338577674663643}
2025-11-07 09:42:48 Train Epoch 072:  84%|████████▍ | 2100/2502 [57:55<11:05,  1.65s/it, Loss=2.9059, Top1=N/A, LR=0.011839]2025-11-07 09:45:34,703 - INFO - Step 182244: {'train_loss_batch': 3.9596142768859863, 'train_lr': 0.011838902854358556, 'batch_time': 1.6542159046007872, 'data_time': 0.00274777378370965}
2025-11-07 09:45:34 Train Epoch 072:  88%|████████▊ | 2200/2502 [1:00:41<08:17,  1.65s/it, Loss=2.9038, Top1=N/A, LR=0.011839]2025-11-07 09:48:20,508 - INFO - Step 182344: {'train_loss_batch': 3.469841480255127, 'train_lr': 0.011838902854358556, 'batch_time': 1.6543902417087164, 'data_time': 0.0026725075126831665}
2025-11-07 09:48:20 Train Epoch 072:  92%|█████████▏| 2300/2502 [1:03:27<05:32,  1.65s/it, Loss=2.9046, Top1=N/A, LR=0.011839]2025-11-07 09:51:06,326 - INFO - Step 182444: {'train_loss_batch': 4.399022102355957, 'train_lr': 0.011838902854358556, 'batch_time': 1.6545547809045453, 'data_time': 0.002601727357588349}
2025-11-07 09:51:06 Train Epoch 072:  96%|█████████▌| 2400/2502 [1:06:12<02:49,  1.66s/it, Loss=2.9017, Top1=69.23%, LR=0.011839]2025-11-07 09:53:51,788 - INFO - Step 182544: {'train_loss_batch': 2.0923571586608887, 'train_lr': 0.011838902854358556, 'batch_time': 1.654557432447161, 'data_time': 0.0025346134166725474}
2025-11-07 09:53:51 Train Epoch 072: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.64s/it, Loss=2.9016, Top1=N/A, LR=0.011839]   2025-11-07 09:56:35,924 - INFO - Step 182644: {'train_loss_batch': 4.552313804626465, 'train_lr': 0.011838902854358556, 'batch_time': 1.6540296183543794, 'data_time': 0.0024993746626715526}
2025-11-07 09:56:35 Train Epoch 072: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=2.9016, Top1=N/A, LR=0.011839]
2025-11-07 09:56:38 Val Epoch 072:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 09:56:42   with torch.cuda.amp.autocast():
2025-11-07 09:56:43 Val Epoch 072: 100%|██████████| 98/98 [01:48<00:00,  1.11s/it, Loss=2.0361, Top1=72.68%, Top5=91.40%]
2025-11-07 09:58:26 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 09:58:26   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 09:58:26 2025-11-07 09:58:26,712 - INFO - Step 72: {'epoch': 72, 'learning_rate': 0.010671686823016007, 'train_loss': 2.901329292477273, 'train_top1': 69.22328838174273, 'train_top5': 87.938440093361, 'train_precision': 69.08028212644787, 'train_recall': 69.11848380823415, 'train_f1': 68.94306968943006, 'val_loss': 2.0360620886611938, 'val_top1': 72.67599997558594, 'val_top5': 91.39999998291016, 'val_precision': 73.70253616753779, 'val_recall': 72.678, 'val_f1': 72.32299281942424}
2025-11-07 09:58:26 2025-11-07 09:58:26,713 - INFO - Epoch 072 Summary - LR: 0.010672, Train Loss: 2.9013, Val Loss: 2.0361, Val F1: 72.32%, Val Precision: 73.70%, Val Recall: 72.68%
2025-11-07 09:58:29 2025-11-07 09:58:29,722 - INFO - New best model saved with validation accuracy: 72.676%
2025-11-07 09:58:29 2025-11-07 09:58:29,722 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_073.pth
2025-11-07 09:58:29 Train Epoch 073:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 09:58:33   with torch.cuda.amp.autocast():
2025-11-07 09:58:34 wandb: WARNING Tried to log to step 72 that is less than the current step 182644. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 09:58:35 Train Epoch 073:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.3266, Top1=N/A, LR=0.010672]2025-11-07 09:58:35,172 - INFO - Step 182646: {'train_loss_batch': 2.3265609741210938, 'train_lr': 0.010671686823016007, 'batch_time': 5.4489123821258545, 'data_time': 3.805612802505493}
2025-11-07 09:58:35 Train Epoch 073:   4%|▍         | 100/2502 [02:50<1:05:29,  1.64s/it, Loss=2.9453, Top1=N/A, LR=0.010672]2025-11-07 10:01:20,406 - INFO - Step 182746: {'train_loss_batch': 3.6794025897979736, 'train_lr': 0.010671686823016007, 'batch_time': 1.6899253118156206, 'data_time': 0.03875494711469896}
2025-11-07 10:01:20 Train Epoch 073:   8%|▊         | 200/2502 [05:34<1:03:49,  1.66s/it, Loss=2.9323, Top1=N/A, LR=0.010672]2025-11-07 10:04:04,680 - INFO - Step 182846: {'train_loss_batch': 2.352017879486084, 'train_lr': 0.010671686823016007, 'batch_time': 1.6664517471446327, 'data_time': 0.01998780971735864}
2025-11-07 10:04:04 Train Epoch 073:  12%|█▏        | 300/2502 [08:20<1:00:07,  1.64s/it, Loss=2.9118, Top1=N/A, LR=0.010672]2025-11-07 10:06:50,228 - INFO - Step 182946: {'train_loss_batch': 2.5511221885681152, 'train_lr': 0.010671686823016007, 'batch_time': 1.6628036102979287, 'data_time': 0.013683375330066364}
2025-11-07 10:06:50 Train Epoch 073:  16%|█▌        | 400/2502 [11:05<58:01,  1.66s/it, Loss=2.8841, Top1=N/A, LR=0.010672]2025-11-07 10:09:35,415 - INFO - Step 183046: {'train_loss_batch': 3.329066753387451, 'train_lr': 0.010671686823016007, 'batch_time': 1.660077005253171, 'data_time': 0.010522555234723554}
2025-11-07 10:09:35 Train Epoch 073:  20%|█▉        | 500/2502 [13:51<54:59,  1.65s/it, Loss=2.8850, Top1=N/A, LR=0.010672]2025-11-07 10:12:21,036 - INFO - Step 183146: {'train_loss_batch': 3.8427252769470215, 'train_lr': 0.010671686823016007, 'batch_time': 1.659305050939381, 'data_time': 0.008631276989173509}
2025-11-07 10:12:21 Train Epoch 073:  24%|██▍       | 600/2502 [16:36<52:36,  1.66s/it, Loss=2.8676, Top1=70.19%, LR=0.010672]2025-11-07 10:15:06,151 - INFO - Step 183246: {'train_loss_batch': 2.1545071601867676, 'train_lr': 0.010671686823016007, 'batch_time': 1.657948341623519, 'data_time': 0.007360101737912602}
2025-11-07 10:15:06 Train Epoch 073:  28%|██▊       | 700/2502 [19:21<49:35,  1.65s/it, Loss=2.8704, Top1=N/A, LR=0.010672]   2025-11-07 10:17:51,564 - INFO - Step 183346: {'train_loss_batch': 3.256206512451172, 'train_lr': 0.010671686823016007, 'batch_time': 1.657402217133069, 'data_time': 0.006445935041180011}
2025-11-07 10:17:51 Train Epoch 073:  32%|███▏      | 800/2502 [22:07<46:46,  1.65s/it, Loss=2.8641, Top1=N/A, LR=0.010672]2025-11-07 10:20:37,224 - INFO - Step 183446: {'train_loss_batch': 2.8274078369140625, 'train_lr': 0.010671686823016007, 'batch_time': 1.6573027573274763, 'data_time': 0.005767827325694719}
2025-11-07 10:20:37 Train Epoch 073:  36%|███▌      | 900/2502 [24:53<44:08,  1.65s/it, Loss=2.8704, Top1=N/A, LR=0.010672]2025-11-07 10:23:22,819 - INFO - Step 183546: {'train_loss_batch': 3.9968791007995605, 'train_lr': 0.010671686823016007, 'batch_time': 1.6571515328876187, 'data_time': 0.005243114043816875}
2025-11-07 10:23:22 Train Epoch 073:  40%|███▉      | 1000/2502 [27:38<41:27,  1.66s/it, Loss=2.8731, Top1=N/A, LR=0.010672]2025-11-07 10:26:07,914 - INFO - Step 183646: {'train_loss_batch': 3.526078224182129, 'train_lr': 0.010671686823016007, 'batch_time': 1.656532351906364, 'data_time': 0.004818791037911063}
2025-11-07 10:26:07 Train Epoch 073:  44%|████▍     | 1100/2502 [30:23<38:50,  1.66s/it, Loss=2.8634, Top1=N/A, LR=0.010672]2025-11-07 10:28:53,334 - INFO - Step 183746: {'train_loss_batch': 2.1839048862457275, 'train_lr': 0.010671686823016007, 'batch_time': 1.6563204757524121, 'data_time': 0.0044750939922696565}
2025-11-07 10:28:53 Train Epoch 073:  48%|████▊     | 1200/2502 [33:09<35:56,  1.66s/it, Loss=2.8661, Top1=N/A, LR=0.010672]2025-11-07 10:31:39,009 - INFO - Step 183846: {'train_loss_batch': 4.3253092765808105, 'train_lr': 0.010671686823016007, 'batch_time': 1.6563555253336173, 'data_time': 0.004182209678732485}
2025-11-07 10:31:39 Train Epoch 073:  52%|█████▏    | 1300/2502 [35:54<32:47,  1.64s/it, Loss=2.8683, Top1=N/A, LR=0.010672]2025-11-07 10:34:24,202 - INFO - Step 183946: {'train_loss_batch': 2.317268133163452, 'train_lr': 0.010671686823016007, 'batch_time': 1.6560153312448536, 'data_time': 0.003934394387810345}
2025-11-07 10:34:24 Train Epoch 073:  56%|█████▌    | 1400/2502 [38:40<30:33,  1.66s/it, Loss=2.8762, Top1=N/A, LR=0.010672]2025-11-07 10:37:09,822 - INFO - Step 184046: {'train_loss_batch': 2.2242889404296875, 'train_lr': 0.010671686823016007, 'batch_time': 1.6560280889719405, 'data_time': 0.0037257848340728128}
2025-11-07 10:37:09 Train Epoch 073:  58%|█████▊    | 1454/2502 [40:08<29:06,  1.67s/it, Loss=2.8762, Top1=N/A, LR=0.010672]
2025-11-07 10:39:55 Train Epoch 073:  64%|██████▍   | 1600/2502 [44:11<24:51,  1.65s/it, Loss=2.8718, Top1=69.90%, LR=0.010672]2025-11-07 10:42:41,571 - INFO - Step 184246: {'train_loss_batch': 2.2513065338134766, 'train_lr': 0.010671686823016007, 'batch_time': 1.6563674052903832, 'data_time': 0.0033869679312196694}
2025-11-07 10:42:41 Train Epoch 073:  68%|██████▊   | 1700/2502 [46:57<22:07,  1.66s/it, Loss=2.8771, Top1=N/A, LR=0.010672]   2025-11-07 10:45:26,909 - INFO - Step 184346: {'train_loss_batch': 2.459956169128418, 'train_lr': 0.010671686823016007, 'batch_time': 1.6561914887728235, 'data_time': 0.0032441016157398078}
2025-11-07 10:45:26 Train Epoch 073:  72%|███████▏  | 1800/2502 [49:42<19:14,  1.64s/it, Loss=2.8774, Top1=N/A, LR=0.010672]2025-11-07 10:48:12,267 - INFO - Step 184446: {'train_loss_batch': 3.0576131343841553, 'train_lr': 0.010671686823016007, 'batch_time': 1.6560466248482084, 'data_time': 0.003119496224258821}
2025-11-07 10:48:12 Train Epoch 073:  76%|███████▌  | 1900/2502 [52:28<16:38,  1.66s/it, Loss=2.8800, Top1=N/A, LR=0.010672]2025-11-07 10:50:58,063 - INFO - Step 184546: {'train_loss_batch': 3.9064130783081055, 'train_lr': 0.010671686823016007, 'batch_time': 1.6561465793882528, 'data_time': 0.003008517511639703}
2025-11-07 10:50:58 Train Epoch 073:  80%|███████▉  | 2000/2502 [55:14<13:46,  1.65s/it, Loss=2.8823, Top1=N/A, LR=0.010672]2025-11-07 10:53:43,860 - INFO - Step 184646: {'train_loss_batch': 2.1755402088165283, 'train_lr': 0.010671686823016007, 'batch_time': 1.6562383699154986, 'data_time': 0.002911217506976797}
2025-11-07 10:53:43 Train Epoch 073:  84%|████████▍ | 2100/2502 [57:59<11:08,  1.66s/it, Loss=2.8809, Top1=N/A, LR=0.010672]2025-11-07 10:56:29,430 - INFO - Step 184746: {'train_loss_batch': 3.3274028301239014, 'train_lr': 0.010671686823016007, 'batch_time': 1.6562123917103495, 'data_time': 0.0028217856739884838}
2025-11-07 10:56:29 Train Epoch 073:  88%|████████▊ | 2200/2502 [1:00:44<08:17,  1.65s/it, Loss=2.8814, Top1=N/A, LR=0.010672]2025-11-07 10:59:14,416 - INFO - Step 184846: {'train_loss_batch': 2.5449113845825195, 'train_lr': 0.010671686823016007, 'batch_time': 1.6559234851601448, 'data_time': 0.0027404024296595474}
2025-11-07 10:59:14 Train Epoch 073:  92%|█████████▏| 2300/2502 [1:03:29<05:35,  1.66s/it, Loss=2.8825, Top1=69.83%, LR=0.010672]2025-11-07 11:01:59,628 - INFO - Step 184946: {'train_loss_batch': 2.1240549087524414, 'train_lr': 0.010671686823016007, 'batch_time': 1.65575844378432, 'data_time': 0.0026663121634180366}
2025-11-07 11:01:59 Train Epoch 073:  96%|█████████▌| 2400/2502 [1:06:15<02:49,  1.66s/it, Loss=2.8815, Top1=69.78%, LR=0.010672]2025-11-07 11:04:45,455 - INFO - Step 185046: {'train_loss_batch': 2.193784236907959, 'train_lr': 0.010671686823016007, 'batch_time': 1.6558626577884938, 'data_time': 0.002598751887139158}
2025-11-07 11:04:45 Train Epoch 073: 100%|█████████▉| 2500/2502 [1:09:01<00:03,  1.66s/it, Loss=2.8830, Top1=N/A, LR=0.010672]   2025-11-07 11:07:31,297 - INFO - Step 185146: {'train_loss_batch': 2.30558705329895, 'train_lr': 0.010671686823016007, 'batch_time': 1.6559649741635327, 'data_time': 0.0025713969020546078}
2025-11-07 11:07:31 Train Epoch 073: 100%|██████████| 2502/2502 [1:09:03<00:00,  1.66s/it, Loss=2.8830, Top1=N/A, LR=0.010672]
2025-11-07 11:07:33 Val Epoch 073:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 11:07:38   with torch.cuda.amp.autocast():
2025-11-07 11:07:38 Val Epoch 073: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=2.0195, Top1=72.54%, Top5=91.42%]
2025-11-07 11:09:24 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 11:09:24   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 11:09:24 2025-11-07 11:09:24,056 - INFO - Step 73: {'epoch': 73, 'learning_rate': 0.009558195366224508, 'train_loss': 2.882701378360355, 'train_top1': 69.78343059738955, 'train_top5': 88.16123870481928, 'train_precision': 69.6218179795208, 'train_recall': 69.65674966400987, 'train_f1': 69.49308399161337, 'val_loss': 2.019479747314453, 'val_top1': 72.54399998779297, 'val_top5': 91.42199998046875, 'val_precision': 73.49719896049783, 'val_recall': 72.54599999999999, 'val_f1': 72.14416444000578}
2025-11-07 11:09:24 2025-11-07 11:09:24,058 - INFO - Epoch 073 Summary - LR: 0.009558, Train Loss: 2.8827, Val Loss: 2.0195, Val F1: 72.14%, Val Precision: 73.50%, Val Recall: 72.55%
2025-11-07 11:09:24 wandb: WARNING Tried to log to step 73 that is less than the current step 185146. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 11:09:24 Train Epoch 074:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 11:09:28   with torch.cuda.amp.autocast():
2025-11-07 11:09:29 Train Epoch 074:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.2627, Top1=N/A, LR=0.009558]2025-11-07 11:09:29,807 - INFO - Step 185148: {'train_loss_batch': 2.262695550918579, 'train_lr': 0.009558195366224508, 'batch_time': 5.21520209312439, 'data_time': 3.5799386501312256}
2025-11-07 11:09:29 Train Epoch 074:   2%|▏         | 60/2502 [01:42<1:07:28,  1.66s/it, Loss=2.2627, Top1=N/A, LR=0.009558]
2025-11-07 11:12:15 Train Epoch 074:   8%|▊         | 200/2502 [05:35<1:03:16,  1.65s/it, Loss=2.8669, Top1=70.37%, LR=0.009558]2025-11-07 11:15:00,581 - INFO - Step 185348: {'train_loss_batch': 2.12185001373291, 'train_lr': 0.009558195366224508, 'batch_time': 1.671585578823564, 'data_time': 0.01884424034042738}
2025-11-07 11:15:00 Train Epoch 074:  12%|█▏        | 300/2502 [08:21<1:00:49,  1.66s/it, Loss=2.9230, Top1=N/A, LR=0.009558]   2025-11-07 11:17:46,341 - INFO - Step 185448: {'train_loss_batch': 3.7805542945861816, 'train_lr': 0.009558195366224508, 'batch_time': 1.6669387175791288, 'data_time': 0.012919815671800379}
2025-11-07 11:17:46 Train Epoch 074:  16%|█▌        | 400/2502 [11:07<57:53,  1.65s/it, Loss=2.9434, Top1=N/A, LR=0.009558]2025-11-07 11:20:31,956 - INFO - Step 185548: {'train_loss_batch': 3.697577476501465, 'train_lr': 0.009558195366224508, 'batch_time': 1.6642469503635777, 'data_time': 0.00997387857508481}
2025-11-07 11:20:31 Train Epoch 074:  20%|█▉        | 500/2502 [13:52<55:14,  1.66s/it, Loss=2.9162, Top1=N/A, LR=0.009558]2025-11-07 11:23:17,311 - INFO - Step 185648: {'train_loss_batch': 3.8042044639587402, 'train_lr': 0.009558195366224508, 'batch_time': 1.6621116402144442, 'data_time': 0.008184381587776596}
2025-11-07 11:23:17 Train Epoch 074:  24%|██▍       | 600/2502 [16:38<52:20,  1.65s/it, Loss=2.8911, Top1=N/A, LR=0.009558]2025-11-07 11:26:02,822 - INFO - Step 185748: {'train_loss_batch': 3.22109317779541, 'train_lr': 0.009558195366224508, 'batch_time': 1.6609461382899229, 'data_time': 0.006986355821225488}
2025-11-07 11:26:02 Train Epoch 074:  28%|██▊       | 700/2502 [19:21<49:01,  1.63s/it, Loss=2.9063, Top1=70.12%, LR=0.009558]2025-11-07 11:28:46,263 - INFO - Step 185848: {'train_loss_batch': 2.103790283203125, 'train_lr': 0.009558195366224508, 'batch_time': 1.657161398042796, 'data_time': 0.006128652629770668}
2025-11-07 11:28:46 Train Epoch 074:  32%|███▏      | 800/2502 [22:05<46:47,  1.65s/it, Loss=2.9052, Top1=N/A, LR=0.009558]   2025-11-07 11:31:30,395 - INFO - Step 185948: {'train_loss_batch': 3.638526439666748, 'train_lr': 0.009558195366224508, 'batch_time': 1.6551824354202709, 'data_time': 0.00548569867375787}
2025-11-07 11:31:30 Train Epoch 074:  36%|███▌      | 900/2502 [24:50<44:10,  1.65s/it, Loss=2.9097, Top1=N/A, LR=0.009558]2025-11-07 11:34:15,523 - INFO - Step 186048: {'train_loss_batch': 4.319411277770996, 'train_lr': 0.009558195366224508, 'batch_time': 1.6547497697463973, 'data_time': 0.004989008792364902}
2025-11-07 11:34:15 Train Epoch 074:  40%|███▉      | 1000/2502 [27:36<41:29,  1.66s/it, Loss=2.9170, Top1=70.06%, LR=0.009558]2025-11-07 11:37:01,043 - INFO - Step 186148: {'train_loss_batch': 2.2776920795440674, 'train_lr': 0.009558195366224508, 'batch_time': 1.6547943788332182, 'data_time': 0.004589308749188434}
2025-11-07 11:37:01 Train Epoch 074:  44%|████▍     | 1100/2502 [30:21<38:45,  1.66s/it, Loss=2.9099, Top1=N/A, LR=0.009558]   2025-11-07 11:39:46,349 - INFO - Step 186248: {'train_loss_batch': 3.6898975372314453, 'train_lr': 0.009558195366224508, 'batch_time': 1.6546363787256946, 'data_time': 0.004260128135577209}
2025-11-07 11:39:46 Train Epoch 074:  48%|████▊     | 1200/2502 [33:06<36:03,  1.66s/it, Loss=2.9091, Top1=N/A, LR=0.009558]2025-11-07 11:42:30,911 - INFO - Step 186348: {'train_loss_batch': 2.9181318283081055, 'train_lr': 0.009558195366224508, 'batch_time': 1.6538854608130793, 'data_time': 0.003988594734897026}
2025-11-07 11:42:30 Train Epoch 074:  52%|█████▏    | 1300/2502 [35:52<33:10,  1.66s/it, Loss=2.9057, Top1=N/A, LR=0.009558]2025-11-07 11:45:16,612 - INFO - Step 186448: {'train_loss_batch': 2.1402034759521484, 'train_lr': 0.009558195366224508, 'batch_time': 1.6541260434149962, 'data_time': 0.0037614042075389538}
2025-11-07 11:45:16 Train Epoch 074:  56%|█████▌    | 1400/2502 [38:37<30:25,  1.66s/it, Loss=2.8970, Top1=N/A, LR=0.009558]2025-11-07 11:48:02,438 - INFO - Step 186548: {'train_loss_batch': 2.6960034370422363, 'train_lr': 0.009558195366224508, 'batch_time': 1.6544209037143618, 'data_time': 0.003567134542689844}
2025-11-07 11:48:02 Train Epoch 074:  60%|█████▉    | 1500/2502 [41:22<27:16,  1.63s/it, Loss=2.8971, Top1=N/A, LR=0.009558]2025-11-07 11:50:47,108 - INFO - Step 186648: {'train_loss_batch': 2.1842594146728516, 'train_lr': 0.009558195366224508, 'batch_time': 1.6539060786753634, 'data_time': 0.0033964724798030968}
2025-11-07 11:50:47 Train Epoch 074:  64%|██████▍   | 1600/2502 [44:07<24:57,  1.66s/it, Loss=2.8964, Top1=N/A, LR=0.009558]2025-11-07 11:53:32,136 - INFO - Step 186748: {'train_loss_batch': 3.0809273719787598, 'train_lr': 0.009558195366224508, 'batch_time': 1.6536792949316725, 'data_time': 0.0032478255975402796}
2025-11-07 11:53:32 Train Epoch 074:  68%|██████▊   | 1700/2502 [46:53<22:13,  1.66s/it, Loss=2.8944, Top1=N/A, LR=0.009558]2025-11-07 11:56:17,982 - INFO - Step 186848: {'train_loss_batch': 2.947341203689575, 'train_lr': 0.009558195366224508, 'batch_time': 1.6539603904161224, 'data_time': 0.0031171449137603023}
2025-11-07 11:56:17 Train Epoch 074:  72%|███████▏  | 1800/2502 [49:38<19:17,  1.65s/it, Loss=2.8911, Top1=N/A, LR=0.009558]2025-11-07 11:59:03,194 - INFO - Step 186948: {'train_loss_batch': 3.7566170692443848, 'train_lr': 0.009558195366224508, 'batch_time': 1.6538579728721183, 'data_time': 0.0030007469594511703}
2025-11-07 11:59:03 Train Epoch 074:  76%|███████▌  | 1900/2502 [52:23<16:24,  1.64s/it, Loss=2.8892, Top1=N/A, LR=0.009558]2025-11-07 12:01:48,404 - INFO - Step 187048: {'train_loss_batch': 2.2789034843444824, 'train_lr': 0.009558195366224508, 'batch_time': 1.6537654145274396, 'data_time': 0.0028958685582465464}
2025-11-07 12:01:48 Train Epoch 074:  80%|███████▉  | 2000/2502 [55:07<13:44,  1.64s/it, Loss=2.8941, Top1=N/A, LR=0.009558]2025-11-07 12:04:31,658 - INFO - Step 187148: {'train_loss_batch': 3.185882091522217, 'train_lr': 0.009558195366224508, 'batch_time': 1.6527045231828208, 'data_time': 0.0028004975154482083}
2025-11-07 12:04:31 Train Epoch 074:  84%|████████▍ | 2100/2502 [57:52<11:03,  1.65s/it, Loss=2.8989, Top1=N/A, LR=0.009558]2025-11-07 12:07:16,733 - INFO - Step 187248: {'train_loss_batch': 3.8516740798950195, 'train_lr': 0.009558195366224508, 'batch_time': 1.6526114380058932, 'data_time': 0.002718908794717185}
2025-11-07 12:07:16 Train Epoch 074:  88%|████████▊ | 2200/2502 [1:00:36<08:18,  1.65s/it, Loss=2.8976, Top1=70.12%, LR=0.009558]2025-11-07 12:10:01,209 - INFO - Step 187348: {'train_loss_batch': 2.1135501861572266, 'train_lr': 0.009558195366224508, 'batch_time': 1.6522549713053307, 'data_time': 0.002645087534598143}
2025-11-07 12:10:01 Train Epoch 074:  92%|█████████▏| 2300/2502 [1:03:22<05:33,  1.65s/it, Loss=2.8973, Top1=N/A, LR=0.009558]   2025-11-07 12:12:46,797 - INFO - Step 187448: {'train_loss_batch': 3.188356399536133, 'train_lr': 0.009558195366224508, 'batch_time': 1.652412042054131, 'data_time': 0.0025741090364219932}
2025-11-07 12:12:46 Train Epoch 074:  96%|█████████▌| 2400/2502 [1:06:06<02:48,  1.66s/it, Loss=2.8958, Top1=N/A, LR=0.009558]2025-11-07 12:15:31,503 - INFO - Step 187548: {'train_loss_batch': 2.2259957790374756, 'train_lr': 0.009558195366224508, 'batch_time': 1.6521890455362749, 'data_time': 0.002509499926807383}
2025-11-07 12:15:31 Train Epoch 074: 100%|█████████▉| 2500/2502 [1:08:52<00:03,  1.65s/it, Loss=2.8956, Top1=N/A, LR=0.009558]2025-11-07 12:18:17,313 - INFO - Step 187648: {'train_loss_batch': 3.7416205406188965, 'train_lr': 0.009558195366224508, 'batch_time': 1.6524255917292507, 'data_time': 0.0024770213717796572}
2025-11-07 12:18:17 Train Epoch 074: 100%|██████████| 2502/2502 [1:08:54<00:00,  1.65s/it, Loss=2.8956, Top1=N/A, LR=0.009558]
2025-11-07 12:18:19 Val Epoch 074:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 12:18:24   with torch.cuda.amp.autocast():
2025-11-07 12:18:24 Val Epoch 074: 100%|██████████| 98/98 [01:52<00:00,  1.15s/it, Loss=1.9740, Top1=73.31%, Top5=91.78%]
2025-11-07 12:20:12 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 12:20:12   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 12:20:12 2025-11-07 12:20:12,484 - INFO - Step 74: {'epoch': 74, 'learning_rate': 0.008499949379376554, 'train_loss': 2.895384528082338, 'train_top1': 70.08896315028902, 'train_top5': 88.27635777938343, 'train_precision': 70.01112492392402, 'train_recall': 69.99116998442028, 'train_f1': 69.85670188343992, 'val_loss': 1.9740493872070313, 'val_top1': 73.31400001953125, 'val_top5': 91.77799998535156, 'val_precision': 73.96459329340456, 'val_recall': 73.312, 'val_f1': 72.89812043608384}
2025-11-07 12:20:12 2025-11-07 12:20:12,485 - INFO - Epoch 074 Summary - LR: 0.008500, Train Loss: 2.8954, Val Loss: 1.9740, Val F1: 72.90%, Val Precision: 73.96%, Val Recall: 73.31%
2025-11-07 12:20:15 2025-11-07 12:20:15,493 - INFO - New best model saved with validation accuracy: 73.314%
2025-11-07 12:20:15 2025-11-07 12:20:15,494 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_075.pth
2025-11-07 12:20:15 Train Epoch 075:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 74 that is less than the current step 187648. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 12:20:19 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 12:20:19   with torch.cuda.amp.autocast():
2025-11-07 12:20:20 Train Epoch 075:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.8517, Top1=N/A, LR=0.008500]2025-11-07 12:20:20,769 - INFO - Step 187650: {'train_loss_batch': 2.8517374992370605, 'train_lr': 0.008499949379376554, 'batch_time': 5.273927211761475, 'data_time': 3.607996702194214}
2025-11-07 12:20:20 Train Epoch 075:   4%|▍         | 100/2502 [02:51<1:06:35,  1.66s/it, Loss=2.7979, Top1=71.14%, LR=0.008500]2025-11-07 12:23:06,570 - INFO - Step 187750: {'train_loss_batch': 2.1819052696228027, 'train_lr': 0.008499949379376554, 'batch_time': 1.6938106211105195, 'data_time': 0.03677285307704812}
2025-11-07 12:23:06 Train Epoch 075:   8%|▊         | 200/2502 [05:37<1:03:45,  1.66s/it, Loss=2.8489, Top1=N/A, LR=0.008500]   2025-11-07 12:25:52,659 - INFO - Step 187850: {'train_loss_batch': 2.968308925628662, 'train_lr': 0.008499949379376554, 'batch_time': 1.677432964097208, 'data_time': 0.01897224146335279}
2025-11-07 12:25:52 Train Epoch 075:  12%|█▏        | 300/2502 [08:23<1:00:52,  1.66s/it, Loss=2.8671, Top1=70.78%, LR=0.008500]2025-11-07 12:28:38,612 - INFO - Step 187950: {'train_loss_batch': 2.1414003372192383, 'train_lr': 0.008499949379376554, 'batch_time': 1.6714838319442598, 'data_time': 0.01299856667502774}
2025-11-07 12:28:38 Train Epoch 075:  16%|█▌        | 400/2502 [11:09<58:04,  1.66s/it, Loss=2.8845, Top1=N/A, LR=0.008500]   2025-11-07 12:31:24,496 - INFO - Step 188050: {'train_loss_batch': 3.5943193435668945, 'train_lr': 0.008499949379376554, 'batch_time': 1.6683288732371722, 'data_time': 0.010020529540102381}
2025-11-07 12:31:24 Train Epoch 075:  20%|█▉        | 500/2502 [13:55<55:23,  1.66s/it, Loss=2.8759, Top1=70.74%, LR=0.008500]2025-11-07 12:34:10,555 - INFO - Step 188150: {'train_loss_batch': 2.114288806915283, 'train_lr': 0.008499949379376554, 'batch_time': 1.6667839267296705, 'data_time': 0.00822689290532095}
2025-11-07 12:34:10 Train Epoch 075:  24%|██▍       | 600/2502 [16:40<52:16,  1.65s/it, Loss=2.8639, Top1=70.85%, LR=0.008500]2025-11-07 12:36:56,445 - INFO - Step 188250: {'train_loss_batch': 2.191946268081665, 'train_lr': 0.008499949379376554, 'batch_time': 1.6654716458376155, 'data_time': 0.00702714959714258}
2025-11-07 12:36:56 Train Epoch 075:  28%|██▊       | 700/2502 [19:26<49:24,  1.65s/it, Loss=2.8944, Top1=N/A, LR=0.008500]   2025-11-07 12:39:41,974 - INFO - Step 188350: {'train_loss_batch': 3.5558793544769287, 'train_lr': 0.008499949379376554, 'batch_time': 1.6640185207851264, 'data_time': 0.006174873523467277}
2025-11-07 12:39:41 Train Epoch 075:  32%|███▏      | 800/2502 [22:11<46:49,  1.65s/it, Loss=2.9123, Top1=N/A, LR=0.008500]2025-11-07 12:42:26,916 - INFO - Step 188450: {'train_loss_batch': 3.301985263824463, 'train_lr': 0.008499949379376554, 'batch_time': 1.662196589468719, 'data_time': 0.005524546912546908}
2025-11-07 12:42:26 Train Epoch 075:  36%|███▌      | 900/2502 [24:56<44:10,  1.65s/it, Loss=2.9004, Top1=70.89%, LR=0.008500]2025-11-07 12:45:12,292 - INFO - Step 188550: {'train_loss_batch': 2.241365432739258, 'train_lr': 0.008499949379376554, 'batch_time': 1.6612595932332843, 'data_time': 0.005026665961702708}
2025-11-07 12:45:12 Train Epoch 075:  40%|███▉      | 1000/2502 [27:42<41:16,  1.65s/it, Loss=2.8992, Top1=N/A, LR=0.008500]   2025-11-07 12:47:57,739 - INFO - Step 188650: {'train_loss_batch': 3.333359718322754, 'train_lr': 0.008499949379376554, 'batch_time': 1.660581654959268, 'data_time': 0.004621994959843623}
2025-11-07 12:47:57 Train Epoch 075:  44%|████▍     | 1100/2502 [30:28<38:46,  1.66s/it, Loss=2.8906, Top1=N/A, LR=0.008500]2025-11-07 12:50:43,630 - INFO - Step 188750: {'train_loss_batch': 2.1054985523223877, 'train_lr': 0.008499949379376554, 'batch_time': 1.660429125585738, 'data_time': 0.004288906189228166}
2025-11-07 12:50:43 Train Epoch 075:  48%|████▊     | 1200/2502 [33:13<35:35,  1.64s/it, Loss=2.8901, Top1=N/A, LR=0.008500]2025-11-07 12:53:28,717 - INFO - Step 188850: {'train_loss_batch': 3.074690103530884, 'train_lr': 0.008499949379376554, 'batch_time': 1.659632884493279, 'data_time': 0.004014539877441305}
2025-11-07 12:53:28 Train Epoch 075:  52%|█████▏    | 1300/2502 [35:58<32:43,  1.63s/it, Loss=2.8849, Top1=N/A, LR=0.008500]2025-11-07 12:56:13,642 - INFO - Step 188950: {'train_loss_batch': 2.8046765327453613, 'train_lr': 0.008499949379376554, 'batch_time': 1.6588348354585898, 'data_time': 0.003782162384103574}
2025-11-07 12:56:13 Train Epoch 075:  56%|█████▌    | 1400/2502 [38:42<30:22,  1.65s/it, Loss=2.8771, Top1=N/A, LR=0.008500]2025-11-07 12:58:57,929 - INFO - Step 189050: {'train_loss_batch': 2.1223716735839844, 'train_lr': 0.008499949379376554, 'batch_time': 1.6576954678583793, 'data_time': 0.0035801216332424037}
2025-11-07 12:58:57 Train Epoch 075:  60%|█████▉    | 1500/2502 [41:28<27:36,  1.65s/it, Loss=2.8841, Top1=N/A, LR=0.008500]2025-11-07 13:01:43,680 - INFO - Step 189150: {'train_loss_batch': 2.670044422149658, 'train_lr': 0.008499949379376554, 'batch_time': 1.6576830268620968, 'data_time': 0.0034037368604137768}
2025-11-07 13:01:43 Train Epoch 075:  64%|██████▍   | 1600/2502 [44:13<24:38,  1.64s/it, Loss=2.8841, Top1=N/A, LR=0.008500]2025-11-07 13:04:29,292 - INFO - Step 189250: {'train_loss_batch': 3.608297824859619, 'train_lr': 0.008499949379376554, 'batch_time': 1.6575849363016084, 'data_time': 0.003255175025220367}
2025-11-07 13:04:29 Train Epoch 075:  68%|██████▊   | 1700/2502 [46:58<22:07,  1.66s/it, Loss=2.8831, Top1=70.64%, LR=0.008500]2025-11-07 13:07:14,379 - INFO - Step 189350: {'train_loss_batch': 2.1365208625793457, 'train_lr': 0.008499949379376554, 'batch_time': 1.6571898359470267, 'data_time': 0.0031200067776921635}
2025-11-07 13:07:14 Train Epoch 075:  72%|███████▏  | 1800/2502 [49:44<19:26,  1.66s/it, Loss=2.8808, Top1=N/A, LR=0.008500]   2025-11-07 13:09:59,580 - INFO - Step 189450: {'train_loss_batch': 2.085095167160034, 'train_lr': 0.008499949379376554, 'batch_time': 1.6569023677734849, 'data_time': 0.00299827857920357}
2025-11-07 13:09:59 Train Epoch 075:  76%|███████▌  | 1900/2502 [52:29<16:40,  1.66s/it, Loss=2.8853, Top1=70.59%, LR=0.008500]2025-11-07 13:12:45,071 - INFO - Step 189550: {'train_loss_batch': 2.0948712825775146, 'train_lr': 0.008499949379376554, 'batch_time': 1.6567972338494346, 'data_time': 0.002892271460262742}
2025-11-07 13:12:45 Train Epoch 075:  79%|███████▉  | 1978/2502 [54:36<14:29,  1.66s/it, Loss=2.8853, Top1=70.59%, LR=0.008500]
2025-11-07 13:15:30 Train Epoch 075:  84%|████████▍ | 2100/2502 [58:00<11:07,  1.66s/it, Loss=2.8867, Top1=N/A, LR=0.008500]2025-11-07 13:18:16,041 - INFO - Step 189750: {'train_loss_batch': 3.281464099884033, 'train_lr': 0.008499949379376554, 'batch_time': 1.6566120441160788, 'data_time': 0.0027094485361425155}
2025-11-07 13:18:16 Train Epoch 075:  88%|████████▊ | 2200/2502 [1:00:46<08:23,  1.67s/it, Loss=2.8790, Top1=N/A, LR=0.008500]2025-11-07 13:21:01,700 - INFO - Step 189850: {'train_loss_batch': 2.617784261703491, 'train_lr': 0.008499949379376554, 'batch_time': 1.6566105353621015, 'data_time': 0.002633263987879599}
2025-11-07 13:21:01 Train Epoch 075:  92%|█████████▏| 2300/2502 [1:03:31<05:33,  1.65s/it, Loss=2.8847, Top1=N/A, LR=0.008500]2025-11-07 13:23:47,386 - INFO - Step 189950: {'train_loss_batch': 4.466770648956299, 'train_lr': 0.008499949379376554, 'batch_time': 1.656621525361195, 'data_time': 0.002562190490616554}
2025-11-07 13:23:47 Train Epoch 075:  96%|█████████▌| 2400/2502 [1:06:17<02:49,  1.66s/it, Loss=2.8860, Top1=70.55%, LR=0.008500]2025-11-07 13:26:33,322 - INFO - Step 190050: {'train_loss_batch': 2.1591222286224365, 'train_lr': 0.008499949379376554, 'batch_time': 1.656735451208557, 'data_time': 0.0024973841718413936}
2025-11-07 13:26:33 Train Epoch 075: 100%|█████████▉| 2500/2502 [1:09:03<00:03,  1.65s/it, Loss=2.8839, Top1=70.51%, LR=0.008500]2025-11-07 13:29:19,035 - INFO - Step 190150: {'train_loss_batch': 2.20828914642334, 'train_lr': 0.008499949379376554, 'batch_time': 1.6567511947476259, 'data_time': 0.0024614967092996785}
2025-11-07 13:29:19 Train Epoch 075: 100%|██████████| 2502/2502 [1:09:05<00:00,  1.66s/it, Loss=2.8839, Top1=70.51%, LR=0.008500]
2025-11-07 13:29:21 Val Epoch 075:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 13:29:25   with torch.cuda.amp.autocast():
2025-11-07 13:29:26 Val Epoch 075: 100%|██████████| 98/98 [01:50<00:00,  1.12s/it, Loss=1.9738, Top1=73.61%, Top5=91.94%]
2025-11-07 13:31:11 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 13:31:11   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 13:31:11 2025-11-07 13:31:11,440 - INFO - Step 75: {'epoch': 75, 'learning_rate': 0.007498394299197949, 'train_loss': 2.8844945277336786, 'train_top1': 70.50947120488323, 'train_top5': 88.64492104564756, 'train_precision': 70.42816579016971, 'train_recall': 70.42130460218506, 'train_f1': 70.28349455801123, 'val_loss': 1.9737930715560914, 'val_top1': 73.61000000976563, 'val_top5': 91.93800000976563, 'val_precision': 74.29649754509535, 'val_recall': 73.61200000000001, 'val_f1': 73.20711510656218}
2025-11-07 13:31:11 2025-11-07 13:31:11,442 - INFO - Epoch 075 Summary - LR: 0.007498, Train Loss: 2.8845, Val Loss: 1.9738, Val F1: 73.21%, Val Precision: 74.30%, Val Recall: 73.61%
2025-11-07 13:31:14 2025-11-07 13:31:14,959 - INFO - New best model saved with validation accuracy: 73.610%
2025-11-07 13:31:14 2025-11-07 13:31:14,959 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_076.pth
2025-11-07 13:31:14 Train Epoch 076:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 75 that is less than the current step 190150. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 13:31:19 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 13:31:19   with torch.cuda.amp.autocast():
2025-11-07 13:31:20 Train Epoch 076:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.1522, Top1=N/A, LR=0.007498]2025-11-07 13:31:20,642 - INFO - Step 190152: {'train_loss_batch': 2.1521732807159424, 'train_lr': 0.007498394299197949, 'batch_time': 5.68014669418335, 'data_time': 4.035490036010742}
2025-11-07 13:31:20 Train Epoch 076:   4%|▍         | 100/2502 [02:51<1:06:20,  1.66s/it, Loss=2.9462, Top1=N/A, LR=0.007498]2025-11-07 13:34:06,079 - INFO - Step 190252: {'train_loss_batch': 3.5702171325683594, 'train_lr': 0.007498394299197949, 'batch_time': 1.6942427205567312, 'data_time': 0.04096463647219214}
2025-11-07 13:34:06 Train Epoch 076:   8%|▊         | 200/2502 [05:36<1:02:55,  1.64s/it, Loss=2.8857, Top1=N/A, LR=0.007498]2025-11-07 13:36:51,561 - INFO - Step 190352: {'train_loss_batch': 2.13489031791687, 'train_lr': 0.007498394299197949, 'batch_time': 1.6746289077682874, 'data_time': 0.021093453933943563}
2025-11-07 13:36:51 Train Epoch 076:  12%|█▏        | 300/2502 [08:21<1:00:49,  1.66s/it, Loss=2.9005, Top1=N/A, LR=0.007498]2025-11-07 13:39:36,297 - INFO - Step 190452: {'train_loss_batch': 2.838916778564453, 'train_lr': 0.007498394299197949, 'batch_time': 1.6655684071917867, 'data_time': 0.014408229593422722}
2025-11-07 13:39:36 Train Epoch 076:  16%|█▌        | 400/2502 [11:06<58:06,  1.66s/it, Loss=2.8724, Top1=N/A, LR=0.007498]2025-11-07 13:42:21,747 - INFO - Step 190552: {'train_loss_batch': 2.2824370861053467, 'train_lr': 0.007498394299197949, 'batch_time': 1.6628087547949126, 'data_time': 0.011071652842876025}
2025-11-07 13:42:21 Train Epoch 076:  20%|█▉        | 500/2502 [13:52<55:12,  1.65s/it, Loss=2.9009, Top1=N/A, LR=0.007498]2025-11-07 13:45:07,261 - INFO - Step 190652: {'train_loss_batch': 3.1568968296051025, 'train_lr': 0.007498394299197949, 'batch_time': 1.6612752445205718, 'data_time': 0.009062332545449872}
2025-11-07 13:45:07 Train Epoch 076:  24%|██▍       | 600/2502 [16:37<52:37,  1.66s/it, Loss=2.9078, Top1=71.28%, LR=0.007498]2025-11-07 13:47:52,289 - INFO - Step 190752: {'train_loss_batch': 2.174652576446533, 'train_lr': 0.007498394299197949, 'batch_time': 1.659446397755983, 'data_time': 0.007727660275933746}
2025-11-07 13:47:52 Train Epoch 076:  28%|██▊       | 700/2502 [19:23<49:37,  1.65s/it, Loss=2.8995, Top1=N/A, LR=0.007498]   2025-11-07 13:50:38,157 - INFO - Step 190852: {'train_loss_batch': 2.2201521396636963, 'train_lr': 0.007498394299197949, 'batch_time': 1.6593376103889586, 'data_time': 0.0067797924754623}
2025-11-07 13:50:38 Train Epoch 076:  32%|███▏      | 800/2502 [22:07<46:16,  1.63s/it, Loss=2.8879, Top1=N/A, LR=0.007498]2025-11-07 13:53:22,184 - INFO - Step 190952: {'train_loss_batch': 2.209228038787842, 'train_lr': 0.007498394299197949, 'batch_time': 1.6569558285297674, 'data_time': 0.006061119682035792}
2025-11-07 13:53:22 Train Epoch 076:  36%|███▌      | 900/2502 [24:51<43:30,  1.63s/it, Loss=2.8756, Top1=N/A, LR=0.007498]2025-11-07 13:56:06,678 - INFO - Step 191052: {'train_loss_batch': 2.225520133972168, 'train_lr': 0.007498394299197949, 'batch_time': 1.655622335967425, 'data_time': 0.005498968933054663}
2025-11-07 13:56:06 Train Epoch 076:  40%|███▉      | 1000/2502 [27:36<41:38,  1.66s/it, Loss=2.8781, Top1=N/A, LR=0.007498]2025-11-07 13:58:51,344 - INFO - Step 191152: {'train_loss_batch': 3.6554160118103027, 'train_lr': 0.007498394299197949, 'batch_time': 1.6547268668373862, 'data_time': 0.0050516328611573975}
2025-11-07 13:58:51 Train Epoch 076:  44%|████▍     | 1100/2502 [30:22<38:43,  1.66s/it, Loss=2.8767, Top1=N/A, LR=0.007498]2025-11-07 14:01:37,281 - INFO - Step 191252: {'train_loss_batch': 2.2939891815185547, 'train_lr': 0.007498394299197949, 'batch_time': 1.6551478065002192, 'data_time': 0.004694419379238645}
2025-11-07 14:01:37 Train Epoch 076:  48%|████▊     | 1200/2502 [33:08<35:52,  1.65s/it, Loss=2.8715, Top1=71.26%, LR=0.007498]2025-11-07 14:04:23,079 - INFO - Step 191352: {'train_loss_batch': 2.04270601272583, 'train_lr': 0.007498394299197949, 'batch_time': 1.6553839904680339, 'data_time': 0.004394821084409232}
2025-11-07 14:04:23 Train Epoch 076:  52%|█████▏    | 1300/2502 [35:53<32:59,  1.65s/it, Loss=2.8724, Top1=N/A, LR=0.007498]   2025-11-07 14:07:07,983 - INFO - Step 191452: {'train_loss_batch': 2.2524006366729736, 'train_lr': 0.007498394299197949, 'batch_time': 1.654895927794249, 'data_time': 0.004132754064174362}
2025-11-07 14:07:07 Train Epoch 076:  56%|█████▌    | 1400/2502 [38:38<30:29,  1.66s/it, Loss=2.8691, Top1=N/A, LR=0.007498]2025-11-07 14:09:53,097 - INFO - Step 191552: {'train_loss_batch': 3.143296718597412, 'train_lr': 0.007498394299197949, 'batch_time': 1.6546281946292527, 'data_time': 0.00391048928995289}
2025-11-07 14:09:53 Train Epoch 076:  60%|█████▉    | 1500/2502 [41:21<27:22,  1.64s/it, Loss=2.8627, Top1=N/A, LR=0.007498]2025-11-07 14:12:36,630 - INFO - Step 191652: {'train_loss_batch': 2.2052972316741943, 'train_lr': 0.007498394299197949, 'batch_time': 1.6533420487771742, 'data_time': 0.0037111681671955837}
2025-11-07 14:12:36 Train Epoch 076:  64%|██████▍   | 1600/2502 [44:07<24:42,  1.64s/it, Loss=2.8596, Top1=N/A, LR=0.007498]2025-11-07 14:15:22,165 - INFO - Step 191752: {'train_loss_batch': 2.3037009239196777, 'train_lr': 0.007498394299197949, 'batch_time': 1.653467280651762, 'data_time': 0.0035452427229681737}
2025-11-07 14:15:22 Train Epoch 076:  68%|██████▊   | 1700/2502 [46:51<21:56,  1.64s/it, Loss=2.8588, Top1=N/A, LR=0.007498]2025-11-07 14:18:06,801 - INFO - Step 191852: {'train_loss_batch': 3.0965185165405273, 'train_lr': 0.007498394299197949, 'batch_time': 1.6530496338267384, 'data_time': 0.0033972032065955}
2025-11-07 14:18:06 Train Epoch 076:  72%|███████▏  | 1800/2502 [49:36<19:17,  1.65s/it, Loss=2.8556, Top1=N/A, LR=0.007498]2025-11-07 14:20:51,780 - INFO - Step 191952: {'train_loss_batch': 2.1621878147125244, 'train_lr': 0.007498394299197949, 'batch_time': 1.6528683226350809, 'data_time': 0.003264141506383579}
2025-11-07 14:20:51 Train Epoch 076:  76%|███████▌  | 1900/2502 [52:21<16:32,  1.65s/it, Loss=2.8513, Top1=N/A, LR=0.007498]2025-11-07 14:23:36,921 - INFO - Step 192052: {'train_loss_batch': 3.7654478549957275, 'train_lr': 0.007498394299197949, 'batch_time': 1.6527914178177032, 'data_time': 0.003147489456425586}
2025-11-07 14:23:36 Train Epoch 076:  80%|███████▉  | 2000/2502 [55:07<13:47,  1.65s/it, Loss=2.8531, Top1=N/A, LR=0.007498]2025-11-07 14:26:22,676 - INFO - Step 192152: {'train_loss_batch': 2.0976293087005615, 'train_lr': 0.007498394299197949, 'batch_time': 1.6530291314484893, 'data_time': 0.0030383658611673166}
2025-11-07 14:26:22 Train Epoch 076:  84%|████████▍ | 2100/2502 [57:52<11:06,  1.66s/it, Loss=2.8488, Top1=71.08%, LR=0.007498]2025-11-07 14:29:07,933 - INFO - Step 192252: {'train_loss_batch': 2.0276525020599365, 'train_lr': 0.007498394299197949, 'batch_time': 1.6530070388844784, 'data_time': 0.002939642752311957}
2025-11-07 14:29:07 Train Epoch 076:  88%|████████▊ | 2200/2502 [1:00:38<08:18,  1.65s/it, Loss=2.8498, Top1=N/A, LR=0.007498]   2025-11-07 14:31:53,640 - INFO - Step 192352: {'train_loss_batch': 2.328767776489258, 'train_lr': 0.007498394299197949, 'batch_time': 1.6531919055177862, 'data_time': 0.002855613631370229}
2025-11-07 14:31:53 Train Epoch 076:  92%|█████████▏| 2300/2502 [1:03:24<05:35,  1.66s/it, Loss=2.8553, Top1=N/A, LR=0.007498]2025-11-07 14:34:39,134 - INFO - Step 192452: {'train_loss_batch': 3.5782032012939453, 'train_lr': 0.007498394299197949, 'batch_time': 1.6532674961015486, 'data_time': 0.0027794535395271203}
2025-11-07 14:34:39 Train Epoch 076:  96%|█████████▌| 2400/2502 [1:06:09<02:48,  1.65s/it, Loss=2.8476, Top1=71.02%, LR=0.007498]2025-11-07 14:37:24,535 - INFO - Step 192552: {'train_loss_batch': 2.118882656097412, 'train_lr': 0.007498394299197949, 'batch_time': 1.6532983039131466, 'data_time': 0.002703390931745511}
2025-11-07 14:37:24 Train Epoch 076: 100%|█████████▉| 2500/2502 [1:08:55<00:03,  1.66s/it, Loss=2.8488, Top1=N/A, LR=0.007498]   2025-11-07 14:40:10,124 - INFO - Step 192652: {'train_loss_batch': 2.0215840339660645, 'train_lr': 0.007498394299197949, 'batch_time': 1.653402183209358, 'data_time': 0.0026694148695502267}
2025-11-07 14:40:10 Train Epoch 076: 100%|██████████| 2502/2502 [1:08:56<00:00,  1.65s/it, Loss=2.8488, Top1=N/A, LR=0.007498]
2025-11-07 14:40:12 Val Epoch 076:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 14:40:16   with torch.cuda.amp.autocast():
2025-11-07 14:40:17 Val Epoch 076: 100%|██████████| 98/98 [01:51<00:00,  1.13s/it, Loss=1.9722, Top1=73.64%, Top5=91.93%]
2025-11-07 14:42:03 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 14:42:03   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 14:42:03 2025-11-07 14:42:03,626 - INFO - Step 76: {'epoch': 76, 'learning_rate': 0.00655489812945511, 'train_loss': 2.8485417133993765, 'train_top1': 71.01171875, 'train_top5': 88.88125, 'train_precision': 70.8966428335264, 'train_recall': 70.88515639094535, 'train_f1': 70.74707789293029, 'val_loss': 1.9721722856521606, 'val_top1': 73.64199999023438, 'val_top5': 91.92799998535156, 'val_precision': 74.39253709796357, 'val_recall': 73.644, 'val_f1': 73.24636656300738}
2025-11-07 14:42:03 2025-11-07 14:42:03,628 - INFO - Epoch 076 Summary - LR: 0.006555, Train Loss: 2.8485, Val Loss: 1.9722, Val F1: 73.25%, Val Precision: 74.39%, Val Recall: 73.64%
2025-11-07 14:42:05 wandb: WARNING Tried to log to step 76 that is less than the current step 192652. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 14:42:07 2025-11-07 14:42:07,280 - INFO - New best model saved with validation accuracy: 73.642%
2025-11-07 14:42:07 2025-11-07 14:42:07,281 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_077.pth
2025-11-07 14:42:07 Train Epoch 077:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 14:42:10   with torch.cuda.amp.autocast():
2025-11-07 14:42:12 Train Epoch 077:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.1787, Top1=71.68%, LR=0.006555]2025-11-07 14:42:12,625 - INFO - Step 192654: {'train_loss_batch': 2.1787166595458984, 'train_lr': 0.00655489812945511, 'batch_time': 5.342656373977661, 'data_time': 3.702039957046509}
2025-11-07 14:42:12 Train Epoch 077:   4%|▍         | 100/2502 [02:50<1:06:35,  1.66s/it, Loss=2.8809, Top1=N/A, LR=0.006555]   2025-11-07 14:44:58,061 - INFO - Step 192754: {'train_loss_batch': 2.730834722518921, 'train_lr': 0.00655489812945511, 'batch_time': 1.6908710286168769, 'data_time': 0.037683569558776255}
2025-11-07 14:44:58 Train Epoch 077:   8%|▊         | 200/2502 [05:36<1:03:20,  1.65s/it, Loss=2.8981, Top1=N/A, LR=0.006555]2025-11-07 14:47:43,971 - INFO - Step 192854: {'train_loss_batch': 4.586666107177734, 'train_lr': 0.00655489812945511, 'batch_time': 1.675064024047472, 'data_time': 0.019458820570760697}
2025-11-07 14:47:43 Train Epoch 077:  12%|█▏        | 300/2502 [08:21<1:00:41,  1.65s/it, Loss=2.8932, Top1=N/A, LR=0.006555]2025-11-07 14:50:28,864 - INFO - Step 192954: {'train_loss_batch': 3.807145595550537, 'train_lr': 0.00655489812945511, 'batch_time': 1.6663839943781247, 'data_time': 0.013362414813120897}
2025-11-07 14:50:28 Train Epoch 077:  16%|█▌        | 400/2502 [11:06<57:17,  1.64s/it, Loss=2.8822, Top1=N/A, LR=0.006555]2025-11-07 14:53:13,363 - INFO - Step 193054: {'train_loss_batch': 3.4481546878814697, 'train_lr': 0.00655489812945511, 'batch_time': 1.6610462832034674, 'data_time': 0.010274187287784871}
2025-11-07 14:53:13 Train Epoch 077:  20%|█▉        | 500/2502 [13:50<55:11,  1.65s/it, Loss=2.8380, Top1=N/A, LR=0.006555]2025-11-07 14:55:58,069 - INFO - Step 193154: {'train_loss_batch': 3.4882376194000244, 'train_lr': 0.00655489812945511, 'batch_time': 1.658254582962828, 'data_time': 0.008426556806126516}
2025-11-07 14:55:58 Train Epoch 077:  24%|██▍       | 600/2502 [16:36<52:25,  1.65s/it, Loss=2.8561, Top1=N/A, LR=0.006555]2025-11-07 14:58:43,607 - INFO - Step 193254: {'train_loss_batch': 2.5184168815612793, 'train_lr': 0.00655489812945511, 'batch_time': 1.6577758796996562, 'data_time': 0.007202160338593799}
2025-11-07 14:58:43 Train Epoch 077:  28%|██▊       | 700/2502 [19:21<49:55,  1.66s/it, Loss=2.8441, Top1=N/A, LR=0.006555]2025-11-07 15:01:29,016 - INFO - Step 193354: {'train_loss_batch': 2.292794942855835, 'train_lr': 0.00655489812945511, 'batch_time': 1.6572503387162756, 'data_time': 0.006321359803095014}
2025-11-07 15:01:29 Train Epoch 077:  32%|███▏      | 800/2502 [22:06<46:44,  1.65s/it, Loss=2.8473, Top1=N/A, LR=0.006555]2025-11-07 15:04:14,076 - INFO - Step 193454: {'train_loss_batch': 2.161212921142578, 'train_lr': 0.00655489812945511, 'batch_time': 1.6564198995797375, 'data_time': 0.0056581785914007945}
2025-11-07 15:04:14 Train Epoch 077:  36%|███▌      | 900/2502 [24:52<43:52,  1.64s/it, Loss=2.8415, Top1=71.52%, LR=0.006555]2025-11-07 15:06:59,380 - INFO - Step 193554: {'train_loss_batch': 2.090826988220215, 'train_lr': 0.00655489812945511, 'batch_time': 1.6560444281447873, 'data_time': 0.005136441178909285}
2025-11-07 15:06:59 Train Epoch 077:  40%|███▉      | 1000/2502 [27:37<41:41,  1.67s/it, Loss=2.8422, Top1=N/A, LR=0.006555]   2025-11-07 15:09:44,427 - INFO - Step 193654: {'train_loss_batch': 3.688204288482666, 'train_lr': 0.00655489812945511, 'batch_time': 1.6554872315604012, 'data_time': 0.0047202272253198466}
2025-11-07 15:09:44 Train Epoch 077:  44%|████▍     | 1100/2502 [30:22<38:11,  1.63s/it, Loss=2.8339, Top1=N/A, LR=0.006555]2025-11-07 15:12:29,733 - INFO - Step 193754: {'train_loss_batch': 2.2264223098754883, 'train_lr': 0.00655489812945511, 'batch_time': 1.6552662396842408, 'data_time': 0.004381907191956509}
2025-11-07 15:12:29 Train Epoch 077:  48%|████▊     | 1200/2502 [33:07<35:52,  1.65s/it, Loss=2.8197, Top1=N/A, LR=0.006555]2025-11-07 15:15:14,563 - INFO - Step 193854: {'train_loss_batch': 3.4279274940490723, 'train_lr': 0.00655489812945511, 'batch_time': 1.6546861174501646, 'data_time': 0.004107054028284738}
2025-11-07 15:15:14 Train Epoch 077:  52%|█████▏    | 1300/2502 [35:52<33:02,  1.65s/it, Loss=2.8211, Top1=N/A, LR=0.006555]2025-11-07 15:17:59,541 - INFO - Step 193954: {'train_loss_batch': 3.7114734649658203, 'train_lr': 0.00655489812945511, 'batch_time': 1.6543088422932137, 'data_time': 0.0038708737041288663}
2025-11-07 15:17:59 Train Epoch 077:  56%|█████▌    | 1400/2502 [38:38<30:24,  1.66s/it, Loss=2.8274, Top1=N/A, LR=0.006555]2025-11-07 15:20:45,610 - INFO - Step 194054: {'train_loss_batch': 2.1259498596191406, 'train_lr': 0.00655489812945511, 'batch_time': 1.654764757421849, 'data_time': 0.003666588274774}
2025-11-07 15:20:45 Train Epoch 077:  60%|█████▉    | 1500/2502 [41:24<27:38,  1.66s/it, Loss=2.8330, Top1=N/A, LR=0.006555]2025-11-07 15:23:31,616 - INFO - Step 194154: {'train_loss_batch': 3.100292682647705, 'train_lr': 0.00655489812945511, 'batch_time': 1.6551170898706893, 'data_time': 0.003494008233910954}
2025-11-07 15:23:31 Train Epoch 077:  64%|██████▍   | 1600/2502 [44:10<24:55,  1.66s/it, Loss=2.8337, Top1=N/A, LR=0.006555]2025-11-07 15:26:17,577 - INFO - Step 194254: {'train_loss_batch': 2.083522081375122, 'train_lr': 0.00655489812945511, 'batch_time': 1.65539762424276, 'data_time': 0.0033422880512263757}
2025-11-07 15:26:17 Train Epoch 077:  68%|██████▊   | 1700/2502 [46:55<21:55,  1.64s/it, Loss=2.8294, Top1=N/A, LR=0.006555]2025-11-07 15:29:02,752 - INFO - Step 194354: {'train_loss_batch': 2.251521587371826, 'train_lr': 0.00655489812945511, 'batch_time': 1.6551832917295857, 'data_time': 0.0032045186652217734}
2025-11-07 15:29:02 Train Epoch 077:  72%|███████▏  | 1800/2502 [49:40<19:26,  1.66s/it, Loss=2.8307, Top1=N/A, LR=0.006555]2025-11-07 15:31:48,094 - INFO - Step 194454: {'train_loss_batch': 2.0429956912994385, 'train_lr': 0.00655489812945511, 'batch_time': 1.6550849654289301, 'data_time': 0.003092538509548935}
2025-11-07 15:31:48 Train Epoch 077:  76%|███████▌  | 1900/2502 [52:26<16:39,  1.66s/it, Loss=2.8285, Top1=N/A, LR=0.006555]2025-11-07 15:34:34,269 - INFO - Step 194554: {'train_loss_batch': 3.628933906555176, 'train_lr': 0.00655489812945511, 'batch_time': 1.6554355663979825, 'data_time': 0.002983223194200575}
2025-11-07 15:34:34 Train Epoch 077:  80%|███████▉  | 2000/2502 [55:12<13:54,  1.66s/it, Loss=2.8266, Top1=N/A, LR=0.006555]2025-11-07 15:37:20,182 - INFO - Step 194654: {'train_loss_batch': 2.5535335540771484, 'train_lr': 0.00655489812945511, 'batch_time': 1.6556199534662601, 'data_time': 0.0028841799345688483}
2025-11-07 15:37:20 Train Epoch 077:  84%|████████▍ | 2100/2502 [57:58<11:07,  1.66s/it, Loss=2.8263, Top1=N/A, LR=0.006555]2025-11-07 15:40:05,880 - INFO - Step 194754: {'train_loss_batch': 3.2603261470794678, 'train_lr': 0.00655489812945511, 'batch_time': 1.6556844663642691, 'data_time': 0.002795864887773168}
2025-11-07 15:40:05 Train Epoch 077:  88%|████████▊ | 2200/2502 [1:00:42<08:21,  1.66s/it, Loss=2.8234, Top1=71.33%, LR=0.006555]2025-11-07 15:42:50,268 - INFO - Step 194854: {'train_loss_batch': 2.2222511768341064, 'train_lr': 0.00655489812945511, 'batch_time': 1.6551482802030988, 'data_time': 0.002713094132426434}
2025-11-07 15:42:50 Train Epoch 077:  92%|█████████▏| 2300/2502 [1:03:28<05:31,  1.64s/it, Loss=2.8244, Top1=N/A, LR=0.006555]   2025-11-07 15:45:35,439 - INFO - Step 194954: {'train_loss_batch': 2.184635639190674, 'train_lr': 0.00655489812945511, 'batch_time': 1.6549987766028798, 'data_time': 0.0026368106359608637}
2025-11-07 15:45:35 Train Epoch 077:  96%|█████████▌| 2400/2502 [1:06:13<02:48,  1.65s/it, Loss=2.8273, Top1=N/A, LR=0.006555]2025-11-07 15:48:21,168 - INFO - Step 195054: {'train_loss_batch': 2.8681416511535645, 'train_lr': 0.00655489812945511, 'batch_time': 1.6550939734505397, 'data_time': 0.002568109092092772}
2025-11-07 15:48:21 Train Epoch 077: 100%|█████████▉| 2500/2502 [1:08:58<00:03,  1.64s/it, Loss=2.8259, Top1=N/A, LR=0.006555]2025-11-07 15:51:06,062 - INFO - Step 195154: {'train_loss_batch': 3.8085384368896484, 'train_lr': 0.00655489812945511, 'batch_time': 1.65484804737239, 'data_time': 0.0025277280750297536}
2025-11-07 15:51:06 Train Epoch 077: 100%|██████████| 2502/2502 [1:09:00<00:00,  1.65s/it, Loss=2.8259, Top1=N/A, LR=0.006555]
2025-11-07 15:51:08 Val Epoch 077:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 15:51:12   with torch.cuda.amp.autocast():
2025-11-07 15:51:13 Val Epoch 077: 100%|██████████| 98/98 [01:51<00:00,  1.14s/it, Loss=1.9608, Top1=73.91%, Top5=92.11%]
2025-11-07 15:53:00 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 15:53:00   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 15:53:00 2025-11-07 15:53:00,190 - INFO - Step 77: {'epoch': 77, 'learning_rate': 0.005670749572426506, 'train_loss': 2.826147673751334, 'train_top1': 71.28633901892431, 'train_top5': 89.05432955677291, 'train_precision': 71.18681924634271, 'train_recall': 71.18652416960035, 'train_f1': 71.03954561832904, 'val_loss': 1.9608441180038452, 'val_top1': 73.90999997070313, 'val_top5': 92.10999998046876, 'val_precision': 74.73934254845844, 'val_recall': 73.904, 'val_f1': 73.5770239625008}
2025-11-07 15:53:00 2025-11-07 15:53:00,192 - INFO - Epoch 077 Summary - LR: 0.005671, Train Loss: 2.8261, Val Loss: 1.9608, Val F1: 73.58%, Val Precision: 74.74%, Val Recall: 73.90%
2025-11-07 15:53:03 2025-11-07 15:53:03,413 - INFO - New best model saved with validation accuracy: 73.910%
2025-11-07 15:53:03 2025-11-07 15:53:03,414 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_078.pth
2025-11-07 15:53:03 Train Epoch 078:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 77 that is less than the current step 195154. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 15:53:07 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 15:53:07   with torch.cuda.amp.autocast():
2025-11-07 15:53:08 Train Epoch 078:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.1057, Top1=72.46%, LR=0.005671]2025-11-07 15:53:08,830 - INFO - Step 195156: {'train_loss_batch': 2.1056554317474365, 'train_lr': 0.005670749572426506, 'batch_time': 5.41494607925415, 'data_time': 3.7791693210601807}
2025-11-07 15:53:08 Train Epoch 078:   4%|▍         | 100/2502 [02:51<1:06:25,  1.66s/it, Loss=2.8057, Top1=N/A, LR=0.005671]   2025-11-07 15:55:54,521 - INFO - Step 195256: {'train_loss_batch': 2.3207192420959473, 'train_lr': 0.005670749572426506, 'batch_time': 1.694115423920131, 'data_time': 0.03839733577010655}
2025-11-07 15:55:54 Train Epoch 078:   8%|▊         | 200/2502 [05:36<1:03:08,  1.65s/it, Loss=2.8235, Top1=N/A, LR=0.005671]2025-11-07 15:58:39,768 - INFO - Step 195356: {'train_loss_batch': 3.9059066772460938, 'train_lr': 0.005670749572426506, 'batch_time': 1.6733946527414654, 'data_time': 0.01978453000386556}
2025-11-07 15:58:39 Train Epoch 078:  12%|█▏        | 300/2502 [08:20<59:52,  1.63s/it, Loss=2.8196, Top1=N/A, LR=0.005671]2025-11-07 16:01:24,406 - INFO - Step 195456: {'train_loss_batch': 2.1098875999450684, 'train_lr': 0.005670749572426506, 'batch_time': 1.664419328651555, 'data_time': 0.013554766328627881}
2025-11-07 16:01:24 Train Epoch 078:  16%|█▌        | 400/2502 [11:05<58:06,  1.66s/it, Loss=2.8253, Top1=72.07%, LR=0.005671]2025-11-07 16:04:08,821 - INFO - Step 195556: {'train_loss_batch': 2.1452715396881104, 'train_lr': 0.005670749572426506, 'batch_time': 1.6593637020510628, 'data_time': 0.010424619898237194}
2025-11-07 16:04:08 Train Epoch 078:  20%|█▉        | 500/2502 [13:50<55:04,  1.65s/it, Loss=2.8445, Top1=N/A, LR=0.005671]   2025-11-07 16:06:54,291 - INFO - Step 195656: {'train_loss_batch': 4.587864875793457, 'train_lr': 0.005670749572426506, 'batch_time': 1.6584328829409358, 'data_time': 0.008546851590246022}
2025-11-07 16:06:54 Train Epoch 078:  24%|██▍       | 600/2502 [16:36<52:38,  1.66s/it, Loss=2.8584, Top1=N/A, LR=0.005671]2025-11-07 16:09:39,729 - INFO - Step 195756: {'train_loss_batch': 4.485555648803711, 'train_lr': 0.005670749572426506, 'batch_time': 1.6577579614128328, 'data_time': 0.007286989351675634}
2025-11-07 16:09:39 Train Epoch 078:  28%|██▊       | 700/2502 [19:21<49:36,  1.65s/it, Loss=2.8565, Top1=72.06%, LR=0.005671]2025-11-07 16:12:25,207 - INFO - Step 195856: {'train_loss_batch': 2.071485996246338, 'train_lr': 0.005670749572426506, 'batch_time': 1.6573332206328824, 'data_time': 0.006390792667100501}
2025-11-07 16:12:25 Train Epoch 078:  32%|███▏      | 800/2502 [22:07<46:43,  1.65s/it, Loss=2.8418, Top1=72.06%, LR=0.005671]2025-11-07 16:15:10,529 - INFO - Step 195956: {'train_loss_batch': 2.1867518424987793, 'train_lr': 0.005670749572426506, 'batch_time': 1.656819545672032, 'data_time': 0.005723394555843129}
2025-11-07 16:15:10 Train Epoch 078:  36%|███▌      | 900/2502 [24:52<44:19,  1.66s/it, Loss=2.8418, Top1=N/A, LR=0.005671]   2025-11-07 16:17:55,942 - INFO - Step 196056: {'train_loss_batch': 2.263115406036377, 'train_lr': 0.005670749572426506, 'batch_time': 1.6565208210135407, 'data_time': 0.0051975745075153856}
2025-11-07 16:17:55 Train Epoch 078:  40%|███▉      | 1000/2502 [27:37<41:36,  1.66s/it, Loss=2.8456, Top1=N/A, LR=0.005671]2025-11-07 16:20:41,215 - INFO - Step 196156: {'train_loss_batch': 4.424904823303223, 'train_lr': 0.005670749572426506, 'batch_time': 1.6561416900836743, 'data_time': 0.00477616817920239}
2025-11-07 16:20:41 Train Epoch 078:  44%|████▍     | 1100/2502 [30:23<38:45,  1.66s/it, Loss=2.8290, Top1=N/A, LR=0.005671]2025-11-07 16:23:26,485 - INFO - Step 196256: {'train_loss_batch': 1.9899919033050537, 'train_lr': 0.005670749572426506, 'batch_time': 1.6558288958373664, 'data_time': 0.004427453802456973}
2025-11-07 16:23:26 Train Epoch 078:  48%|████▊     | 1200/2502 [33:08<36:00,  1.66s/it, Loss=2.8241, Top1=N/A, LR=0.005671]2025-11-07 16:26:12,150 - INFO - Step 196356: {'train_loss_batch': 3.174640655517578, 'train_lr': 0.005670749572426506, 'batch_time': 1.655896926501907, 'data_time': 0.004145849753577544}
2025-11-07 16:26:12 Train Epoch 078:  52%|█████▏    | 1300/2502 [35:54<33:15,  1.66s/it, Loss=2.8266, Top1=N/A, LR=0.005671]2025-11-07 16:28:57,433 - INFO - Step 196456: {'train_loss_batch': 3.4131040573120117, 'train_lr': 0.005670749572426506, 'batch_time': 1.6556614784897519, 'data_time': 0.003904840379930477}
2025-11-07 16:28:57 Train Epoch 078:  56%|█████▌    | 1400/2502 [38:39<30:08,  1.64s/it, Loss=2.8241, Top1=N/A, LR=0.005671]2025-11-07 16:31:42,537 - INFO - Step 196556: {'train_loss_batch': 2.275237798690796, 'train_lr': 0.005670749572426506, 'batch_time': 1.655331246602714, 'data_time': 0.0036988853982820587}
2025-11-07 16:31:42 Train Epoch 078:  60%|█████▉    | 1500/2502 [41:24<27:32,  1.65s/it, Loss=2.8243, Top1=N/A, LR=0.005671]2025-11-07 16:34:27,462 - INFO - Step 196656: {'train_loss_batch': 3.1345176696777344, 'train_lr': 0.005670749572426506, 'batch_time': 1.6549259663263534, 'data_time': 0.003518746266120438}
2025-11-07 16:34:27 Train Epoch 078:  64%|██████▍   | 1600/2502 [44:10<24:53,  1.66s/it, Loss=2.8210, Top1=N/A, LR=0.005671]2025-11-07 16:37:13,485 - INFO - Step 196756: {'train_loss_batch': 3.7573704719543457, 'train_lr': 0.005670749572426506, 'batch_time': 1.6552570175930978, 'data_time': 0.003361337114914293}
2025-11-07 16:37:13 Train Epoch 078:  68%|██████▊   | 1700/2502 [46:55<21:59,  1.65s/it, Loss=2.8188, Top1=N/A, LR=0.005671]2025-11-07 16:39:58,427 - INFO - Step 196856: {'train_loss_batch': 2.2115724086761475, 'train_lr': 0.005670749572426506, 'batch_time': 1.654913845796714, 'data_time': 0.0032236500391884457}
2025-11-07 16:39:58 Train Epoch 078:  72%|███████▏  | 1800/2502 [49:40<19:26,  1.66s/it, Loss=2.8142, Top1=N/A, LR=0.005671]2025-11-07 16:42:43,956 - INFO - Step 196956: {'train_loss_batch': 2.1661362648010254, 'train_lr': 0.005670749572426506, 'batch_time': 1.654934543692225, 'data_time': 0.003097256841029411}
2025-11-07 16:42:43 Train Epoch 078:  76%|███████▌  | 1900/2502 [52:25<16:33,  1.65s/it, Loss=2.8106, Top1=N/A, LR=0.005671]2025-11-07 16:45:28,613 - INFO - Step 197056: {'train_loss_batch': 4.333009719848633, 'train_lr': 0.005670749572426506, 'batch_time': 1.6544946578224982, 'data_time': 0.0029910058488600007}
2025-11-07 16:45:28 Train Epoch 078:  80%|███████▉  | 2000/2502 [55:10<13:44,  1.64s/it, Loss=2.8136, Top1=N/A, LR=0.005671]2025-11-07 16:48:13,925 - INFO - Step 197156: {'train_loss_batch': 2.114145278930664, 'train_lr': 0.005670749572426506, 'batch_time': 1.654425740420729, 'data_time': 0.002889214724912934}
2025-11-07 16:48:13 Train Epoch 078:  84%|████████▍ | 2100/2502 [57:55<11:04,  1.65s/it, Loss=2.8164, Top1=N/A, LR=0.005671]2025-11-07 16:50:59,085 - INFO - Step 197256: {'train_loss_batch': 3.52282977104187, 'train_lr': 0.005670749572426506, 'batch_time': 1.6542910356172091, 'data_time': 0.0028009420346782527}
2025-11-07 16:50:59 Train Epoch 078:  88%|████████▊ | 2200/2502 [1:00:41<08:21,  1.66s/it, Loss=2.8097, Top1=N/A, LR=0.005671]2025-11-07 16:53:44,926 - INFO - Step 197356: {'train_loss_batch': 2.131774425506592, 'train_lr': 0.005670749572426506, 'batch_time': 1.6544783780489223, 'data_time': 0.0027194703399782993}
2025-11-07 16:53:44 Train Epoch 078:  92%|█████████▏| 2300/2502 [1:03:26<05:33,  1.65s/it, Loss=2.8132, Top1=N/A, LR=0.005671]2025-11-07 16:56:30,190 - INFO - Step 197456: {'train_loss_batch': 4.363737106323242, 'train_lr': 0.005670749572426506, 'batch_time': 1.6543983511694718, 'data_time': 0.0026461469873248467}
2025-11-07 16:56:30 Train Epoch 078:  96%|█████████▌| 2400/2502 [1:06:12<02:49,  1.66s/it, Loss=2.8088, Top1=N/A, LR=0.005671]2025-11-07 16:59:15,983 - INFO - Step 197556: {'train_loss_batch': 2.4011547565460205, 'train_lr': 0.005670749572426506, 'batch_time': 1.654545042227825, 'data_time': 0.002578223759509384}
2025-11-07 16:59:15 Train Epoch 078: 100%|█████████▉| 2500/2502 [1:08:58<00:03,  1.65s/it, Loss=2.8051, Top1=N/A, LR=0.005671]2025-11-07 17:02:01,444 - INFO - Step 197656: {'train_loss_batch': 4.549720764160156, 'train_lr': 0.005670749572426506, 'batch_time': 1.6545478998303937, 'data_time': 0.0025473171975411496}
2025-11-07 17:02:01 Train Epoch 078: 100%|██████████| 2502/2502 [1:08:59<00:00,  1.65s/it, Loss=2.8051, Top1=N/A, LR=0.005671]
2025-11-07 17:02:03 Val Epoch 078:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 17:02:08   with torch.cuda.amp.autocast():
2025-11-07 17:02:08 Val Epoch 078: 100%|██████████| 98/98 [01:48<00:00,  1.11s/it, Loss=1.9364, Top1=74.50%, Top5=92.43%]
2025-11-07 17:03:52 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 17:03:52   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 17:03:52 2025-11-07 17:03:52,355 - INFO - Step 78: {'epoch': 78, 'learning_rate': 0.004847156268690288, 'train_loss': 2.8048984983460987, 'train_top1': 71.84916708416834, 'train_top5': 89.42964053106212, 'train_precision': 71.78834721691297, 'train_recall': 71.78379710626561, 'train_f1': 71.64341453289222, 'val_loss': 1.93642330368042, 'val_top1': 74.49799999267579, 'val_top5': 92.43399998291015, 'val_precision': 75.09732964327713, 'val_recall': 74.5, 'val_f1': 74.10972364370927}
2025-11-07 17:03:52 2025-11-07 17:03:52,357 - INFO - Epoch 078 Summary - LR: 0.004847, Train Loss: 2.8049, Val Loss: 1.9364, Val F1: 74.11%, Val Precision: 75.10%, Val Recall: 74.50%
2025-11-07 17:03:55 2025-11-07 17:03:55,825 - INFO - New best model saved with validation accuracy: 74.498%
2025-11-07 17:03:55 2025-11-07 17:03:55,825 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_079.pth
2025-11-07 17:03:55 Train Epoch 079:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 78 that is less than the current step 197656. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 17:03:59 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 17:03:59   with torch.cuda.amp.autocast():
2025-11-07 17:04:01 Train Epoch 079:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.0261, Top1=75.59%, LR=0.004847]2025-11-07 17:04:01,509 - INFO - Step 197658: {'train_loss_batch': 2.0261425971984863, 'train_lr': 0.004847156268690288, 'batch_time': 5.681061744689941, 'data_time': 4.04511570930481}
2025-11-07 17:04:01 Train Epoch 079:   4%|▍         | 100/2502 [02:51<1:06:28,  1.66s/it, Loss=2.8764, Top1=N/A, LR=0.004847]   2025-11-07 17:06:47,122 - INFO - Step 197758: {'train_loss_batch': 3.5244369506835938, 'train_lr': 0.004847156268690288, 'batch_time': 1.6959952151421274, 'data_time': 0.041085894745175204}
2025-11-07 17:06:47 Train Epoch 079:   8%|▊         | 200/2502 [05:36<1:03:11,  1.65s/it, Loss=2.8145, Top1=72.17%, LR=0.004847]2025-11-07 17:09:32,203 - INFO - Step 197858: {'train_loss_batch': 2.073040008544922, 'train_lr': 0.004847156268690288, 'batch_time': 1.6735111884216765, 'data_time': 0.021142794718196738}
2025-11-07 17:09:32 Train Epoch 079:  12%|█▏        | 300/2502 [08:21<1:00:20,  1.64s/it, Loss=2.7772, Top1=N/A, LR=0.004847]   2025-11-07 17:12:16,908 - INFO - Step 197958: {'train_loss_batch': 2.159532070159912, 'train_lr': 0.004847156268690288, 'batch_time': 1.664720745973809, 'data_time': 0.014445686657167353}
2025-11-07 17:12:16 Train Epoch 079:  16%|█▌        | 400/2502 [11:06<58:02,  1.66s/it, Loss=2.7869, Top1=N/A, LR=0.004847]2025-11-07 17:15:02,575 - INFO - Step 198058: {'train_loss_batch': 2.185018539428711, 'train_lr': 0.004847156268690288, 'batch_time': 1.6627121144102102, 'data_time': 0.011093205050042739}
2025-11-07 17:15:02 Train Epoch 079:  20%|█▉        | 500/2502 [13:51<55:29,  1.66s/it, Loss=2.7675, Top1=N/A, LR=0.004847]2025-11-07 17:17:47,802 - INFO - Step 198158: {'train_loss_batch': 2.058370590209961, 'train_lr': 0.004847156268690288, 'batch_time': 1.660626176350607, 'data_time': 0.009077810717675976}
2025-11-07 17:17:47 Train Epoch 079:  24%|██▍       | 600/2502 [16:37<52:16,  1.65s/it, Loss=2.7869, Top1=72.34%, LR=0.004847]2025-11-07 17:20:33,293 - INFO - Step 198258: {'train_loss_batch': 2.1974291801452637, 'train_lr': 0.004847156268690288, 'batch_time': 1.6596744302505264, 'data_time': 0.0077379031506632015}
2025-11-07 17:20:33 Train Epoch 079:  28%|██▊       | 700/2502 [19:22<49:06,  1.64s/it, Loss=2.8015, Top1=N/A, LR=0.004847]   2025-11-07 17:23:18,232 - INFO - Step 198358: {'train_loss_batch': 3.789811611175537, 'train_lr': 0.004847156268690288, 'batch_time': 1.658207726036431, 'data_time': 0.0067794999790599785}
2025-11-07 17:23:18 Train Epoch 079:  32%|███▏      | 800/2502 [22:06<47:04,  1.66s/it, Loss=2.8116, Top1=N/A, LR=0.004847]2025-11-07 17:26:02,465 - INFO - Step 198458: {'train_loss_batch': 2.1236824989318848, 'train_lr': 0.004847156268690288, 'batch_time': 1.6562255884377697, 'data_time': 0.0060554172811139095}
2025-11-07 17:26:02 Train Epoch 079:  36%|███▌      | 900/2502 [24:52<44:09,  1.65s/it, Loss=2.8229, Top1=N/A, LR=0.004847]2025-11-07 17:28:47,989 - INFO - Step 198558: {'train_loss_batch': 3.2497735023498535, 'train_lr': 0.004847156268690288, 'batch_time': 1.6561158484015428, 'data_time': 0.0054971512890814676}
2025-11-07 17:28:47 Train Epoch 079:  40%|███▉      | 1000/2502 [27:37<41:41,  1.67s/it, Loss=2.8318, Top1=N/A, LR=0.004847]2025-11-07 17:31:33,669 - INFO - Step 198658: {'train_loss_batch': 2.1170976161956787, 'train_lr': 0.004847156268690288, 'batch_time': 1.656184105725436, 'data_time': 0.005046279280335753}
2025-11-07 17:31:33 Train Epoch 079:  44%|████▍     | 1100/2502 [30:22<38:24,  1.64s/it, Loss=2.8127, Top1=72.26%, LR=0.004847]2025-11-07 17:34:18,801 - INFO - Step 198758: {'train_loss_batch': 2.0709686279296875, 'train_lr': 0.004847156268690288, 'batch_time': 1.655742471592736, 'data_time': 0.004677826442250763}
2025-11-07 17:34:18 Train Epoch 079:  48%|████▊     | 1200/2502 [33:08<36:01,  1.66s/it, Loss=2.8018, Top1=N/A, LR=0.004847]   2025-11-07 17:37:04,254 - INFO - Step 198858: {'train_loss_batch': 2.139029026031494, 'train_lr': 0.004847156268690288, 'batch_time': 1.6556411488268596, 'data_time': 0.004374620420152599}
2025-11-07 17:37:04 Train Epoch 079:  52%|█████▏    | 1300/2502 [35:54<33:12,  1.66s/it, Loss=2.8150, Top1=N/A, LR=0.004847]2025-11-07 17:39:50,236 - INFO - Step 198958: {'train_loss_batch': 2.999904155731201, 'train_lr': 0.004847156268690288, 'batch_time': 1.6559621417275032, 'data_time': 0.004116982336139606}
2025-11-07 17:39:50 Train Epoch 079:  56%|█████▌    | 1400/2502 [38:40<30:16,  1.65s/it, Loss=2.8104, Top1=N/A, LR=0.004847]2025-11-07 17:42:35,977 - INFO - Step 199058: {'train_loss_batch': 3.645559310913086, 'train_lr': 0.004847156268690288, 'batch_time': 1.6560652911195068, 'data_time': 0.003898829413175072}
2025-11-07 17:42:35 Train Epoch 079:  60%|█████▉    | 1500/2502 [41:25<27:39,  1.66s/it, Loss=2.7956, Top1=N/A, LR=0.004847]2025-11-07 17:45:21,156 - INFO - Step 199158: {'train_loss_batch': 2.5793685913085938, 'train_lr': 0.004847156268690288, 'batch_time': 1.6557804437417494, 'data_time': 0.003705247571514417}
2025-11-07 17:45:21 Train Epoch 079:  64%|██████▍   | 1600/2502 [44:10<24:54,  1.66s/it, Loss=2.7942, Top1=N/A, LR=0.004847]2025-11-07 17:48:06,439 - INFO - Step 199258: {'train_loss_batch': 3.0318081378936768, 'train_lr': 0.004847156268690288, 'batch_time': 1.655595486942937, 'data_time': 0.003535733529733614}
2025-11-07 17:48:06 Train Epoch 079:  68%|██████▊   | 1700/2502 [46:55<22:08,  1.66s/it, Loss=2.7911, Top1=N/A, LR=0.004847]2025-11-07 17:50:51,732 - INFO - Step 199358: {'train_loss_batch': 4.55938720703125, 'train_lr': 0.004847156268690288, 'batch_time': 1.6554385598164176, 'data_time': 0.003388106717844699}
2025-11-07 17:50:51 Train Epoch 079:  72%|███████▏  | 1800/2502 [49:41<19:15,  1.65s/it, Loss=2.7827, Top1=N/A, LR=0.004847]2025-11-07 17:53:36,976 - INFO - Step 199458: {'train_loss_batch': 2.166038751602173, 'train_lr': 0.004847156268690288, 'batch_time': 1.655272212047037, 'data_time': 0.00325714198170206}
2025-11-07 17:53:36 Train Epoch 079:  76%|███████▌  | 1900/2502 [52:26<16:35,  1.65s/it, Loss=2.7840, Top1=N/A, LR=0.004847]2025-11-07 17:56:22,320 - INFO - Step 199558: {'train_loss_batch': 2.020550489425659, 'train_lr': 0.004847156268690288, 'batch_time': 1.6551759624280533, 'data_time': 0.0031405297910960207}
2025-11-07 17:56:22 Train Epoch 079:  80%|███████▉  | 2000/2502 [55:10<13:46,  1.65s/it, Loss=2.7843, Top1=N/A, LR=0.004847]2025-11-07 17:59:06,041 - INFO - Step 199658: {'train_loss_batch': 2.4798996448516846, 'train_lr': 0.004847156268690288, 'batch_time': 1.6542777244238065, 'data_time': 0.003032918217061818}
2025-11-07 17:59:06 Train Epoch 079:  84%|████████▍ | 2100/2502 [57:55<11:03,  1.65s/it, Loss=2.7818, Top1=N/A, LR=0.004847]2025-11-07 18:01:50,914 - INFO - Step 199758: {'train_loss_batch': 3.5872702598571777, 'train_lr': 0.004847156268690288, 'batch_time': 1.6540136196566786, 'data_time': 0.0029378711241078003}
2025-11-07 18:01:50 Train Epoch 079:  88%|████████▊ | 2200/2502 [1:00:40<08:16,  1.65s/it, Loss=2.7809, Top1=N/A, LR=0.004847]2025-11-07 18:04:36,390 - INFO - Step 199858: {'train_loss_batch': 2.144084930419922, 'train_lr': 0.004847156268690288, 'batch_time': 1.6540474405293029, 'data_time': 0.002853258151132808}
2025-11-07 18:04:36 Train Epoch 079:  92%|█████████▏| 2300/2502 [1:03:25<05:34,  1.65s/it, Loss=2.7798, Top1=N/A, LR=0.004847]2025-11-07 18:07:21,316 - INFO - Step 199958: {'train_loss_batch': 4.26760721206665, 'train_lr': 0.004847156268690288, 'batch_time': 1.6538392306928373, 'data_time': 0.0027731585015633065}
2025-11-07 18:07:21 Train Epoch 079:  96%|█████████▌| 2400/2502 [1:06:11<02:48,  1.65s/it, Loss=2.7814, Top1=N/A, LR=0.004847]2025-11-07 18:10:07,027 - INFO - Step 200058: {'train_loss_batch': 4.209084510803223, 'train_lr': 0.004847156268690288, 'batch_time': 1.6539755200207704, 'data_time': 0.0027043729064763463}
2025-11-07 18:10:07 Train Epoch 079: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.65s/it, Loss=2.7814, Top1=N/A, LR=0.004847]2025-11-07 18:12:52,129 - INFO - Step 200158: {'train_loss_batch': 3.646202564239502, 'train_lr': 0.004847156268690288, 'batch_time': 1.6538570771833174, 'data_time': 0.0026774843041299104}
2025-11-07 18:12:52 Train Epoch 079: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=2.7814, Top1=N/A, LR=0.004847]
2025-11-07 18:12:54 Val Epoch 079:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 18:12:58   with torch.cuda.amp.autocast():
2025-11-07 18:12:59 Val Epoch 079: 100%|██████████| 98/98 [01:49<00:00,  1.11s/it, Loss=1.9335, Top1=74.80%, Top5=92.44%]
2025-11-07 18:14:43 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 18:14:43   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 18:14:43 2025-11-07 18:14:43,513 - INFO - Step 79: {'epoch': 79, 'learning_rate': 0.004085243147632579, 'train_loss': 2.781103300486061, 'train_top1': 72.265625, 'train_top5': 89.64255459337349, 'train_precision': 72.1698873578036, 'train_recall': 72.14723106416466, 'train_f1': 72.02021615994693, 'val_loss': 1.9334572392272948, 'val_top1': 74.79800001708985, 'val_top5': 92.43599997802734, 'val_precision': 75.47542298899347, 'val_recall': 74.802, 'val_f1': 74.43897648095353}
2025-11-07 18:14:43 2025-11-07 18:14:43,514 - INFO - Epoch 079 Summary - LR: 0.004085, Train Loss: 2.7811, Val Loss: 1.9335, Val F1: 74.44%, Val Precision: 75.48%, Val Recall: 74.80%
2025-11-07 18:14:46 2025-11-07 18:14:46,553 - INFO - New best model saved with validation accuracy: 74.798%
2025-11-07 18:14:46 2025-11-07 18:14:46,554 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_080.pth
2025-11-07 18:14:46 Train Epoch 080:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 79 that is less than the current step 200158. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 18:14:50 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 18:14:50   with torch.cuda.amp.autocast():
2025-11-07 18:14:51 Train Epoch 080:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.7471, Top1=N/A, LR=0.004085]2025-11-07 18:14:51,778 - INFO - Step 200160: {'train_loss_batch': 3.747080087661743, 'train_lr': 0.004085243147632579, 'batch_time': 5.2229835987091064, 'data_time': 3.582894802093506}
2025-11-07 18:14:51 Train Epoch 080:   4%|▍         | 100/2502 [02:50<1:06:05,  1.65s/it, Loss=2.8913, Top1=N/A, LR=0.004085]2025-11-07 18:17:36,763 - INFO - Step 200260: {'train_loss_batch': 3.299459457397461, 'train_lr': 0.004085243147632579, 'batch_time': 1.6852327833081235, 'data_time': 0.03655975643951114}
2025-11-07 18:17:36 Train Epoch 080:   8%|▊         | 200/2502 [05:35<1:03:01,  1.64s/it, Loss=2.8954, Top1=N/A, LR=0.004085]2025-11-07 18:20:21,599 - INFO - Step 200360: {'train_loss_batch': 3.643918037414551, 'train_lr': 0.004085243147632579, 'batch_time': 1.6668841672773977, 'data_time': 0.018845787095786326}
2025-11-07 18:20:21 Train Epoch 080:  12%|█▏        | 300/2502 [08:19<1:00:14,  1.64s/it, Loss=2.8466, Top1=N/A, LR=0.004085]2025-11-07 18:23:06,552 - INFO - Step 200460: {'train_loss_batch': 2.716540813446045, 'train_lr': 0.004085243147632579, 'batch_time': 1.6611186063962917, 'data_time': 0.012942452763402185}
2025-11-07 18:23:06 Train Epoch 080:  16%|█▌        | 400/2502 [11:04<57:47,  1.65s/it, Loss=2.8320, Top1=72.86%, LR=0.004085]2025-11-07 18:25:51,035 - INFO - Step 200560: {'train_loss_batch': 2.1361007690429688, 'train_lr': 0.004085243147632579, 'batch_time': 1.6570568934937664, 'data_time': 0.00995855200617688}
2025-11-07 18:25:51 Train Epoch 080:  20%|█▉        | 500/2502 [13:49<55:14,  1.66s/it, Loss=2.8181, Top1=N/A, LR=0.004085]   2025-11-07 18:28:36,530 - INFO - Step 200660: {'train_loss_batch': 2.10441255569458, 'train_lr': 0.004085243147632579, 'batch_time': 1.6566357650680694, 'data_time': 0.008173465728759766}
2025-11-07 18:28:36 Train Epoch 080:  24%|██▍       | 600/2502 [16:36<52:26,  1.65s/it, Loss=2.8142, Top1=N/A, LR=0.004085]2025-11-07 18:31:22,579 - INFO - Step 200760: {'train_loss_batch': 2.475858449935913, 'train_lr': 0.004085243147632579, 'batch_time': 1.657275780663514, 'data_time': 0.006975112857913812}
2025-11-07 18:31:22 Train Epoch 080:  28%|██▊       | 700/2502 [19:21<49:48,  1.66s/it, Loss=2.8249, Top1=72.88%, LR=0.004085]2025-11-07 18:34:08,256 - INFO - Step 200860: {'train_loss_batch': 2.156039237976074, 'train_lr': 0.004085243147632579, 'batch_time': 1.6572032954315317, 'data_time': 0.0061284026473803115}
2025-11-07 18:34:08 Train Epoch 080:  32%|███▏      | 800/2502 [22:07<47:08,  1.66s/it, Loss=2.8069, Top1=72.84%, LR=0.004085]2025-11-07 18:36:54,010 - INFO - Step 200960: {'train_loss_batch': 2.091167449951172, 'train_lr': 0.004085243147632579, 'batch_time': 1.6572453502412146, 'data_time': 0.005484202083725757}
2025-11-07 18:36:54 Train Epoch 080:  36%|███▌      | 900/2502 [24:52<43:34,  1.63s/it, Loss=2.8142, Top1=N/A, LR=0.004085]   2025-11-07 18:39:38,851 - INFO - Step 201060: {'train_loss_batch': 2.9403457641601562, 'train_lr': 0.004085243147632579, 'batch_time': 1.6562642900316618, 'data_time': 0.004985317405929311}
2025-11-07 18:39:38 Train Epoch 080:  40%|███▉      | 1000/2502 [27:35<41:01,  1.64s/it, Loss=2.8212, Top1=N/A, LR=0.004085]2025-11-07 18:42:21,634 - INFO - Step 201160: {'train_loss_batch': 3.22698712348938, 'train_lr': 0.004085243147632579, 'batch_time': 1.6534234596180035, 'data_time': 0.0045766173066435515}
2025-11-07 18:42:21 Train Epoch 080:  44%|████▍     | 1100/2502 [30:19<38:24,  1.64s/it, Loss=2.8191, Top1=N/A, LR=0.004085]2025-11-07 18:45:06,532 - INFO - Step 201260: {'train_loss_batch': 3.3394453525543213, 'train_lr': 0.004085243147632579, 'batch_time': 1.6530196662386583, 'data_time': 0.004254759494875042}
2025-11-07 18:45:06 Train Epoch 080:  48%|████▊     | 1200/2502 [33:05<35:54,  1.65s/it, Loss=2.8092, Top1=N/A, LR=0.004085]2025-11-07 18:47:52,096 - INFO - Step 201360: {'train_loss_batch': 2.5084457397460938, 'train_lr': 0.004085243147632579, 'batch_time': 1.6532379788820393, 'data_time': 0.0039882711526456225}
2025-11-07 18:47:52 Train Epoch 080:  52%|█████▏    | 1300/2502 [35:50<32:44,  1.63s/it, Loss=2.8046, Top1=72.83%, LR=0.004085]2025-11-07 18:50:36,933 - INFO - Step 201460: {'train_loss_batch': 2.0372982025146484, 'train_lr': 0.004085243147632579, 'batch_time': 1.6528633819187173, 'data_time': 0.003761040623787272}
2025-11-07 18:50:36 Train Epoch 080:  56%|█████▌    | 1400/2502 [38:34<30:32,  1.66s/it, Loss=2.8035, Top1=72.81%, LR=0.004085]2025-11-07 18:53:21,136 - INFO - Step 201560: {'train_loss_batch': 2.041677474975586, 'train_lr': 0.004085243147632579, 'batch_time': 1.6520897448701062, 'data_time': 0.0035615881198989246}
2025-11-07 18:53:21 Train Epoch 080:  60%|█████▉    | 1500/2502 [41:19<27:13,  1.63s/it, Loss=2.8032, Top1=N/A, LR=0.004085]   2025-11-07 18:56:06,023 - INFO - Step 201660: {'train_loss_batch': 3.129744052886963, 'train_lr': 0.004085243147632579, 'batch_time': 1.6518750206618846, 'data_time': 0.003390337291516756}
2025-11-07 18:56:06 Train Epoch 080:  64%|██████▍   | 1600/2502 [44:04<24:50,  1.65s/it, Loss=2.7985, Top1=N/A, LR=0.004085]2025-11-07 18:58:50,729 - INFO - Step 201760: {'train_loss_batch': 2.5566630363464355, 'train_lr': 0.004085243147632579, 'batch_time': 1.6515747208211067, 'data_time': 0.0032427820245002972}
2025-11-07 18:58:50 Train Epoch 080:  68%|██████▊   | 1700/2502 [46:49<21:56,  1.64s/it, Loss=2.8090, Top1=N/A, LR=0.004085]2025-11-07 19:01:36,067 - INFO - Step 201860: {'train_loss_batch': 3.6653857231140137, 'train_lr': 0.004085243147632579, 'batch_time': 1.6516800627576682, 'data_time': 0.0031111435214608366}
2025-11-07 19:01:36 Train Epoch 080:  72%|███████▏  | 1800/2502 [49:33<19:19,  1.65s/it, Loss=2.8068, Top1=N/A, LR=0.004085]2025-11-07 19:04:19,661 - INFO - Step 201960: {'train_loss_batch': 4.116009712219238, 'train_lr': 0.004085243147632579, 'batch_time': 1.6508063842428187, 'data_time': 0.002991841012805386}
2025-11-07 19:04:19 Train Epoch 080:  76%|███████▌  | 1900/2502 [52:17<16:25,  1.64s/it, Loss=2.7983, Top1=N/A, LR=0.004085]2025-11-07 19:07:03,632 - INFO - Step 202060: {'train_loss_batch': 3.6554694175720215, 'train_lr': 0.004085243147632579, 'batch_time': 1.6502228183284802, 'data_time': 0.0028898106444076384}
2025-11-07 19:07:03 Train Epoch 080:  80%|███████▉  | 2000/2502 [55:02<13:53,  1.66s/it, Loss=2.7951, Top1=N/A, LR=0.004085]2025-11-07 19:09:48,827 - INFO - Step 202160: {'train_loss_batch': 3.332033634185791, 'train_lr': 0.004085243147632579, 'batch_time': 1.650308855410399, 'data_time': 0.002794972781477303}
2025-11-07 19:09:48 Train Epoch 080:  84%|████████▍ | 2100/2502 [57:47<11:07,  1.66s/it, Loss=2.7891, Top1=N/A, LR=0.004085]2025-11-07 19:12:34,534 - INFO - Step 202260: {'train_loss_batch': 3.531141996383667, 'train_lr': 0.004085243147632579, 'batch_time': 1.6506304307417436, 'data_time': 0.002713388740761061}
2025-11-07 19:12:34 Train Epoch 080:  88%|████████▊ | 2200/2502 [1:00:32<08:13,  1.63s/it, Loss=2.7836, Top1=N/A, LR=0.004085]2025-11-07 19:15:19,549 - INFO - Step 202360: {'train_loss_batch': 2.0941109657287598, 'train_lr': 0.004085243147632579, 'batch_time': 1.6506086085829503, 'data_time': 0.002638325806045359}
2025-11-07 19:15:19 Train Epoch 080:  92%|█████████▏| 2300/2502 [1:03:17<05:34,  1.65s/it, Loss=2.7863, Top1=N/A, LR=0.004085]2025-11-07 19:18:04,473 - INFO - Step 202460: {'train_loss_batch': 2.1164023876190186, 'train_lr': 0.004085243147632579, 'batch_time': 1.6505488815746945, 'data_time': 0.0025684988949413873}
2025-11-07 19:18:04 Train Epoch 080:  96%|█████████▌| 2400/2502 [1:06:02<02:48,  1.65s/it, Loss=2.7911, Top1=N/A, LR=0.004085]2025-11-07 19:20:49,067 - INFO - Step 202560: {'train_loss_batch': 1.9945107698440552, 'train_lr': 0.004085243147632579, 'batch_time': 1.650356747616534, 'data_time': 0.002498884888203727}
2025-11-07 19:20:49 Train Epoch 080: 100%|█████████▉| 2500/2502 [1:08:47<00:03,  1.66s/it, Loss=2.7903, Top1=N/A, LR=0.004085]2025-11-07 19:23:34,193 - INFO - Step 202660: {'train_loss_batch': 3.557347536087036, 'train_lr': 0.004085243147632579, 'batch_time': 1.650392885829677, 'data_time': 0.0024633550586723317}
2025-11-07 19:23:34 Train Epoch 080: 100%|██████████| 2502/2502 [1:08:49<00:00,  1.65s/it, Loss=2.7903, Top1=N/A, LR=0.004085]
2025-11-07 19:23:36 Val Epoch 080:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 19:23:40   with torch.cuda.amp.autocast():
2025-11-07 19:23:41 Val Epoch 080: 100%|██████████| 98/98 [01:49<00:00,  1.12s/it, Loss=1.9140, Top1=75.17%, Top5=92.72%]
2025-11-07 19:25:26 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 19:25:26   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 19:25:26 2025-11-07 19:25:26,051 - INFO - Step 80: {'epoch': 80, 'learning_rate': 0.0033860508909292317, 'train_loss': 2.790450981004442, 'train_top1': 72.7535277374031, 'train_top5': 89.88462936046511, 'train_precision': 72.65356214221829, 'train_recall': 72.64008559679064, 'train_f1': 72.51333268262988, 'val_loss': 1.914015984992981, 'val_top1': 75.17399998535156, 'val_top5': 92.71600000732421, 'val_precision': 75.72873334149733, 'val_recall': 75.17399999999999, 'val_f1': 74.8180367757215}
2025-11-07 19:25:26 2025-11-07 19:25:26,052 - INFO - Epoch 080 Summary - LR: 0.003386, Train Loss: 2.7905, Val Loss: 1.9140, Val F1: 74.82%, Val Precision: 75.73%, Val Recall: 75.17%
2025-11-07 19:25:26 wandb: WARNING Tried to log to step 80 that is less than the current step 202660. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 19:25:29 2025-11-07 19:25:29,257 - INFO - New best model saved with validation accuracy: 75.174%
2025-11-07 19:25:29 2025-11-07 19:25:29,257 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_081.pth
2025-11-07 19:25:29 Train Epoch 081:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 19:25:33   with torch.cuda.amp.autocast():
2025-11-07 19:25:34 Train Epoch 081:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.0247, Top1=75.78%, LR=0.003386]2025-11-07 19:25:34,878 - INFO - Step 202662: {'train_loss_batch': 2.0246901512145996, 'train_lr': 0.0033860508909292317, 'batch_time': 5.619063854217529, 'data_time': 3.9867615699768066}
2025-11-07 19:25:34 Train Epoch 081:   4%|▍         | 100/2502 [02:51<1:06:34,  1.66s/it, Loss=2.7121, Top1=N/A, LR=0.003386]   2025-11-07 19:28:20,623 - INFO - Step 202762: {'train_loss_batch': 2.0364575386047363, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6966776658992957, 'data_time': 0.040546903515806296}
2025-11-07 19:28:20 Train Epoch 081:   8%|▊         | 200/2502 [05:37<1:03:20,  1.65s/it, Loss=2.7003, Top1=N/A, LR=0.003386]2025-11-07 19:31:06,311 - INFO - Step 202862: {'train_loss_batch': 2.047450304031372, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6768781569466662, 'data_time': 0.02085283621033626}
2025-11-07 19:31:06 Train Epoch 081:  12%|█▏        | 300/2502 [08:22<1:00:40,  1.65s/it, Loss=2.6855, Top1=73.64%, LR=0.003386]2025-11-07 19:33:51,546 - INFO - Step 202962: {'train_loss_batch': 2.0217959880828857, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6687280109950475, 'data_time': 0.014251639280604366}
2025-11-07 19:33:51 Train Epoch 081:  16%|█▌        | 400/2502 [11:06<58:09,  1.66s/it, Loss=2.7108, Top1=N/A, LR=0.003386]   2025-11-07 19:36:36,147 - INFO - Step 203062: {'train_loss_batch': 2.1317527294158936, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6630622294181006, 'data_time': 0.01095137096699931}
2025-11-07 19:36:36 Train Epoch 081:  20%|█▉        | 500/2502 [13:52<55:25,  1.66s/it, Loss=2.7134, Top1=N/A, LR=0.003386]2025-11-07 19:39:21,892 - INFO - Step 203162: {'train_loss_batch': 3.408285140991211, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6619419352023188, 'data_time': 0.008969254122522776}
2025-11-07 19:39:21 Train Epoch 081:  24%|██▍       | 600/2502 [16:37<52:04,  1.64s/it, Loss=2.7143, Top1=N/A, LR=0.003386]2025-11-07 19:42:07,111 - INFO - Step 203262: {'train_loss_batch': 3.763944387435913, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6603186979468372, 'data_time': 0.007647375100464274}
2025-11-07 19:42:07 Train Epoch 081:  28%|██▊       | 700/2502 [19:23<50:03,  1.67s/it, Loss=2.7207, Top1=73.36%, LR=0.003386]2025-11-07 19:44:52,329 - INFO - Step 203362: {'train_loss_batch': 2.029999017715454, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6591583770964182, 'data_time': 0.0066989025954003}
2025-11-07 19:44:52 Train Epoch 081:  32%|███▏      | 800/2502 [22:09<47:07,  1.66s/it, Loss=2.7261, Top1=N/A, LR=0.003386]   2025-11-07 19:47:38,262 - INFO - Step 203462: {'train_loss_batch': 3.6822385787963867, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6591794684287462, 'data_time': 0.005999757109509871}
2025-11-07 19:47:38 Train Epoch 081:  36%|███▌      | 900/2502 [24:54<43:42,  1.64s/it, Loss=2.7368, Top1=73.39%, LR=0.003386]2025-11-07 19:50:23,739 - INFO - Step 203562: {'train_loss_batch': 2.031233787536621, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6586897611353426, 'data_time': 0.005449534520986474}
2025-11-07 19:50:23 Train Epoch 081:  40%|███▉      | 1000/2502 [27:39<41:39,  1.66s/it, Loss=2.7354, Top1=N/A, LR=0.003386]   2025-11-07 19:53:08,792 - INFO - Step 203662: {'train_loss_batch': 3.165637493133545, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6578737122195584, 'data_time': 0.005001426100373625}
2025-11-07 19:53:08 Train Epoch 081:  44%|████▍     | 1100/2502 [30:25<38:53,  1.66s/it, Loss=2.7334, Top1=N/A, LR=0.003386]2025-11-07 19:55:54,606 - INFO - Step 203762: {'train_loss_batch': 2.0777716636657715, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6578980570593063, 'data_time': 0.00463749797207783}
2025-11-07 19:55:54 Train Epoch 081:  48%|████▊     | 1200/2502 [33:09<35:41,  1.64s/it, Loss=2.7460, Top1=N/A, LR=0.003386]2025-11-07 19:58:38,982 - INFO - Step 203862: {'train_loss_batch': 2.4767346382141113, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6567207029915967, 'data_time': 0.004333450633421429}
2025-11-07 19:58:38 Train Epoch 081:  52%|█████▏    | 1300/2502 [35:55<33:09,  1.66s/it, Loss=2.7500, Top1=N/A, LR=0.003386]2025-11-07 20:01:24,426 - INFO - Step 203962: {'train_loss_batch': 3.6717724800109863, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6565453593864705, 'data_time': 0.004081752829877896}
2025-11-07 20:01:24 Train Epoch 081:  56%|█████▌    | 1400/2502 [38:40<30:20,  1.65s/it, Loss=2.7438, Top1=N/A, LR=0.003386]2025-11-07 20:04:10,181 - INFO - Step 204062: {'train_loss_batch': 3.4788949489593506, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6566166933905133, 'data_time': 0.0038617703848273137}
2025-11-07 20:04:10 Train Epoch 081:  60%|█████▉    | 1500/2502 [41:25<27:32,  1.65s/it, Loss=2.7467, Top1=N/A, LR=0.003386]2025-11-07 20:06:55,145 - INFO - Step 204162: {'train_loss_batch': 4.496718406677246, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6561518765385352, 'data_time': 0.0036698156480071227}
2025-11-07 20:06:55 Train Epoch 081:  64%|██████▍   | 1600/2502 [44:11<24:55,  1.66s/it, Loss=2.7386, Top1=N/A, LR=0.003386]2025-11-07 20:09:40,841 - INFO - Step 204262: {'train_loss_batch': 2.830249309539795, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6562023010944888, 'data_time': 0.003504639041192974}
2025-11-07 20:09:40 Train Epoch 081:  68%|██████▊   | 1700/2502 [46:56<21:55,  1.64s/it, Loss=2.7324, Top1=N/A, LR=0.003386]2025-11-07 20:12:25,999 - INFO - Step 204362: {'train_loss_batch': 2.0100364685058594, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6559301651904752, 'data_time': 0.003356285055968987}
2025-11-07 20:12:26 Train Epoch 081:  72%|███████▏  | 1800/2502 [49:40<19:07,  1.63s/it, Loss=2.7250, Top1=73.22%, LR=0.003386]2025-11-07 20:15:10,178 - INFO - Step 204462: {'train_loss_batch': 2.0054779052734375, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6551452029883233, 'data_time': 0.003221876551084291}
2025-11-07 20:15:10 Train Epoch 081:  76%|███████▌  | 1900/2502 [52:25<16:32,  1.65s/it, Loss=2.7335, Top1=73.21%, LR=0.003386]2025-11-07 20:17:55,231 - INFO - Step 204562: {'train_loss_batch': 2.0352845191955566, 'train_lr': 0.0033860508909292317, 'batch_time': 1.654901993017583, 'data_time': 0.003104667924442522}
2025-11-07 20:17:55 Train Epoch 081:  80%|███████▉  | 2000/2502 [55:11<13:42,  1.64s/it, Loss=2.7418, Top1=N/A, LR=0.003386]   2025-11-07 20:20:40,527 - INFO - Step 204662: {'train_loss_batch': 2.085197687149048, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6548050839206327, 'data_time': 0.0029984615493690533}
2025-11-07 20:20:40 Train Epoch 081:  84%|████████▍ | 2100/2502 [57:56<11:04,  1.65s/it, Loss=2.7474, Top1=73.18%, LR=0.003386]2025-11-07 20:23:25,448 - INFO - Step 204762: {'train_loss_batch': 2.039429187774658, 'train_lr': 0.0033860508909292317, 'batch_time': 1.654538336394799, 'data_time': 0.002902477142755217}
2025-11-07 20:23:25 Train Epoch 081:  88%|████████▊ | 2200/2502 [1:00:40<08:15,  1.64s/it, Loss=2.7435, Top1=N/A, LR=0.003386]   2025-11-07 20:26:09,829 - INFO - Step 204862: {'train_loss_batch': 2.1982550621032715, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6540511382378973, 'data_time': 0.002813148693516275}
2025-11-07 20:26:09 Train Epoch 081:  92%|█████████▏| 2300/2502 [1:03:25<05:32,  1.65s/it, Loss=2.7432, Top1=N/A, LR=0.003386]2025-11-07 20:28:54,903 - INFO - Step 204962: {'train_loss_batch': 4.409430027008057, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6539072489956057, 'data_time': 0.002734120852633281}
2025-11-07 20:28:54 Train Epoch 081:  96%|█████████▌| 2400/2502 [1:06:11<02:49,  1.66s/it, Loss=2.7412, Top1=73.13%, LR=0.003386]2025-11-07 20:31:40,532 - INFO - Step 205062: {'train_loss_batch': 2.099907875061035, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6540061334231058, 'data_time': 0.002663795375069297}
2025-11-07 20:31:40 Train Epoch 081: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.66s/it, Loss=2.7432, Top1=N/A, LR=0.003386]   2025-11-07 20:34:25,658 - INFO - Step 205162: {'train_loss_batch': 2.4207382202148438, 'train_lr': 0.0033860508909292317, 'batch_time': 1.6538964293089833, 'data_time': 0.0026286430046206614}
2025-11-07 20:34:25 Train Epoch 081: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=2.7432, Top1=N/A, LR=0.003386]
2025-11-07 20:34:27 Val Epoch 081:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 20:34:32   with torch.cuda.amp.autocast():
2025-11-07 20:34:32 Val Epoch 081: 100%|██████████| 98/98 [01:49<00:00,  1.11s/it, Loss=1.8956, Top1=75.57%, Top5=92.97%]
2025-11-07 20:36:17 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 20:36:17   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 20:36:17 2025-11-07 20:36:17,234 - INFO - Step 81: {'epoch': 81, 'learning_rate': 0.0027505345110998307, 'train_loss': 2.7433422955868245, 'train_top1': 73.13687193627452, 'train_top5': 90.00076593137256, 'train_precision': 73.07699706649913, 'train_recall': 73.05329849909229, 'train_f1': 72.92267279562404, 'val_loss': 1.8955585709381104, 'val_top1': 75.56799997070313, 'val_top5': 92.96600000488282, 'val_precision': 76.0553534812372, 'val_recall': 75.56200000000001, 'val_f1': 75.20862111177543}
2025-11-07 20:36:17 2025-11-07 20:36:17,235 - INFO - Epoch 081 Summary - LR: 0.002751, Train Loss: 2.7433, Val Loss: 1.8956, Val F1: 75.21%, Val Precision: 76.06%, Val Recall: 75.56%
2025-11-07 20:36:20 2025-11-07 20:36:20,938 - INFO - New best model saved with validation accuracy: 75.568%
2025-11-07 20:36:20 2025-11-07 20:36:20,939 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_082.pth
2025-11-07 20:36:20 Train Epoch 082:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 20:36:24   with torch.cuda.amp.autocast():
2025-11-07 20:36:26 Train Epoch 082:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.2020, Top1=N/A, LR=0.002751]2025-11-07 20:36:26,574 - INFO - Step 205164: {'train_loss_batch': 2.2020418643951416, 'train_lr': 0.0027505345110998307, 'batch_time': 5.633134841918945, 'data_time': 3.988605499267578}
2025-11-07 20:36:26 Train Epoch 082:   0%|          | 1/2502 [00:05<3:55:05,  5.64s/it, Loss=2.2020, Top1=N/A, LR=0.002751]wandb: WARNING Tried to log to step 81 that is less than the current step 205162. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 20:36:28 Train Epoch 082:   4%|▍         | 100/2502 [02:50<1:05:33,  1.64s/it, Loss=2.7623, Top1=N/A, LR=0.002751]2025-11-07 20:39:11,200 - INFO - Step 205264: {'train_loss_batch': 3.7153024673461914, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6857445499684551, 'data_time': 0.04058597111465907}
2025-11-07 20:39:11 Train Epoch 082:   8%|▊         | 200/2502 [05:35<1:03:30,  1.66s/it, Loss=2.8315, Top1=N/A, LR=0.002751]2025-11-07 20:41:55,976 - INFO - Step 205364: {'train_loss_batch': 3.493433952331543, 'train_lr': 0.0027505345110998307, 'batch_time': 1.66684329450427, 'data_time': 0.020912181085615017}
2025-11-07 20:41:55 Train Epoch 082:  12%|█▏        | 300/2502 [08:20<1:00:31,  1.65s/it, Loss=2.7665, Top1=N/A, LR=0.002751]2025-11-07 20:44:41,197 - INFO - Step 205464: {'train_loss_batch': 2.0249123573303223, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6619820087851085, 'data_time': 0.014304466817862172}
2025-11-07 20:44:41 Train Epoch 082:  16%|█▌        | 400/2502 [11:05<58:01,  1.66s/it, Loss=2.7509, Top1=73.42%, LR=0.002751]2025-11-07 20:47:26,324 - INFO - Step 205564: {'train_loss_batch': 2.014535427093506, 'train_lr': 0.0027505345110998307, 'batch_time': 1.659308658870973, 'data_time': 0.01099845477173156}
2025-11-07 20:47:26 Train Epoch 082:  20%|█▉        | 500/2502 [13:49<54:26,  1.63s/it, Loss=2.7373, Top1=N/A, LR=0.002751]   2025-11-07 20:50:10,850 - INFO - Step 205664: {'train_loss_batch': 2.7245442867279053, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6565059197401095, 'data_time': 0.008996895925251549}
2025-11-07 20:50:10 Train Epoch 082:  24%|██▍       | 600/2502 [16:34<52:34,  1.66s/it, Loss=2.7460, Top1=N/A, LR=0.002751]2025-11-07 20:52:55,224 - INFO - Step 205764: {'train_loss_batch': 3.033663511276245, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6543809720958131, 'data_time': 0.00766795288504856}
2025-11-07 20:52:55 Train Epoch 082:  28%|██▊       | 700/2502 [19:19<49:40,  1.65s/it, Loss=2.7580, Top1=N/A, LR=0.002751]2025-11-07 20:55:40,696 - INFO - Step 205864: {'train_loss_batch': 4.4091644287109375, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6544279698468478, 'data_time': 0.006716888403246304}
2025-11-07 20:55:40 Train Epoch 082:  32%|███▏      | 800/2502 [22:03<46:23,  1.64s/it, Loss=2.7494, Top1=N/A, LR=0.002751]2025-11-07 20:58:24,578 - INFO - Step 205964: {'train_loss_batch': 4.429961681365967, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6524796962142734, 'data_time': 0.006003717060541541}
2025-11-07 20:58:24 Train Epoch 082:  36%|███▌      | 900/2502 [24:48<44:20,  1.66s/it, Loss=2.7413, Top1=N/A, LR=0.002751]2025-11-07 21:01:09,819 - INFO - Step 206064: {'train_loss_batch': 3.40069580078125, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6524716458230648, 'data_time': 0.005449357493206875}
2025-11-07 21:01:09 Train Epoch 082:  40%|███▉      | 1000/2502 [27:34<41:32,  1.66s/it, Loss=2.7434, Top1=N/A, LR=0.002751]2025-11-07 21:03:55,336 - INFO - Step 206164: {'train_loss_batch': 2.3839964866638184, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6527408515061293, 'data_time': 0.005003718110350343}
2025-11-07 21:03:55 Train Epoch 082:  44%|████▍     | 1100/2502 [30:19<38:47,  1.66s/it, Loss=2.7476, Top1=N/A, LR=0.002751]2025-11-07 21:06:40,490 - INFO - Step 206264: {'train_loss_batch': 3.7549562454223633, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6526318146465693, 'data_time': 0.004640544142969947}
2025-11-07 21:06:40 Train Epoch 082:  48%|████▊     | 1200/2502 [33:04<35:29,  1.64s/it, Loss=2.7435, Top1=N/A, LR=0.002751]2025-11-07 21:09:25,718 - INFO - Step 206364: {'train_loss_batch': 2.127157211303711, 'train_lr': 0.0027505345110998307, 'batch_time': 1.652602195144196, 'data_time': 0.004338072102631657}
2025-11-07 21:09:25 Train Epoch 082:  52%|█████▏    | 1300/2502 [35:49<33:08,  1.65s/it, Loss=2.7430, Top1=N/A, LR=0.002751]2025-11-07 21:12:10,802 - INFO - Step 206464: {'train_loss_batch': 2.399798631668091, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6524670576701066, 'data_time': 0.004080139060463931}
2025-11-07 21:12:10 Train Epoch 082:  56%|█████▌    | 1400/2502 [38:35<30:16,  1.65s/it, Loss=2.7440, Top1=N/A, LR=0.002751]2025-11-07 21:14:56,172 - INFO - Step 206564: {'train_loss_batch': 3.3597288131713867, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6525545370400079, 'data_time': 0.003858562199921373}
2025-11-07 21:14:56 Train Epoch 082:  60%|█████▉    | 1500/2502 [41:21<27:47,  1.66s/it, Loss=2.7471, Top1=N/A, LR=0.002751]2025-11-07 21:17:41,969 - INFO - Step 206664: {'train_loss_batch': 3.0583462715148926, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6529155644792306, 'data_time': 0.00367129952966333}
2025-11-07 21:17:41 Train Epoch 082:  64%|██████▍   | 1600/2502 [44:07<24:53,  1.66s/it, Loss=2.7566, Top1=73.53%, LR=0.002751]2025-11-07 21:20:27,958 - INFO - Step 206764: {'train_loss_batch': 2.009103775024414, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6533507577037156, 'data_time': 0.0035084114455938486}
2025-11-07 21:20:27 Train Epoch 082:  68%|██████▊   | 1700/2502 [46:52<22:12,  1.66s/it, Loss=2.7583, Top1=N/A, LR=0.002751]   2025-11-07 21:23:13,901 - INFO - Step 206864: {'train_loss_batch': 3.2184090614318848, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6537079031785609, 'data_time': 0.0033646011128277025}
2025-11-07 21:23:13 Train Epoch 082:  72%|███████▏  | 1800/2502 [49:38<19:27,  1.66s/it, Loss=2.7538, Top1=N/A, LR=0.002751]2025-11-07 21:25:59,460 - INFO - Step 206964: {'train_loss_batch': 3.499596357345581, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6538122047125665, 'data_time': 0.0032334592460195994}
2025-11-07 21:25:59 Train Epoch 082:  76%|███████▌  | 1900/2502 [52:24<16:32,  1.65s/it, Loss=2.7511, Top1=73.53%, LR=0.002751]2025-11-07 21:28:45,040 - INFO - Step 207064: {'train_loss_batch': 1.9800236225128174, 'train_lr': 0.0027505345110998307, 'batch_time': 1.653916810826839, 'data_time': 0.003116733585138938}
2025-11-07 21:28:45 Train Epoch 082:  80%|███████▉  | 2000/2502 [55:09<13:47,  1.65s/it, Loss=2.7478, Top1=N/A, LR=0.002751]   2025-11-07 21:31:30,186 - INFO - Step 207164: {'train_loss_batch': 3.6200101375579834, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6537936225883487, 'data_time': 0.003012803600526702}
2025-11-07 21:31:30 Train Epoch 082:  84%|████████▍ | 2100/2502 [57:54<11:08,  1.66s/it, Loss=2.7518, Top1=73.56%, LR=0.002751]2025-11-07 21:34:15,940 - INFO - Step 207264: {'train_loss_batch': 1.9425352811813354, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6539719296545712, 'data_time': 0.002920878608699982}
2025-11-07 21:34:15 Train Epoch 082:  88%|████████▊ | 2200/2502 [1:00:40<08:15,  1.64s/it, Loss=2.7587, Top1=73.58%, LR=0.002751]2025-11-07 21:37:01,153 - INFO - Step 207364: {'train_loss_batch': 1.9863654375076294, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6538882785252904, 'data_time': 0.002829180691037488}
2025-11-07 21:37:01 Train Epoch 082:  92%|█████████▏| 2300/2502 [1:03:25<05:33,  1.65s/it, Loss=2.7584, Top1=N/A, LR=0.002751]   2025-11-07 21:39:46,478 - INFO - Step 207464: {'train_loss_batch': 2.7067408561706543, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6538603574180022, 'data_time': 0.0027510270197875393}
2025-11-07 21:39:46 Train Epoch 082:  96%|█████████▌| 2400/2502 [1:06:11<02:49,  1.66s/it, Loss=2.7573, Top1=N/A, LR=0.002751]2025-11-07 21:42:32,409 - INFO - Step 207564: {'train_loss_batch': 2.137364149093628, 'train_lr': 0.0027505345110998307, 'batch_time': 1.654087375373157, 'data_time': 0.002680583180908559}
2025-11-07 21:42:32 Train Epoch 082: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.66s/it, Loss=2.7558, Top1=73.55%, LR=0.002751]2025-11-07 21:45:17,330 - INFO - Step 207664: {'train_loss_batch': 1.86822509765625, 'train_lr': 0.0027505345110998307, 'batch_time': 1.6538922025984832, 'data_time': 0.002647871973036957}
2025-11-07 21:45:17 Train Epoch 082: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=2.7558, Top1=73.55%, LR=0.002751]
2025-11-07 21:45:19 Val Epoch 082:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 21:45:23   with torch.cuda.amp.autocast():
2025-11-07 21:45:24 Val Epoch 082: 100%|██████████| 98/98 [01:50<00:00,  1.12s/it, Loss=1.8905, Top1=75.77%, Top5=92.91%]
2025-11-07 21:47:09 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 21:47:09   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 21:47:09 2025-11-07 21:47:09,729 - INFO - Step 82: {'epoch': 82, 'learning_rate': 0.002179562047075466, 'train_loss': 2.7558340886704547, 'train_top1': 73.54971732107356, 'train_top5': 90.29261928429423, 'train_precision': 73.440026091031, 'train_recall': 73.44887330378599, 'train_f1': 73.31730345580462, 'val_loss': 1.8904619471740722, 'val_top1': 75.76600001464844, 'val_top5': 92.90600000732422, 'val_precision': 76.25185523747608, 'val_recall': 75.768, 'val_f1': 75.41826228324086}
2025-11-07 21:47:09 2025-11-07 21:47:09,730 - INFO - Epoch 082 Summary - LR: 0.002180, Train Loss: 2.7558, Val Loss: 1.8905, Val F1: 75.42%, Val Precision: 76.25%, Val Recall: 75.77%
2025-11-07 21:47:12 2025-11-07 21:47:12,716 - INFO - New best model saved with validation accuracy: 75.766%
2025-11-07 21:47:12 2025-11-07 21:47:12,717 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_083.pth
2025-11-07 21:47:12 Train Epoch 083:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 21:47:16   with torch.cuda.amp.autocast():
2025-11-07 21:47:16 wandb: WARNING Tried to log to step 82 that is less than the current step 207664. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 21:47:18 Train Epoch 083:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.1183, Top1=71.68%, LR=0.002180]2025-11-07 21:47:18,031 - INFO - Step 207666: {'train_loss_batch': 2.118335008621216, 'train_lr': 0.002179562047075466, 'batch_time': 5.312845468521118, 'data_time': 3.6773147583007812}
2025-11-07 21:47:18 Train Epoch 083:   4%|▍         | 100/2502 [02:50<1:05:44,  1.64s/it, Loss=2.7722, Top1=N/A, LR=0.002180]   2025-11-07 21:50:03,039 - INFO - Step 207766: {'train_loss_batch': 2.3971903324127197, 'train_lr': 0.002179562047075466, 'batch_time': 1.6863450248642724, 'data_time': 0.03748135047383828}
2025-11-07 21:50:03 Train Epoch 083:   8%|▊         | 200/2502 [05:35<1:03:02,  1.64s/it, Loss=2.6953, Top1=74.10%, LR=0.002180]2025-11-07 21:52:48,287 - INFO - Step 207866: {'train_loss_batch': 1.9874070882797241, 'train_lr': 0.002179562047075466, 'batch_time': 1.6694938590870567, 'data_time': 0.01932180224366449}
2025-11-07 21:52:48 Train Epoch 083:  12%|█▏        | 300/2502 [08:20<1:00:20,  1.64s/it, Loss=2.7166, Top1=N/A, LR=0.002180]   2025-11-07 21:55:33,008 - INFO - Step 207966: {'train_loss_batch': 4.204412460327148, 'train_lr': 0.002179562047075466, 'batch_time': 1.6620914825173312, 'data_time': 0.013235160282679967}
2025-11-07 21:55:33 Train Epoch 083:  16%|█▌        | 400/2502 [11:05<57:47,  1.65s/it, Loss=2.7507, Top1=N/A, LR=0.002180]2025-11-07 21:58:18,088 - INFO - Step 208066: {'train_loss_batch': 2.052748441696167, 'train_lr': 0.002179562047075466, 'batch_time': 1.659275559713121, 'data_time': 0.010174750687178234}
2025-11-07 21:58:18 Train Epoch 083:  20%|█▉        | 500/2502 [13:50<54:37,  1.64s/it, Loss=2.7571, Top1=N/A, LR=0.002180]2025-11-07 22:01:03,166 - INFO - Step 208166: {'train_loss_batch': 2.089667320251465, 'train_lr': 0.002179562047075466, 'batch_time': 1.6575786672428459, 'data_time': 0.008346417706883597}
2025-11-07 22:01:03 Train Epoch 083:  24%|██▍       | 600/2502 [16:35<52:11,  1.65s/it, Loss=2.7608, Top1=N/A, LR=0.002180]2025-11-07 22:03:47,956 - INFO - Step 208266: {'train_loss_batch': 3.3770692348480225, 'train_lr': 0.002179562047075466, 'batch_time': 1.6559678242726255, 'data_time': 0.007121087310714849}
2025-11-07 22:03:47 Train Epoch 083:  28%|██▊       | 700/2502 [19:20<49:34,  1.65s/it, Loss=2.7497, Top1=N/A, LR=0.002180]2025-11-07 22:06:32,871 - INFO - Step 208366: {'train_loss_batch': 3.424145460128784, 'train_lr': 0.002179562047075466, 'batch_time': 1.6549955145608681, 'data_time': 0.006245454265795829}
2025-11-07 22:06:32 Train Epoch 083:  32%|███▏      | 800/2502 [22:05<47:20,  1.67s/it, Loss=2.7479, Top1=N/A, LR=0.002180]2025-11-07 22:09:18,631 - INFO - Step 208466: {'train_loss_batch': 2.9077155590057373, 'train_lr': 0.002179562047075466, 'batch_time': 1.655320750044824, 'data_time': 0.005591542533274447}
2025-11-07 22:09:18 Train Epoch 083:  36%|███▌      | 900/2502 [24:51<43:50,  1.64s/it, Loss=2.7361, Top1=N/A, LR=0.002180]2025-11-07 22:12:04,067 - INFO - Step 208566: {'train_loss_batch': 3.45188570022583, 'train_lr': 0.002179562047075466, 'batch_time': 1.6552139947999198, 'data_time': 0.00508415262389527}
2025-11-07 22:12:04 Train Epoch 083:  40%|███▉      | 1000/2502 [27:36<41:37,  1.66s/it, Loss=2.7309, Top1=N/A, LR=0.002180]2025-11-07 22:14:49,701 - INFO - Step 208666: {'train_loss_batch': 2.8364479541778564, 'train_lr': 0.002179562047075466, 'batch_time': 1.6553255809056056, 'data_time': 0.00468104202430565}
2025-11-07 22:14:49 Train Epoch 083:  44%|████▍     | 1100/2502 [30:21<38:24,  1.64s/it, Loss=2.7377, Top1=74.11%, LR=0.002180]2025-11-07 22:17:34,288 - INFO - Step 208766: {'train_loss_batch': 2.0063815116882324, 'train_lr': 0.002179562047075466, 'batch_time': 1.654465959680611, 'data_time': 0.004348041142906307}
2025-11-07 22:17:34 Train Epoch 083:  48%|████▊     | 1200/2502 [33:07<36:04,  1.66s/it, Loss=2.7297, Top1=74.06%, LR=0.002180]2025-11-07 22:20:19,749 - INFO - Step 208866: {'train_loss_batch': 1.9975117444992065, 'train_lr': 0.002179562047075466, 'batch_time': 1.6544782251839236, 'data_time': 0.004073145387571718}
2025-11-07 22:20:19 Train Epoch 083:  52%|█████▏    | 1300/2502 [35:52<32:44,  1.63s/it, Loss=2.7256, Top1=N/A, LR=0.002180]   2025-11-07 22:23:04,923 - INFO - Step 208966: {'train_loss_batch': 2.1586689949035645, 'train_lr': 0.002179562047075466, 'batch_time': 1.65426778078629, 'data_time': 0.003840355942745927}
2025-11-07 22:23:04 Train Epoch 083:  56%|█████▌    | 1400/2502 [38:36<30:28,  1.66s/it, Loss=2.7186, Top1=N/A, LR=0.002180]2025-11-07 22:25:49,715 - INFO - Step 209066: {'train_loss_batch': 1.975378155708313, 'train_lr': 0.002179562047075466, 'batch_time': 1.6538143595314978, 'data_time': 0.0036364195603800194}
2025-11-07 22:25:49 Train Epoch 083:  60%|█████▉    | 1500/2502 [41:22<27:41,  1.66s/it, Loss=2.7116, Top1=N/A, LR=0.002180]2025-11-07 22:28:35,573 - INFO - Step 209166: {'train_loss_batch': 3.7235617637634277, 'train_lr': 0.002179562047075466, 'batch_time': 1.6541319396954866, 'data_time': 0.0034658823070488}
2025-11-07 22:28:35 Train Epoch 083:  64%|██████▍   | 1600/2502 [44:08<24:43,  1.64s/it, Loss=2.7163, Top1=N/A, LR=0.002180]2025-11-07 22:31:21,342 - INFO - Step 209266: {'train_loss_batch': 3.250978469848633, 'train_lr': 0.002179562047075466, 'batch_time': 1.6543537486277098, 'data_time': 0.003314876020289748}
2025-11-07 22:31:21 Train Epoch 083:  68%|██████▊   | 1700/2502 [46:52<22:04,  1.65s/it, Loss=2.7102, Top1=N/A, LR=0.002180]2025-11-07 22:34:05,268 - INFO - Step 209366: {'train_loss_batch': 3.490527391433716, 'train_lr': 0.002179562047075466, 'batch_time': 1.6534662622343015, 'data_time': 0.0031777138852989023}
2025-11-07 22:34:05 Train Epoch 083:  72%|███████▏  | 1800/2502 [49:37<19:07,  1.63s/it, Loss=2.7080, Top1=N/A, LR=0.002180]2025-11-07 22:36:50,481 - INFO - Step 209466: {'train_loss_batch': 2.1523241996765137, 'train_lr': 0.002179562047075466, 'batch_time': 1.653391719195394, 'data_time': 0.0030574000325221473}
2025-11-07 22:36:50 Train Epoch 083:  76%|███████▌  | 1900/2502 [52:22<16:31,  1.65s/it, Loss=2.7085, Top1=74.04%, LR=0.002180]2025-11-07 22:39:34,981 - INFO - Step 209566: {'train_loss_batch': 2.0549895763397217, 'train_lr': 0.002179562047075466, 'batch_time': 1.652950456304214, 'data_time': 0.0029456408509199775}
2025-11-07 22:39:34 Train Epoch 083:  80%|███████▉  | 2000/2502 [55:07<13:52,  1.66s/it, Loss=2.7028, Top1=N/A, LR=0.002180]   2025-11-07 22:42:20,399 - INFO - Step 209666: {'train_loss_batch': 4.205538749694824, 'train_lr': 0.002179562047075466, 'batch_time': 1.6530118605781947, 'data_time': 0.0028519202684653155}
2025-11-07 22:42:20 Train Epoch 083:  84%|████████▍ | 2100/2502 [57:53<11:07,  1.66s/it, Loss=2.6984, Top1=N/A, LR=0.002180]2025-11-07 22:45:06,320 - INFO - Step 209766: {'train_loss_batch': 3.2183938026428223, 'train_lr': 0.002179562047075466, 'batch_time': 1.6533067204848975, 'data_time': 0.0027657354065260963}
2025-11-07 22:45:06 Train Epoch 083:  88%|████████▊ | 2200/2502 [1:00:39<08:22,  1.66s/it, Loss=2.7057, Top1=N/A, LR=0.002180]2025-11-07 22:47:52,156 - INFO - Step 209866: {'train_loss_batch': 3.1939597129821777, 'train_lr': 0.002179562047075466, 'batch_time': 1.6535363182161462, 'data_time': 0.0026875899521560357}
2025-11-07 22:47:52 Train Epoch 083:  92%|█████████▏| 2300/2502 [1:03:25<05:35,  1.66s/it, Loss=2.7101, Top1=N/A, LR=0.002180]2025-11-07 22:50:37,979 - INFO - Step 209966: {'train_loss_batch': 3.2335073947906494, 'train_lr': 0.002179562047075466, 'batch_time': 1.6537399926117222, 'data_time': 0.0026146355321851828}
2025-11-07 22:50:37 Train Epoch 083:  96%|█████████▌| 2400/2502 [1:06:10<02:48,  1.65s/it, Loss=2.7117, Top1=N/A, LR=0.002180]2025-11-07 22:53:23,088 - INFO - Step 210066: {'train_loss_batch': 3.869562864303589, 'train_lr': 0.002179562047075466, 'batch_time': 1.653629517763766, 'data_time': 0.002549371536648904}
2025-11-07 22:53:23 Train Epoch 083: 100%|█████████▉| 2500/2502 [1:08:56<00:03,  1.66s/it, Loss=2.7081, Top1=74.02%, LR=0.002180]2025-11-07 22:56:08,993 - INFO - Step 210166: {'train_loss_batch': 2.087587594985962, 'train_lr': 0.002179562047075466, 'batch_time': 1.6538462335707806, 'data_time': 0.0025240442648929197}
2025-11-07 22:56:08 Train Epoch 083: 100%|██████████| 2502/2502 [1:08:58<00:00,  1.65s/it, Loss=2.7081, Top1=74.02%, LR=0.002180]
2025-11-07 22:56:11 Val Epoch 083:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 22:56:15   with torch.cuda.amp.autocast():
2025-11-07 22:56:16 Val Epoch 083: 100%|██████████| 98/98 [01:51<00:00,  1.13s/it, Loss=1.8767, Top1=76.10%, Top5=93.11%]
2025-11-07 22:58:02 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-07 22:58:02   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-07 22:58:02 2025-11-07 22:58:02,563 - INFO - Step 83: {'epoch': 83, 'learning_rate': 0.001673913378561975, 'train_loss': 2.7078164271789964, 'train_top1': 74.02154859767892, 'train_top5': 90.52148815280464, 'train_precision': 73.96546185311917, 'train_recall': 73.90786396707958, 'train_f1': 73.80354538512653, 'val_loss': 1.8766667949676514, 'val_top1': 76.10400001953126, 'val_top5': 93.1100000048828, 'val_precision': 76.47293625262152, 'val_recall': 76.104, 'val_f1': 75.735930863869}
2025-11-07 22:58:02 2025-11-07 22:58:02,564 - INFO - Epoch 083 Summary - LR: 0.001674, Train Loss: 2.7078, Val Loss: 1.8767, Val F1: 75.74%, Val Precision: 76.47%, Val Recall: 76.10%
2025-11-07 22:58:05 2025-11-07 22:58:05,706 - INFO - New best model saved with validation accuracy: 76.104%
2025-11-07 22:58:05 2025-11-07 22:58:05,706 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_084.pth
2025-11-07 22:58:05 Train Epoch 084:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 83 that is less than the current step 210166. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-07 22:58:09 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-07 22:58:09   with torch.cuda.amp.autocast():
2025-11-07 22:58:11 Train Epoch 084:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.2084, Top1=N/A, LR=0.001674]2025-11-07 22:58:11,256 - INFO - Step 210168: {'train_loss_batch': 2.2083606719970703, 'train_lr': 0.001673913378561975, 'batch_time': 5.548312664031982, 'data_time': 3.9041244983673096}
2025-11-07 22:58:11 Train Epoch 084:   4%|▍         | 100/2502 [02:50<1:06:14,  1.65s/it, Loss=2.6269, Top1=N/A, LR=0.001674]2025-11-07 23:00:56,690 - INFO - Step 210268: {'train_loss_batch': 3.376558780670166, 'train_lr': 0.001673913378561975, 'batch_time': 1.6928916237141827, 'data_time': 0.039643006749672464}
2025-11-07 23:00:56 Train Epoch 084:   8%|▊         | 200/2502 [05:37<1:03:37,  1.66s/it, Loss=2.6365, Top1=74.89%, LR=0.001674]2025-11-07 23:03:42,752 - INFO - Step 210368: {'train_loss_batch': 1.8387508392333984, 'train_lr': 0.001673913378561975, 'batch_time': 1.6768375866448701, 'data_time': 0.020428492655208456}
2025-11-07 23:03:42 Train Epoch 084:  12%|█▏        | 300/2502 [08:22<1:01:01,  1.66s/it, Loss=2.6589, Top1=74.90%, LR=0.001674]2025-11-07 23:06:28,645 - INFO - Step 210468: {'train_loss_batch': 1.9855200052261353, 'train_lr': 0.001673913378561975, 'batch_time': 1.6708853221018845, 'data_time': 0.013979117339631648}
2025-11-07 23:06:28 Train Epoch 084:  16%|█▌        | 400/2502 [11:08<58:17,  1.66s/it, Loss=2.6487, Top1=N/A, LR=0.001674]   2025-11-07 23:09:14,359 - INFO - Step 210568: {'train_loss_batch': 3.2441322803497314, 'train_lr': 0.001673913378561975, 'batch_time': 1.6674584355437547, 'data_time': 0.010744038959988335}
2025-11-07 23:09:14 Train Epoch 084:  20%|█▉        | 500/2502 [13:53<54:51,  1.64s/it, Loss=2.6574, Top1=N/A, LR=0.001674]2025-11-07 23:11:59,548 - INFO - Step 210668: {'train_loss_batch': 1.993342638015747, 'train_lr': 0.001673913378561975, 'batch_time': 1.6643492837627967, 'data_time': 0.008798758664768851}
2025-11-07 23:11:59 Train Epoch 084:  24%|██▍       | 600/2502 [16:39<52:34,  1.66s/it, Loss=2.6715, Top1=N/A, LR=0.001674]2025-11-07 23:14:45,253 - INFO - Step 210768: {'train_loss_batch': 3.524740695953369, 'train_lr': 0.001673913378561975, 'batch_time': 1.6631359689049237, 'data_time': 0.0074947300845890396}
2025-11-07 23:14:45 Train Epoch 084:  28%|██▊       | 700/2502 [19:25<49:45,  1.66s/it, Loss=2.6815, Top1=N/A, LR=0.001674]2025-11-07 23:17:31,123 - INFO - Step 210868: {'train_loss_batch': 2.921023368835449, 'train_lr': 0.001673913378561975, 'batch_time': 1.662501633422351, 'data_time': 0.0065699395711683855}
2025-11-07 23:17:31 Train Epoch 084:  32%|███▏      | 800/2502 [22:10<47:03,  1.66s/it, Loss=2.6732, Top1=74.41%, LR=0.001674]2025-11-07 23:20:16,684 - INFO - Step 210968: {'train_loss_batch': 1.9983975887298584, 'train_lr': 0.001673913378561975, 'batch_time': 1.6616408577870192, 'data_time': 0.00588173306687792}
2025-11-07 23:20:16 Train Epoch 084:  36%|███▌      | 900/2502 [24:56<44:15,  1.66s/it, Loss=2.6860, Top1=N/A, LR=0.001674]   2025-11-07 23:23:02,608 - INFO - Step 211068: {'train_loss_batch': 1.931933879852295, 'train_lr': 0.001673913378561975, 'batch_time': 1.6613750619179135, 'data_time': 0.005339825193043157}
2025-11-07 23:23:02 Train Epoch 084:  40%|███▉      | 1000/2502 [27:42<41:28,  1.66s/it, Loss=2.6827, Top1=74.27%, LR=0.001674]2025-11-07 23:25:48,006 - INFO - Step 211168: {'train_loss_batch': 2.073087692260742, 'train_lr': 0.001673913378561975, 'batch_time': 1.6606358791088367, 'data_time': 0.0049067089012214595}
2025-11-07 23:25:48 Train Epoch 084:  44%|████▍     | 1100/2502 [30:27<38:25,  1.64s/it, Loss=2.6840, Top1=N/A, LR=0.001674]   2025-11-07 23:28:33,054 - INFO - Step 211268: {'train_loss_batch': 1.948486089706421, 'train_lr': 0.001673913378561975, 'batch_time': 1.6597136294809285, 'data_time': 0.0045470000396091}
2025-11-07 23:28:33 Train Epoch 084:  48%|████▊     | 1200/2502 [33:12<35:41,  1.65s/it, Loss=2.6795, Top1=N/A, LR=0.001674]2025-11-07 23:31:18,048 - INFO - Step 211368: {'train_loss_batch': 1.9539287090301514, 'train_lr': 0.001673913378561975, 'batch_time': 1.6588992810467698, 'data_time': 0.004247687639145926}
2025-11-07 23:31:18 Train Epoch 084:  52%|█████▏    | 1300/2502 [35:57<33:02,  1.65s/it, Loss=2.6934, Top1=N/A, LR=0.001674]2025-11-07 23:34:03,460 - INFO - Step 211468: {'train_loss_batch': 1.945887565612793, 'train_lr': 0.001673913378561975, 'batch_time': 1.658531641245438, 'data_time': 0.00399871554217093}
2025-11-07 23:34:03 Train Epoch 084:  56%|█████▌    | 1400/2502 [38:42<30:03,  1.64s/it, Loss=2.6953, Top1=N/A, LR=0.001674]2025-11-07 23:36:48,432 - INFO - Step 211568: {'train_loss_batch': 2.0295324325561523, 'train_lr': 0.001673913378561975, 'batch_time': 1.6579026748757972, 'data_time': 0.0037859556591570333}
2025-11-07 23:36:48 Train Epoch 084:  60%|█████▉    | 1500/2502 [41:27<27:37,  1.65s/it, Loss=2.7005, Top1=N/A, LR=0.001674]2025-11-07 23:39:33,627 - INFO - Step 211668: {'train_loss_batch': 3.733301877975464, 'train_lr': 0.001673913378561975, 'batch_time': 1.6575060276727849, 'data_time': 0.0035989481476765327}
2025-11-07 23:39:33 Train Epoch 084:  64%|██████▍   | 1600/2502 [44:13<25:00,  1.66s/it, Loss=2.7106, Top1=N/A, LR=0.001674]2025-11-07 23:42:19,688 - INFO - Step 211768: {'train_loss_batch': 4.195921897888184, 'train_lr': 0.001673913378561975, 'batch_time': 1.6576994975457557, 'data_time': 0.0034432895179095676}
2025-11-07 23:42:19 Train Epoch 084:  68%|██████▊   | 1700/2502 [46:59<22:09,  1.66s/it, Loss=2.7242, Top1=N/A, LR=0.001674]2025-11-07 23:45:05,246 - INFO - Step 211868: {'train_loss_batch': 3.395279884338379, 'train_lr': 0.001673913378561975, 'batch_time': 1.6575750575774841, 'data_time': 0.0033018254028356194}
2025-11-07 23:45:05 Train Epoch 084:  72%|███████▏  | 1800/2502 [49:44<19:14,  1.64s/it, Loss=2.7301, Top1=N/A, LR=0.001674]2025-11-07 23:47:50,562 - INFO - Step 211968: {'train_loss_batch': 2.8775787353515625, 'train_lr': 0.001673913378561975, 'batch_time': 1.6573299219183364, 'data_time': 0.003175704364045867}
2025-11-07 23:47:50 Train Epoch 084:  76%|███████▌  | 1900/2502 [52:30<16:34,  1.65s/it, Loss=2.7348, Top1=N/A, LR=0.001674]2025-11-07 23:50:35,744 - INFO - Step 212068: {'train_loss_batch': 2.0590314865112305, 'train_lr': 0.001673913378561975, 'batch_time': 1.6570398585536743, 'data_time': 0.0030611331434014094}
2025-11-07 23:50:35 Train Epoch 084:  80%|███████▉  | 2000/2502 [55:15<13:55,  1.66s/it, Loss=2.7310, Top1=N/A, LR=0.001674]2025-11-07 23:53:21,405 - INFO - Step 212168: {'train_loss_batch': 3.235103130340576, 'train_lr': 0.001673913378561975, 'batch_time': 1.6570181325934399, 'data_time': 0.002961387400743903}
2025-11-07 23:53:21 Train Epoch 084:  84%|████████▍ | 2100/2502 [58:00<11:02,  1.65s/it, Loss=2.7274, Top1=N/A, LR=0.001674]2025-11-07 23:56:06,695 - INFO - Step 212268: {'train_loss_batch': 4.313133239746094, 'train_lr': 0.001673913378561975, 'batch_time': 1.656821889459718, 'data_time': 0.0028688337960622243}
2025-11-07 23:56:06 Train Epoch 084:  88%|████████▊ | 2200/2502 [1:00:46<08:19,  1.65s/it, Loss=2.7223, Top1=N/A, LR=0.001674]2025-11-07 23:58:52,471 - INFO - Step 212368: {'train_loss_batch': 2.042520523071289, 'train_lr': 0.001673913378561975, 'batch_time': 1.6568647581357838, 'data_time': 0.002787414977139963}
2025-11-07 23:58:52 Train Epoch 084:  92%|█████████▏| 2300/2502 [1:03:31<05:33,  1.65s/it, Loss=2.7219, Top1=74.26%, LR=0.001674]2025-11-08 00:01:37,642 - INFO - Step 212468: {'train_loss_batch': 1.9493839740753174, 'train_lr': 0.001673913378561975, 'batch_time': 1.6566403276865402, 'data_time': 0.0027158355671444544}
2025-11-08 00:01:37 Train Epoch 084:  96%|█████████▌| 2400/2502 [1:06:17<02:49,  1.67s/it, Loss=2.7183, Top1=N/A, LR=0.001674]   2025-11-08 00:04:22,927 - INFO - Step 212568: {'train_loss_batch': 3.4872894287109375, 'train_lr': 0.001673913378561975, 'batch_time': 1.6564824380560847, 'data_time': 0.0026508251461869526}
2025-11-08 00:04:22 Train Epoch 084: 100%|█████████▉| 2500/2502 [1:09:02<00:03,  1.65s/it, Loss=2.7147, Top1=N/A, LR=0.001674]2025-11-08 00:07:08,132 - INFO - Step 212668: {'train_loss_batch': 2.7362732887268066, 'train_lr': 0.001673913378561975, 'batch_time': 1.6563051782193923, 'data_time': 0.0026100655166400237}
2025-11-08 00:07:08 Train Epoch 084: 100%|██████████| 2502/2502 [1:09:04<00:00,  1.66s/it, Loss=2.7147, Top1=N/A, LR=0.001674]
2025-11-08 00:07:10 Val Epoch 084:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-08 00:07:14   with torch.cuda.amp.autocast():
2025-11-08 00:07:15 Val Epoch 084: 100%|██████████| 98/98 [01:49<00:00,  1.12s/it, Loss=1.8755, Top1=76.11%, Top5=93.18%]
2025-11-08 00:08:59 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-08 00:08:59   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-08 00:09:00 2025-11-08 00:08:59,999 - INFO - Step 84: {'epoch': 84, 'learning_rate': 0.0012342791608180722, 'train_loss': 2.7149509552666706, 'train_top1': 74.28701073232324, 'train_top5': 90.63052398989899, 'train_precision': 74.22849726177799, 'train_recall': 74.17432764426295, 'train_f1': 74.0702589391921, 'val_loss': 1.8755403314208985, 'val_top1': 76.10799998779297, 'val_top5': 93.17600000488281, 'val_precision': 76.5444704181147, 'val_recall': 76.112, 'val_f1': 75.75714810951828}
2025-11-08 00:09:00 2025-11-08 00:09:00,001 - INFO - Epoch 084 Summary - LR: 0.001234, Train Loss: 2.7150, Val Loss: 1.8755, Val F1: 75.76%, Val Precision: 76.54%, Val Recall: 76.11%
2025-11-08 00:09:03 2025-11-08 00:09:03,441 - INFO - New best model saved with validation accuracy: 76.108%
2025-11-08 00:09:03 2025-11-08 00:09:03,442 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_085.pth
2025-11-08 00:09:03 Train Epoch 085:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 84 that is less than the current step 212668. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-08 00:09:07 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-08 00:09:07   with torch.cuda.amp.autocast():
2025-11-08 00:09:08 Train Epoch 085:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.5460, Top1=N/A, LR=0.001234]2025-11-08 00:09:08,642 - INFO - Step 212670: {'train_loss_batch': 3.5460457801818848, 'train_lr': 0.0012342791608180722, 'batch_time': 5.196983814239502, 'data_time': 3.5572421550750732}
2025-11-08 00:09:08 Train Epoch 085:   4%|▍         | 100/2502 [02:50<1:06:15,  1.66s/it, Loss=2.7645, Top1=74.48%, LR=0.001234]2025-11-08 00:11:53,660 - INFO - Step 212770: {'train_loss_batch': 2.101391315460205, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6853063484229664, 'data_time': 0.03623759392464515}
2025-11-08 00:11:53 Train Epoch 085:   8%|▊         | 200/2502 [05:34<1:03:24,  1.65s/it, Loss=2.6923, Top1=N/A, LR=0.001234]   2025-11-08 00:14:38,273 - INFO - Step 212870: {'train_loss_batch': 2.0735251903533936, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6658174018954757, 'data_time': 0.01872082847860915}
2025-11-08 00:14:38 Train Epoch 085:  12%|█▏        | 300/2502 [08:20<1:00:38,  1.65s/it, Loss=2.6881, Top1=74.44%, LR=0.001234]2025-11-08 00:17:23,776 - INFO - Step 212970: {'train_loss_batch': 1.9023606777191162, 'train_lr': 0.0012342791608180722, 'batch_time': 1.662231772444969, 'data_time': 0.012843797373217206}
2025-11-08 00:17:23 Train Epoch 085:  16%|█▌        | 400/2502 [11:05<58:05,  1.66s/it, Loss=2.7002, Top1=N/A, LR=0.001234]   2025-11-08 00:20:09,341 - INFO - Step 213070: {'train_loss_batch': 2.039900302886963, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6605900421998745, 'data_time': 0.00990638828039764}
2025-11-08 00:20:09 Train Epoch 085:  20%|█▉        | 500/2502 [13:51<54:33,  1.64s/it, Loss=2.7174, Top1=74.45%, LR=0.001234]2025-11-08 00:22:54,501 - INFO - Step 213170: {'train_loss_batch': 1.9053916931152344, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6587958540507182, 'data_time': 0.008136730232162628}
2025-11-08 00:22:54 Train Epoch 085:  24%|██▍       | 600/2502 [16:34<52:27,  1.65s/it, Loss=2.7162, Top1=74.61%, LR=0.001234]2025-11-08 00:25:38,374 - INFO - Step 213270: {'train_loss_batch': 1.9827632904052734, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6554565171830469, 'data_time': 0.006939073965672447}
2025-11-08 00:25:38 Train Epoch 085:  28%|██▊       | 700/2502 [19:19<49:41,  1.65s/it, Loss=2.7031, Top1=74.59%, LR=0.001234]2025-11-08 00:28:22,799 - INFO - Step 213370: {'train_loss_batch': 1.9901955127716064, 'train_lr': 0.0012342791608180722, 'batch_time': 1.653856972995056, 'data_time': 0.006101462368278123}
2025-11-08 00:28:22 Train Epoch 085:  32%|███▏      | 800/2502 [22:05<47:02,  1.66s/it, Loss=2.7133, Top1=N/A, LR=0.001234]   2025-11-08 00:31:08,588 - INFO - Step 213470: {'train_loss_batch': 1.982022762298584, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6543609447098255, 'data_time': 0.005470088953977816}
2025-11-08 00:31:08 Train Epoch 085:  36%|███▌      | 900/2502 [24:50<44:04,  1.65s/it, Loss=2.7130, Top1=N/A, LR=0.001234]2025-11-08 00:33:54,099 - INFO - Step 213570: {'train_loss_batch': 3.6152215003967285, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6544435913369606, 'data_time': 0.004983870223677251}
2025-11-08 00:33:54 Train Epoch 085:  40%|███▉      | 1000/2502 [27:36<41:09,  1.64s/it, Loss=2.7137, Top1=N/A, LR=0.001234]2025-11-08 00:36:39,525 - INFO - Step 213670: {'train_loss_batch': 3.5789530277252197, 'train_lr': 0.0012342791608180722, 'batch_time': 1.65442511037394, 'data_time': 0.004584229075825298}
2025-11-08 00:36:39 Train Epoch 085:  44%|████▍     | 1100/2502 [30:20<38:24,  1.64s/it, Loss=2.7110, Top1=N/A, LR=0.001234]2025-11-08 00:39:24,378 - INFO - Step 213770: {'train_loss_batch': 2.0120809078216553, 'train_lr': 0.0012342791608180722, 'batch_time': 1.653888954886732, 'data_time': 0.004257602111303623}
2025-11-08 00:39:24 Train Epoch 085:  48%|████▊     | 1200/2502 [33:06<35:37,  1.64s/it, Loss=2.7118, Top1=74.64%, LR=0.001234]2025-11-08 00:42:09,983 - INFO - Step 213870: {'train_loss_batch': 2.0331995487213135, 'train_lr': 0.0012342791608180722, 'batch_time': 1.654068999048276, 'data_time': 0.003987685528325598}
2025-11-08 00:42:09 Train Epoch 085:  52%|█████▏    | 1300/2502 [35:51<33:03,  1.65s/it, Loss=2.7145, Top1=N/A, LR=0.001234]   2025-11-08 00:44:55,064 - INFO - Step 213970: {'train_loss_batch': 2.019286632537842, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6538184347013434, 'data_time': 0.0037656440632238834}
2025-11-08 00:44:55 Train Epoch 085:  56%|█████▌    | 1400/2502 [38:37<30:23,  1.66s/it, Loss=2.7055, Top1=N/A, LR=0.001234]2025-11-08 00:47:40,502 - INFO - Step 214070: {'train_loss_batch': 2.0344839096069336, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6538579355045184, 'data_time': 0.0035723086172644363}
2025-11-08 00:47:40 Train Epoch 085:  60%|█████▉    | 1500/2502 [41:22<27:52,  1.67s/it, Loss=2.7022, Top1=N/A, LR=0.001234]2025-11-08 00:50:26,314 - INFO - Step 214170: {'train_loss_batch': 3.1709580421447754, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6541423294085174, 'data_time': 0.003405364333272854}
2025-11-08 00:50:26 Train Epoch 085:  64%|██████▍   | 1600/2502 [44:07<24:50,  1.65s/it, Loss=2.7030, Top1=N/A, LR=0.001234]2025-11-08 00:53:11,362 - INFO - Step 214270: {'train_loss_batch': 4.107771873474121, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6539132566767734, 'data_time': 0.0032600077296107504}
2025-11-08 00:53:11 Train Epoch 085:  68%|██████▊   | 1700/2502 [46:52<21:52,  1.64s/it, Loss=2.7085, Top1=N/A, LR=0.001234]2025-11-08 00:55:56,216 - INFO - Step 214370: {'train_loss_batch': 2.0064680576324463, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6535971278236026, 'data_time': 0.003130684173646778}
2025-11-08 00:55:56 Train Epoch 085:  72%|███████▏  | 1800/2502 [49:37<19:17,  1.65s/it, Loss=2.7139, Top1=N/A, LR=0.001234]2025-11-08 00:58:41,285 - INFO - Step 214470: {'train_loss_batch': 4.439206123352051, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6534357652076412, 'data_time': 0.003014035121657198}
2025-11-08 00:58:41 Train Epoch 085:  76%|███████▌  | 1900/2502 [52:23<16:35,  1.65s/it, Loss=2.7073, Top1=74.56%, LR=0.001234]2025-11-08 01:01:26,594 - INFO - Step 214570: {'train_loss_batch': 2.0153613090515137, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6534172650828354, 'data_time': 0.0029110049398744313}
2025-11-08 01:01:26 Train Epoch 085:  80%|███████▉  | 2000/2502 [55:08<13:52,  1.66s/it, Loss=2.7079, Top1=N/A, LR=0.001234]   2025-11-08 01:04:11,862 - INFO - Step 214670: {'train_loss_batch': 2.0442731380462646, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6533804200757212, 'data_time': 0.0028191237137473743}
2025-11-08 01:04:11 Train Epoch 085:  84%|████████▍ | 2100/2502 [57:54<11:05,  1.65s/it, Loss=2.7098, Top1=N/A, LR=0.001234]2025-11-08 01:06:57,458 - INFO - Step 214770: {'train_loss_batch': 3.4056029319763184, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6535029895869622, 'data_time': 0.0027347767370079655}
2025-11-08 01:06:57 Train Epoch 085:  88%|████████▊ | 2200/2502 [1:00:39<08:21,  1.66s/it, Loss=2.7056, Top1=N/A, LR=0.001234]2025-11-08 01:09:43,099 - INFO - Step 214870: {'train_loss_batch': 2.0890698432922363, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6536350965174909, 'data_time': 0.002659823774262376}
2025-11-08 01:09:43 Train Epoch 085:  92%|█████████▏| 2300/2502 [1:03:24<05:33,  1.65s/it, Loss=2.7041, Top1=N/A, LR=0.001234]2025-11-08 01:12:28,093 - INFO - Step 214970: {'train_loss_batch': 3.5908336639404297, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6534741171439384, 'data_time': 0.002590677728864329}
2025-11-08 01:12:28 Train Epoch 085:  96%|█████████▌| 2400/2502 [1:06:09<02:48,  1.65s/it, Loss=2.7114, Top1=N/A, LR=0.001234]2025-11-08 01:15:13,094 - INFO - Step 215070: {'train_loss_batch': 2.0647337436676025, 'train_lr': 0.0012342791608180722, 'batch_time': 1.6533298037639015, 'data_time': 0.002527297411993711}
2025-11-08 01:15:13 Train Epoch 085: 100%|█████████▉| 2500/2502 [1:08:54<00:03,  1.66s/it, Loss=2.7104, Top1=74.67%, LR=0.001234]2025-11-08 01:17:58,416 - INFO - Step 215170: {'train_loss_batch': 1.8511099815368652, 'train_lr': 0.0012342791608180722, 'batch_time': 1.653325618624163, 'data_time': 0.002491362330342521}
2025-11-08 01:17:58 Train Epoch 085: 100%|██████████| 2502/2502 [1:08:56<00:00,  1.65s/it, Loss=2.7104, Top1=74.67%, LR=0.001234]
2025-11-08 01:18:00 Val Epoch 085:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-08 01:18:05   with torch.cuda.amp.autocast():
2025-11-08 01:18:05 Val Epoch 085: 100%|██████████| 98/98 [01:49<00:00,  1.12s/it, Loss=1.8624, Top1=76.34%, Top5=93.27%]
2025-11-08 01:19:50 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-08 01:19:50   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-08 01:19:50 2025-11-08 01:19:50,245 - INFO - Step 85: {'epoch': 85, 'learning_rate': 0.0008612598813033303, 'train_loss': 2.71009428123776, 'train_top1': 74.67793367346938, 'train_top5': 90.83864795918367, 'train_precision': 74.636143077216, 'train_recall': 74.5832573979874, 'train_f1': 74.47540998237444, 'val_loss': 1.8623979119491578, 'val_top1': 76.34199999267578, 'val_top5': 93.27400000732422, 'val_precision': 76.71241055691958, 'val_recall': 76.34, 'val_f1': 75.97819554929009}
2025-11-08 01:19:50 2025-11-08 01:19:50,246 - INFO - Epoch 085 Summary - LR: 0.000861, Train Loss: 2.7101, Val Loss: 1.8624, Val F1: 75.98%, Val Precision: 76.71%, Val Recall: 76.34%
2025-11-08 01:19:53 2025-11-08 01:19:53,228 - INFO - New best model saved with validation accuracy: 76.342%
2025-11-08 01:19:53 2025-11-08 01:19:53,229 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_086.pth
2025-11-08 01:19:53 Train Epoch 086:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-08 01:19:56   with torch.cuda.amp.autocast():
2025-11-08 01:19:57 wandb: WARNING Tried to log to step 85 that is less than the current step 215170. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-08 01:19:58 Train Epoch 086:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.0365, Top1=N/A, LR=0.000861]2025-11-08 01:19:58,499 - INFO - Step 215172: {'train_loss_batch': 2.0365307331085205, 'train_lr': 0.0008612598813033303, 'batch_time': 5.267645835876465, 'data_time': 3.6196444034576416}
2025-11-08 01:19:58 Train Epoch 086:   4%|▍         | 100/2502 [02:50<1:06:18,  1.66s/it, Loss=2.7498, Top1=N/A, LR=0.000861]2025-11-08 01:22:44,045 - INFO - Step 215272: {'train_loss_batch': 3.465386390686035, 'train_lr': 0.0008612598813033303, 'batch_time': 1.691227934148052, 'data_time': 0.036855650420236113}
2025-11-08 01:22:44 Train Epoch 086:   8%|▊         | 200/2502 [05:36<1:03:18,  1.65s/it, Loss=2.7505, Top1=N/A, LR=0.000861]2025-11-08 01:25:29,702 - INFO - Step 215372: {'train_loss_batch': 1.9789429903030396, 'train_lr': 0.0008612598813033303, 'batch_time': 1.673988201131868, 'data_time': 0.019034974017546544}
2025-11-08 01:25:29 Train Epoch 086:  12%|█▏        | 300/2502 [08:21<1:00:12,  1.64s/it, Loss=2.7309, Top1=75.18%, LR=0.000861]2025-11-08 01:28:15,076 - INFO - Step 215472: {'train_loss_batch': 1.9848825931549072, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6672584384778806, 'data_time': 0.013048336751437267}
2025-11-08 01:28:15 Train Epoch 086:  16%|█▌        | 400/2502 [11:06<58:19,  1.66s/it, Loss=2.7220, Top1=N/A, LR=0.000861]   2025-11-08 01:30:59,842 - INFO - Step 215572: {'train_loss_batch': 2.780501127243042, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6623710063924813, 'data_time': 0.010042205415759004}
2025-11-08 01:30:59 Train Epoch 086:  20%|█▉        | 500/2502 [13:51<55:23,  1.66s/it, Loss=2.7078, Top1=75.00%, LR=0.000861]2025-11-08 01:33:44,933 - INFO - Step 215672: {'train_loss_batch': 2.029630184173584, 'train_lr': 0.0008612598813033303, 'batch_time': 1.660082739984204, 'data_time': 0.008227721421780463}
2025-11-08 01:33:44 Train Epoch 086:  24%|██▍       | 600/2502 [16:36<52:17,  1.65s/it, Loss=2.7184, Top1=N/A, LR=0.000861]   2025-11-08 01:36:30,001 - INFO - Step 215772: {'train_loss_batch': 1.9616807699203491, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6585180747528838, 'data_time': 0.00702800528578671}
2025-11-08 01:36:30 Train Epoch 086:  28%|██▊       | 700/2502 [19:21<49:36,  1.65s/it, Loss=2.7246, Top1=N/A, LR=0.000861]2025-11-08 01:39:15,134 - INFO - Step 215872: {'train_loss_batch': 1.9641640186309814, 'train_lr': 0.0008612598813033303, 'batch_time': 1.657491911835065, 'data_time': 0.006173723944583735}
2025-11-08 01:39:15 Train Epoch 086:  32%|███▏      | 800/2502 [22:07<46:53,  1.65s/it, Loss=2.7553, Top1=N/A, LR=0.000861]2025-11-08 01:42:00,957 - INFO - Step 215972: {'train_loss_batch': 1.9751996994018555, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6575843359200695, 'data_time': 0.005536686019802213}
2025-11-08 01:42:00 Train Epoch 086:  36%|███▌      | 900/2502 [24:52<43:55,  1.65s/it, Loss=2.7353, Top1=N/A, LR=0.000861]2025-11-08 01:44:45,763 - INFO - Step 216072: {'train_loss_batch': 3.368457794189453, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6565268174657282, 'data_time': 0.005032271576774504}
2025-11-08 01:44:45 Train Epoch 086:  40%|███▉      | 1000/2502 [27:38<41:22,  1.65s/it, Loss=2.7389, Top1=N/A, LR=0.000861]2025-11-08 01:47:31,309 - INFO - Step 216172: {'train_loss_batch': 4.170372009277344, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6564196809069378, 'data_time': 0.004642520393882241}
2025-11-08 01:47:31 Train Epoch 086:  44%|████▍     | 1100/2502 [30:23<38:46,  1.66s/it, Loss=2.7431, Top1=N/A, LR=0.000861]2025-11-08 01:50:17,035 - INFO - Step 216272: {'train_loss_batch': 1.9506834745407104, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6564958452853584, 'data_time': 0.004315277103074132}
2025-11-08 01:50:17 Train Epoch 086:  48%|████▊     | 1200/2502 [33:08<36:04,  1.66s/it, Loss=2.7458, Top1=N/A, LR=0.000861]2025-11-08 01:53:01,915 - INFO - Step 216372: {'train_loss_batch': 2.735476016998291, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6558550847360831, 'data_time': 0.004045566849466367}
2025-11-08 01:53:01 Train Epoch 086:  52%|█████▏    | 1300/2502 [35:54<33:13,  1.66s/it, Loss=2.7483, Top1=N/A, LR=0.000861]2025-11-08 01:55:47,833 - INFO - Step 216472: {'train_loss_batch': 2.060436725616455, 'train_lr': 0.0008612598813033303, 'batch_time': 1.656110218907209, 'data_time': 0.003814902514517078}
2025-11-08 01:55:47 Train Epoch 086:  56%|█████▌    | 1400/2502 [38:40<30:29,  1.66s/it, Loss=2.7388, Top1=N/A, LR=0.000861]2025-11-08 01:58:33,291 - INFO - Step 216572: {'train_loss_batch': 1.917732834815979, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6560009337594728, 'data_time': 0.003620135111948322}
2025-11-08 01:58:33 Train Epoch 086:  60%|█████▉    | 1500/2502 [41:25<27:43,  1.66s/it, Loss=2.7363, Top1=74.82%, LR=0.000861]2025-11-08 02:01:19,119 - INFO - Step 216672: {'train_loss_batch': 2.1137490272521973, 'train_lr': 0.0008612598813033303, 'batch_time': 1.656152227574551, 'data_time': 0.0034493940977316393}
2025-11-08 02:01:19 Train Epoch 086:  64%|██████▍   | 1600/2502 [44:11<24:57,  1.66s/it, Loss=2.7279, Top1=N/A, LR=0.000861]   2025-11-08 02:04:05,066 - INFO - Step 216772: {'train_loss_batch': 2.222379207611084, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6563596527402211, 'data_time': 0.003300035990155689}
2025-11-08 02:04:05 Train Epoch 086:  68%|██████▊   | 1700/2502 [46:57<22:05,  1.65s/it, Loss=2.7327, Top1=N/A, LR=0.000861]2025-11-08 02:06:50,668 - INFO - Step 216872: {'train_loss_batch': 3.4228405952453613, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6563398320838887, 'data_time': 0.003167564906771221}
2025-11-08 02:06:50 Train Epoch 086:  72%|███████▏  | 1800/2502 [49:42<19:25,  1.66s/it, Loss=2.7342, Top1=74.90%, LR=0.000861]2025-11-08 02:09:35,821 - INFO - Step 216972: {'train_loss_batch': 2.046600818634033, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6560727167897327, 'data_time': 0.003053199179234735}
2025-11-08 02:09:35 Train Epoch 086:  76%|███████▌  | 1900/2502 [52:28<16:39,  1.66s/it, Loss=2.7297, Top1=N/A, LR=0.000861]   2025-11-08 02:12:21,988 - INFO - Step 217072: {'train_loss_batch': 3.520832061767578, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6563670235141712, 'data_time': 0.00295014296124823}
2025-11-08 02:12:21 Train Epoch 086:  80%|███████▉  | 2000/2502 [55:13<13:50,  1.65s/it, Loss=2.7274, Top1=N/A, LR=0.000861]2025-11-08 02:15:06,835 - INFO - Step 217172: {'train_loss_batch': 4.3163251876831055, 'train_lr': 0.0008612598813033303, 'batch_time': 1.655971718096602, 'data_time': 0.0028538538300353607}
2025-11-08 02:15:06 Train Epoch 086:  84%|████████▍ | 2100/2502 [57:59<11:03,  1.65s/it, Loss=2.7225, Top1=N/A, LR=0.000861]2025-11-08 02:17:52,688 - INFO - Step 217272: {'train_loss_batch': 2.784196615219116, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6560934451238023, 'data_time': 0.0027727228525080945}
2025-11-08 02:17:52 Train Epoch 086:  88%|████████▊ | 2200/2502 [1:00:44<08:16,  1.64s/it, Loss=2.7224, Top1=N/A, LR=0.000861]2025-11-08 02:20:38,034 - INFO - Step 217372: {'train_loss_batch': 2.2682113647460938, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6559739815869259, 'data_time': 0.0026930995769578726}
2025-11-08 02:20:38 Train Epoch 086:  92%|█████████▏| 2300/2502 [1:03:30<05:35,  1.66s/it, Loss=2.7216, Top1=N/A, LR=0.000861]2025-11-08 02:23:23,771 - INFO - Step 217472: {'train_loss_batch': 3.467271327972412, 'train_lr': 0.0008612598813033303, 'batch_time': 1.6560343315890436, 'data_time': 0.002622879572092891}
2025-11-08 02:23:23 Train Epoch 086:  96%|█████████▌| 2400/2502 [1:06:15<02:47,  1.64s/it, Loss=2.7194, Top1=N/A, LR=0.000861]2025-11-08 02:26:08,901 - INFO - Step 217572: {'train_loss_batch': 2.1823291778564453, 'train_lr': 0.0008612598813033303, 'batch_time': 1.655837129821682, 'data_time': 0.002557767922061029}
2025-11-08 02:26:08 Train Epoch 086: 100%|█████████▉| 2500/2502 [1:09:00<00:03,  1.65s/it, Loss=2.7204, Top1=N/A, LR=0.000861]2025-11-08 02:28:54,049 - INFO - Step 217672: {'train_loss_batch': 3.363070011138916, 'train_lr': 0.0008612598813033303, 'batch_time': 1.655662930521761, 'data_time': 0.002520330044709411}
2025-11-08 02:28:54 Train Epoch 086: 100%|██████████| 2502/2502 [1:09:02<00:00,  1.66s/it, Loss=2.7204, Top1=N/A, LR=0.000861]
2025-11-08 02:28:56 Val Epoch 086:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-08 02:29:00   with torch.cuda.amp.autocast():
2025-11-08 02:29:01 Val Epoch 086: 100%|██████████| 98/98 [01:49<00:00,  1.12s/it, Loss=1.8714, Top1=76.46%, Top5=93.31%]
2025-11-08 02:30:46 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-08 02:30:46   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-08 02:30:46 2025-11-08 02:30:46,296 - INFO - Step 86: {'epoch': 86, 'learning_rate': 0.0005553650394845579, 'train_loss': 2.720392617771475, 'train_top1': 74.88231169871794, 'train_top5': 90.87915665064102, 'train_precision': 74.81923599505163, 'train_recall': 74.78591393088247, 'train_f1': 74.65972945019546, 'val_loss': 1.871428917388916, 'val_top1': 76.46000001708984, 'val_top5': 93.30599997802734, 'val_precision': 76.76971365908497, 'val_recall': 76.458, 'val_f1': 76.08873641706711}
2025-11-08 02:30:46 2025-11-08 02:30:46,298 - INFO - Epoch 086 Summary - LR: 0.000555, Train Loss: 2.7204, Val Loss: 1.8714, Val F1: 76.09%, Val Precision: 76.77%, Val Recall: 76.46%
2025-11-08 02:30:47 wandb: WARNING Tried to log to step 86 that is less than the current step 217672. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-08 02:30:50 2025-11-08 02:30:50,017 - INFO - New best model saved with validation accuracy: 76.460%
2025-11-08 02:30:50 2025-11-08 02:30:50,018 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_087.pth
2025-11-08 02:30:50 Train Epoch 087:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-08 02:30:54   with torch.cuda.amp.autocast():
2025-11-08 02:30:55 Train Epoch 087:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=2.0758, Top1=71.68%, LR=0.000555]2025-11-08 02:30:55,734 - INFO - Step 217674: {'train_loss_batch': 2.0757572650909424, 'train_lr': 0.0005553650394845579, 'batch_time': 5.714801549911499, 'data_time': 4.071780443191528}
2025-11-08 02:30:55 Train Epoch 087:   4%|▍         | 100/2502 [02:51<1:06:34,  1.66s/it, Loss=2.5872, Top1=74.66%, LR=0.000555]2025-11-08 02:33:41,447 - INFO - Step 217774: {'train_loss_batch': 1.9458744525909424, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6973015034552847, 'data_time': 0.04135469398876228}
2025-11-08 02:33:41 Train Epoch 087:   8%|▊         | 200/2502 [05:37<1:03:42,  1.66s/it, Loss=2.6363, Top1=N/A, LR=0.000555]   2025-11-08 02:36:27,611 - INFO - Step 217874: {'train_loss_batch': 2.166691780090332, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6795585226656786, 'data_time': 0.021323572936935806}
2025-11-08 02:36:27 Train Epoch 087:  12%|█▏        | 300/2502 [08:23<1:00:42,  1.65s/it, Loss=2.6853, Top1=74.78%, LR=0.000555]2025-11-08 02:39:13,533 - INFO - Step 217974: {'train_loss_batch': 2.076401948928833, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6728008522147753, 'data_time': 0.014606672267977185}
2025-11-08 02:39:13 Train Epoch 087:  16%|█▌        | 400/2502 [11:09<58:04,  1.66s/it, Loss=2.7215, Top1=N/A, LR=0.000555]   2025-11-08 02:41:59,225 - INFO - Step 218074: {'train_loss_batch': 3.5856990814208984, 'train_lr': 0.0005553650394845579, 'batch_time': 1.668841664036016, 'data_time': 0.01122756016224697}
2025-11-08 02:41:59 Train Epoch 087:  20%|█▉        | 500/2502 [13:54<55:21,  1.66s/it, Loss=2.7210, Top1=N/A, LR=0.000555]2025-11-08 02:44:44,614 - INFO - Step 218174: {'train_loss_batch': 3.4771060943603516, 'train_lr': 0.0005553650394845579, 'batch_time': 1.66585712899229, 'data_time': 0.009194361711452582}
2025-11-08 02:44:44 Train Epoch 087:  24%|██▍       | 600/2502 [16:39<52:31,  1.66s/it, Loss=2.7156, Top1=N/A, LR=0.000555]2025-11-08 02:47:29,820 - INFO - Step 218274: {'train_loss_batch': 2.131010055541992, 'train_lr': 0.0005553650394845579, 'batch_time': 1.66355942212008, 'data_time': 0.007833362220726076}
2025-11-08 02:47:29 Train Epoch 087:  28%|██▊       | 700/2502 [19:25<49:48,  1.66s/it, Loss=2.6964, Top1=N/A, LR=0.000555]2025-11-08 02:50:15,940 - INFO - Step 218374: {'train_loss_batch': 3.465049982070923, 'train_lr': 0.0005553650394845579, 'batch_time': 1.663223246875061, 'data_time': 0.006870919389493455}
2025-11-08 02:50:15 Train Epoch 087:  32%|███▏      | 800/2502 [22:11<46:33,  1.64s/it, Loss=2.7089, Top1=N/A, LR=0.000555]2025-11-08 02:53:01,218 - INFO - Step 218474: {'train_loss_batch': 3.58503794670105, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6619195045156872, 'data_time': 0.006143644954381364}
2025-11-08 02:53:01 Train Epoch 087:  36%|███▌      | 900/2502 [24:56<44:04,  1.65s/it, Loss=2.6956, Top1=N/A, LR=0.000555]2025-11-08 02:55:46,151 - INFO - Step 218574: {'train_loss_batch': 2.2874526977539062, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6605217234540595, 'data_time': 0.0055736354929493215}
2025-11-08 02:55:46 Train Epoch 087:  40%|███▉      | 1000/2502 [27:41<41:14,  1.65s/it, Loss=2.6894, Top1=N/A, LR=0.000555]2025-11-08 02:58:31,638 - INFO - Step 218674: {'train_loss_batch': 1.8894463777542114, 'train_lr': 0.0005553650394845579, 'batch_time': 1.659957483455494, 'data_time': 0.00512141519254976}
2025-11-08 02:58:31 Train Epoch 087:  44%|████▍     | 1100/2502 [30:26<38:42,  1.66s/it, Loss=2.6884, Top1=N/A, LR=0.000555]2025-11-08 03:01:16,697 - INFO - Step 218774: {'train_loss_batch': 3.1078522205352783, 'train_lr': 0.0005553650394845579, 'batch_time': 1.659106332318118, 'data_time': 0.004746992516582603}
2025-11-08 03:01:16 Train Epoch 087:  48%|████▊     | 1200/2502 [33:10<35:48,  1.65s/it, Loss=2.6835, Top1=N/A, LR=0.000555]2025-11-08 03:04:00,666 - INFO - Step 218874: {'train_loss_batch': 4.216282844543457, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6574890814057794, 'data_time': 0.004438604145224743}
2025-11-08 03:04:00 Train Epoch 087:  52%|█████▏    | 1300/2502 [35:56<33:18,  1.66s/it, Loss=2.6805, Top1=N/A, LR=0.000555]2025-11-08 03:06:46,441 - INFO - Step 218974: {'train_loss_batch': 3.4184412956237793, 'train_lr': 0.0005553650394845579, 'batch_time': 1.657508926332959, 'data_time': 0.004179321189370181}
2025-11-08 03:06:46 Train Epoch 087:  56%|█████▌    | 1400/2502 [38:41<30:31,  1.66s/it, Loss=2.6835, Top1=N/A, LR=0.000555]2025-11-08 03:09:31,704 - INFO - Step 219074: {'train_loss_batch': 2.785137176513672, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6571609083879513, 'data_time': 0.0039561090939050055}
2025-11-08 03:09:31 Train Epoch 087:  60%|█████▉    | 1500/2502 [41:27<27:26,  1.64s/it, Loss=2.6857, Top1=74.94%, LR=0.000555]2025-11-08 03:12:17,446 - INFO - Step 219174: {'train_loss_batch': 2.070225954055786, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6571779340048935, 'data_time': 0.00376056814733781}
2025-11-08 03:12:17 Train Epoch 087:  64%|██████▍   | 1600/2502 [44:10<24:38,  1.64s/it, Loss=2.6842, Top1=N/A, LR=0.000555]   2025-11-08 03:15:00,795 - INFO - Step 219274: {'train_loss_batch': 2.782301902770996, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6556979083478787, 'data_time': 0.003584275314169031}
2025-11-08 03:15:00 Train Epoch 087:  68%|██████▊   | 1700/2502 [46:56<22:09,  1.66s/it, Loss=2.6820, Top1=N/A, LR=0.000555]2025-11-08 03:17:46,680 - INFO - Step 219374: {'train_loss_batch': 2.401219606399536, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6558835310209925, 'data_time': 0.0034336051402689078}
2025-11-08 03:17:46 Train Epoch 087:  72%|███████▏  | 1800/2502 [49:41<19:23,  1.66s/it, Loss=2.6797, Top1=N/A, LR=0.000555]2025-11-08 03:20:31,844 - INFO - Step 219474: {'train_loss_batch': 2.905811309814453, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6556478367455465, 'data_time': 0.0033007214825263227}
2025-11-08 03:20:31 Train Epoch 087:  76%|███████▌  | 1900/2502 [52:27<16:36,  1.65s/it, Loss=2.6757, Top1=N/A, LR=0.000555]2025-11-08 03:23:17,192 - INFO - Step 219574: {'train_loss_batch': 1.9153133630752563, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6555332314773712, 'data_time': 0.0031832242751986903}
2025-11-08 03:23:17 Train Epoch 087:  80%|███████▉  | 2000/2502 [55:12<13:50,  1.65s/it, Loss=2.6754, Top1=74.95%, LR=0.000555]2025-11-08 03:26:02,626 - INFO - Step 219674: {'train_loss_batch': 2.0427770614624023, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6554736280846394, 'data_time': 0.0030743213131212105}
2025-11-08 03:26:02 Train Epoch 087:  84%|████████▍ | 2100/2502 [57:58<11:02,  1.65s/it, Loss=2.6769, Top1=N/A, LR=0.000555]   2025-11-08 03:28:48,214 - INFO - Step 219774: {'train_loss_batch': 3.814971685409546, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6554929817477502, 'data_time': 0.0029774112737729172}
2025-11-08 03:28:48 Train Epoch 087:  88%|████████▊ | 2200/2502 [1:00:43<08:19,  1.65s/it, Loss=2.6760, Top1=74.99%, LR=0.000555]2025-11-08 03:31:33,239 - INFO - Step 219874: {'train_loss_batch': 1.9221792221069336, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6552544004101475, 'data_time': 0.0028919925585707334}
2025-11-08 03:31:33 Train Epoch 087:  92%|█████████▏| 2300/2502 [1:03:28<05:36,  1.66s/it, Loss=2.6745, Top1=N/A, LR=0.000555]   2025-11-08 03:34:18,510 - INFO - Step 219974: {'train_loss_batch': 3.4036827087402344, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6551438677264316, 'data_time': 0.0028111736756829997}
2025-11-08 03:34:18 Train Epoch 087:  96%|█████████▌| 2400/2502 [1:06:14<02:49,  1.66s/it, Loss=2.6760, Top1=N/A, LR=0.000555]2025-11-08 03:37:04,203 - INFO - Step 220074: {'train_loss_batch': 3.521620273590088, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6552182284557337, 'data_time': 0.0027376724649895634}
2025-11-08 03:37:04 Train Epoch 087: 100%|█████████▉| 2500/2502 [1:08:59<00:03,  1.64s/it, Loss=2.6739, Top1=N/A, LR=0.000555]2025-11-08 03:39:49,447 - INFO - Step 220174: {'train_loss_batch': 1.916327714920044, 'train_lr': 0.0005553650394845579, 'batch_time': 1.6551070666132046, 'data_time': 0.002707746685719023}
2025-11-08 03:39:49 Train Epoch 087: 100%|██████████| 2502/2502 [1:09:01<00:00,  1.66s/it, Loss=2.6739, Top1=N/A, LR=0.000555]
2025-11-08 03:39:51 Val Epoch 087:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-08 03:39:56   with torch.cuda.amp.autocast():
2025-11-08 03:39:56 Val Epoch 087: 100%|██████████| 98/98 [01:50<00:00,  1.12s/it, Loss=1.8400, Top1=76.71%, Top5=93.49%]
2025-11-08 03:41:41 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-08 03:41:41   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-08 03:41:41 2025-11-08 03:41:41,923 - INFO - Step 87: {'epoch': 87, 'learning_rate': 0.00031701245092079596, 'train_loss': 2.6736594151726347, 'train_top1': 75.01860119047619, 'train_top5': 90.9772094979296, 'train_precision': 74.96108237255858, 'train_recall': 74.94216485496558, 'train_f1': 74.81778001890866, 'val_loss': 1.840041690864563, 'val_top1': 76.71400001708984, 'val_top5': 93.48999997802734, 'val_precision': 77.02121884230752, 'val_recall': 76.71, 'val_f1': 76.36141037508138}
2025-11-08 03:41:41 2025-11-08 03:41:41,925 - INFO - Epoch 087 Summary - LR: 0.000317, Train Loss: 2.6737, Val Loss: 1.8400, Val F1: 76.36%, Val Precision: 77.02%, Val Recall: 76.71%
2025-11-08 03:41:45 2025-11-08 03:41:45,122 - INFO - New best model saved with validation accuracy: 76.714%
2025-11-08 03:41:45 2025-11-08 03:41:45,123 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_088.pth
2025-11-08 03:41:45 Train Epoch 088:   0%|          | 0/2502 [00:00<?, ?it/s]wandb: WARNING Tried to log to step 87 that is less than the current step 220174. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-08 03:41:48 /home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-08 03:41:48   with torch.cuda.amp.autocast():
2025-11-08 03:41:50 Train Epoch 088:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=3.4864, Top1=N/A, LR=0.000317]2025-11-08 03:41:50,586 - INFO - Step 220176: {'train_loss_batch': 3.486394166946411, 'train_lr': 0.00031701245092079596, 'batch_time': 5.4612414836883545, 'data_time': 3.8254244327545166}
2025-11-08 03:41:50 Train Epoch 088:   4%|▍         | 100/2502 [02:51<1:05:48,  1.64s/it, Loss=2.6780, Top1=N/A, LR=0.000317]2025-11-08 03:44:36,247 - INFO - Step 220276: {'train_loss_batch': 1.899126648902893, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6942880035627006, 'data_time': 0.03894505406370257}
2025-11-08 03:44:36 Train Epoch 088:   8%|▊         | 200/2502 [05:36<1:03:16,  1.65s/it, Loss=2.7067, Top1=N/A, LR=0.000317]2025-11-08 03:47:21,434 - INFO - Step 220376: {'train_loss_batch': 2.0089900493621826, 'train_lr': 0.00031701245092079596, 'batch_time': 1.673181720040924, 'data_time': 0.020065280335459543}
2025-11-08 03:47:21 Train Epoch 088:  12%|█▏        | 300/2502 [08:21<1:00:44,  1.66s/it, Loss=2.6899, Top1=75.54%, LR=0.000317]2025-11-08 03:50:06,569 - INFO - Step 220476: {'train_loss_batch': 1.944178581237793, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6659292778699502, 'data_time': 0.013728472877578482}
2025-11-08 03:50:06 Train Epoch 088:  16%|█▌        | 400/2502 [11:07<57:47,  1.65s/it, Loss=2.6938, Top1=N/A, LR=0.000317]   2025-11-08 03:52:52,154 - INFO - Step 220576: {'train_loss_batch': 2.989011764526367, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6634127386193025, 'data_time': 0.010570968475722316}
2025-11-08 03:52:52 Train Epoch 088:  20%|█▉        | 500/2502 [13:52<54:50,  1.64s/it, Loss=2.6828, Top1=N/A, LR=0.000317]2025-11-08 03:55:37,686 - INFO - Step 220676: {'train_loss_batch': 1.965452790260315, 'train_lr': 0.00031701245092079596, 'batch_time': 1.661796737335875, 'data_time': 0.008660159425107304}
2025-11-08 03:55:37 Train Epoch 088:  24%|██▍       | 600/2502 [16:37<52:39,  1.66s/it, Loss=2.6801, Top1=N/A, LR=0.000317]2025-11-08 03:58:23,082 - INFO - Step 220776: {'train_loss_batch': 3.2127695083618164, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6604927534271596, 'data_time': 0.007383033955553407}
2025-11-08 03:58:23 Train Epoch 088:  28%|██▊       | 700/2502 [19:22<49:10,  1.64s/it, Loss=2.6573, Top1=N/A, LR=0.000317]2025-11-08 04:01:07,568 - INFO - Step 220876: {'train_loss_batch': 2.3339927196502686, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6582622793363606, 'data_time': 0.006474402423591995}
2025-11-08 04:01:07 Train Epoch 088:  32%|███▏      | 800/2502 [22:07<46:52,  1.65s/it, Loss=2.6524, Top1=N/A, LR=0.000317]2025-11-08 04:03:52,509 - INFO - Step 220976: {'train_loss_batch': 2.0027694702148438, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6571575869037567, 'data_time': 0.005793995327419705}
2025-11-08 04:03:52 Train Epoch 088:  36%|███▌      | 900/2502 [24:52<44:21,  1.66s/it, Loss=2.6504, Top1=N/A, LR=0.000317]2025-11-08 04:06:38,026 - INFO - Step 221076: {'train_loss_batch': 3.4160351753234863, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6569362520245416, 'data_time': 0.005258201891256622}
2025-11-08 04:06:38 Train Epoch 088:  40%|███▉      | 1000/2502 [27:38<41:26,  1.66s/it, Loss=2.6552, Top1=75.23%, LR=0.000317]2025-11-08 04:09:23,227 - INFO - Step 221176: {'train_loss_batch': 1.9210909605026245, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6564440620052707, 'data_time': 0.004830696485140226}
2025-11-08 04:09:23 Train Epoch 088:  44%|████▍     | 1100/2502 [30:23<38:52,  1.66s/it, Loss=2.6600, Top1=N/A, LR=0.000317]   2025-11-08 04:12:08,992 - INFO - Step 221276: {'train_loss_batch': 3.3740668296813965, 'train_lr': 0.00031701245092079596, 'batch_time': 1.656553479349689, 'data_time': 0.004486483514146519}
2025-11-08 04:12:08 Train Epoch 088:  48%|████▊     | 1200/2502 [33:09<35:35,  1.64s/it, Loss=2.6788, Top1=N/A, LR=0.000317]2025-11-08 04:14:54,314 - INFO - Step 221376: {'train_loss_batch': 4.406767845153809, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6562761450488799, 'data_time': 0.0042016055562117014}
2025-11-08 04:14:54 Train Epoch 088:  52%|█████▏    | 1300/2502 [35:54<33:16,  1.66s/it, Loss=2.6860, Top1=N/A, LR=0.000317]2025-11-08 04:17:39,584 - INFO - Step 221476: {'train_loss_batch': 3.4044623374938965, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6560005608381994, 'data_time': 0.003953937014463221}
2025-11-08 04:17:39 Train Epoch 088:  56%|█████▌    | 1400/2502 [38:40<30:12,  1.65s/it, Loss=2.6722, Top1=N/A, LR=0.000317]2025-11-08 04:20:25,290 - INFO - Step 221576: {'train_loss_batch': 4.120242595672607, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6560765412090337, 'data_time': 0.003746960522190832}
2025-11-08 04:20:25 Train Epoch 088:  60%|█████▉    | 1500/2502 [41:25<27:36,  1.65s/it, Loss=2.6714, Top1=N/A, LR=0.000317]2025-11-08 04:23:10,308 - INFO - Step 221676: {'train_loss_batch': 2.2812631130218506, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6556831171796926, 'data_time': 0.0035653706791399322}
2025-11-08 04:23:10 Train Epoch 088:  64%|██████▍   | 1600/2502 [44:10<25:01,  1.66s/it, Loss=2.6620, Top1=N/A, LR=0.000317]2025-11-08 04:25:55,406 - INFO - Step 221776: {'train_loss_batch': 2.073925495147705, 'train_lr': 0.00031701245092079596, 'batch_time': 1.655389358966072, 'data_time': 0.0034030451765662056}
2025-11-08 04:25:55 Train Epoch 088:  68%|██████▊   | 1700/2502 [46:55<22:08,  1.66s/it, Loss=2.6557, Top1=N/A, LR=0.000317]2025-11-08 04:28:40,723 - INFO - Step 221876: {'train_loss_batch': 1.9989039897918701, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6552585933153803, 'data_time': 0.003262130321298607}
2025-11-08 04:28:40 Train Epoch 088:  72%|███████▏  | 1800/2502 [49:41<19:23,  1.66s/it, Loss=2.6556, Top1=N/A, LR=0.000317]2025-11-08 04:31:26,126 - INFO - Step 221976: {'train_loss_batch': 2.0449471473693848, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6551903075208139, 'data_time': 0.0031368899252731093}
2025-11-08 04:31:26 Train Epoch 088:  76%|███████▌  | 1900/2502 [52:26<16:37,  1.66s/it, Loss=2.6593, Top1=N/A, LR=0.000317]2025-11-08 04:34:11,549 - INFO - Step 222076: {'train_loss_batch': 3.928675651550293, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6551395153635617, 'data_time': 0.003026311865359592}
2025-11-08 04:34:11 Train Epoch 088:  80%|███████▉  | 2000/2502 [55:11<13:47,  1.65s/it, Loss=2.6586, Top1=N/A, LR=0.000317]2025-11-08 04:36:56,629 - INFO - Step 222176: {'train_loss_batch': 2.0543787479400635, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6549226525186123, 'data_time': 0.002926274813871751}
2025-11-08 04:36:56 Train Epoch 088:  84%|████████▍ | 2100/2502 [57:57<11:06,  1.66s/it, Loss=2.6571, Top1=N/A, LR=0.000317]2025-11-08 04:39:42,362 - INFO - Step 222276: {'train_loss_batch': 2.3884236812591553, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6550368164222278, 'data_time': 0.0028364328132930794}
2025-11-08 04:39:42 Train Epoch 088:  88%|████████▊ | 2200/2502 [1:00:43<08:19,  1.65s/it, Loss=2.6533, Top1=N/A, LR=0.000317]2025-11-08 04:42:28,339 - INFO - Step 222376: {'train_loss_batch': 1.947293996810913, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6552520605283993, 'data_time': 0.002753300214019595}
2025-11-08 04:42:28 Train Epoch 088:  92%|█████████▏| 2300/2502 [1:03:29<05:32,  1.65s/it, Loss=2.6532, Top1=N/A, LR=0.000317]2025-11-08 04:45:14,228 - INFO - Step 222476: {'train_loss_batch': 2.090367317199707, 'train_lr': 0.00031701245092079596, 'batch_time': 1.655410229460357, 'data_time': 0.0026787015366999804}
2025-11-08 04:45:14 Train Epoch 088:  96%|█████████▌| 2400/2502 [1:06:14<02:48,  1.66s/it, Loss=2.6558, Top1=N/A, LR=0.000317]2025-11-08 04:47:59,765 - INFO - Step 222576: {'train_loss_batch': 2.406801223754883, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6554082654407252, 'data_time': 0.002609748236589857}
2025-11-08 04:47:59 Train Epoch 088: 100%|█████████▉| 2500/2502 [1:09:00<00:03,  1.66s/it, Loss=2.6538, Top1=N/A, LR=0.000317]2025-11-08 04:50:45,792 - INFO - Step 222676: {'train_loss_batch': 3.7134885787963867, 'train_lr': 0.00031701245092079596, 'batch_time': 1.6556028458939605, 'data_time': 0.0025671913546593083}
2025-11-08 04:50:45 Train Epoch 088: 100%|██████████| 2502/2502 [1:09:02<00:00,  1.66s/it, Loss=2.6538, Top1=N/A, LR=0.000317]
2025-11-08 04:50:48 Val Epoch 088:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-08 04:50:52   with torch.cuda.amp.autocast():
2025-11-08 04:50:52 Val Epoch 088: 100%|██████████| 98/98 [01:50<00:00,  1.13s/it, Loss=1.8468, Top1=76.77%, Top5=93.51%]
2025-11-08 04:52:38 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-08 04:52:38   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-08 04:52:38 2025-11-08 04:52:38,497 - INFO - Step 88: {'epoch': 88, 'learning_rate': 0.0001465276765775279, 'train_loss': 2.6535352375105226, 'train_top1': 75.33171495445345, 'train_top5': 91.08560538967612, 'train_precision': 75.26211958389732, 'train_recall': 75.2312840550654, 'train_f1': 75.10850286682349, 'val_loss': 1.846822377166748, 'val_top1': 76.76999999023438, 'val_top5': 93.51000000488281, 'val_precision': 77.11578517636774, 'val_recall': 76.776, 'val_f1': 76.42984023991103}
2025-11-08 04:52:38 2025-11-08 04:52:38,499 - INFO - Epoch 088 Summary - LR: 0.000147, Train Loss: 2.6535, Val Loss: 1.8468, Val F1: 76.43%, Val Precision: 77.12%, Val Recall: 76.78%
2025-11-08 04:52:41 2025-11-08 04:52:41,719 - INFO - New best model saved with validation accuracy: 76.770%
2025-11-08 04:52:41 2025-11-08 04:52:41,720 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_089.pth
2025-11-08 04:52:41 Train Epoch 089:   0%|          | 0/2502 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:69: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-08 04:52:45   with torch.cuda.amp.autocast():
2025-11-08 04:52:47 Train Epoch 089:   0%|          | 0/2502 [00:05<?, ?it/s, Loss=1.9791, Top1=76.17%, LR=0.000147]2025-11-08 04:52:47,387 - INFO - Step 222678: {'train_loss_batch': 1.9790579080581665, 'train_lr': 0.0001465276765775279, 'batch_time': 5.662370443344116, 'data_time': 4.006737470626831}
2025-11-08 04:52:47 Train Epoch 089:   0%|          | 1/2502 [00:05<3:56:05,  5.66s/it, Loss=1.9791, Top1=76.17%, LR=0.000147]wandb: WARNING Tried to log to step 88 that is less than the current step 222676. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-11-08 04:52:49 Train Epoch 089:   4%|▍         | 100/2502 [02:51<1:06:26,  1.66s/it, Loss=2.7653, Top1=N/A, LR=0.000147]   2025-11-08 04:55:33,084 - INFO - Step 222778: {'train_loss_batch': 3.4134507179260254, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6966281338493423, 'data_time': 0.040661457741614615}
2025-11-08 04:55:33 Train Epoch 089:   8%|▊         | 200/2502 [05:36<1:03:26,  1.65s/it, Loss=2.7008, Top1=N/A, LR=0.000147]2025-11-08 04:58:18,708 - INFO - Step 222878: {'train_loss_batch': 3.660893201828003, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6765320787382363, 'data_time': 0.020914777594419263}
2025-11-08 04:58:18 Train Epoch 089:  12%|█▏        | 300/2502 [08:22<1:00:24,  1.65s/it, Loss=2.7003, Top1=75.15%, LR=0.000147]2025-11-08 05:01:04,082 - INFO - Step 222978: {'train_loss_batch': 2.0921807289123535, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6689600532633126, 'data_time': 0.014283278455765937}
2025-11-08 05:01:04 Train Epoch 089:  16%|█▌        | 400/2502 [11:07<57:43,  1.65s/it, Loss=2.6960, Top1=N/A, LR=0.000147]   2025-11-08 05:03:49,631 - INFO - Step 223078: {'train_loss_batch': 3.2011122703552246, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6655998057558055, 'data_time': 0.010973729992150665}
2025-11-08 05:03:49 Train Epoch 089:  20%|█▉        | 500/2502 [13:53<54:58,  1.65s/it, Loss=2.6779, Top1=N/A, LR=0.000147]2025-11-08 05:06:35,256 - INFO - Step 223178: {'train_loss_batch': 2.0148415565490723, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6637331130737791, 'data_time': 0.008979735498180885}
2025-11-08 05:06:35 Train Epoch 089:  24%|██▍       | 600/2502 [16:38<52:29,  1.66s/it, Loss=2.6830, Top1=75.20%, LR=0.000147]2025-11-08 05:09:20,124 - INFO - Step 223278: {'train_loss_batch': 2.1065754890441895, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6612285528325796, 'data_time': 0.007650470178258201}
2025-11-08 05:09:20 Train Epoch 089:  28%|██▊       | 700/2502 [19:24<49:36,  1.65s/it, Loss=2.6837, Top1=N/A, LR=0.000147]   2025-11-08 05:12:06,054 - INFO - Step 223378: {'train_loss_batch': 2.077296257019043, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6609537047088232, 'data_time': 0.006699398818947959}
2025-11-08 05:12:06 Train Epoch 089:  32%|███▏      | 800/2502 [22:08<46:20,  1.63s/it, Loss=2.6941, Top1=N/A, LR=0.000147]2025-11-08 05:14:50,533 - INFO - Step 223478: {'train_loss_batch': 3.992899179458618, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6589352307694682, 'data_time': 0.00598862555143092}
2025-11-08 05:14:50 Train Epoch 089:  36%|███▌      | 900/2502 [24:54<44:21,  1.66s/it, Loss=2.6886, Top1=N/A, LR=0.000147]2025-11-08 05:17:35,734 - INFO - Step 223578: {'train_loss_batch': 3.0029544830322266, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6581667160220468, 'data_time': 0.005432364414058435}
2025-11-08 05:17:35 Train Epoch 089:  40%|███▉      | 1000/2502 [27:38<41:10,  1.64s/it, Loss=2.6885, Top1=N/A, LR=0.000147]2025-11-08 05:20:20,614 - INFO - Step 223678: {'train_loss_batch': 2.0073611736297607, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6572299446616616, 'data_time': 0.004991244602870274}
2025-11-08 05:20:20 Train Epoch 089:  44%|████▍     | 1100/2502 [30:24<38:28,  1.65s/it, Loss=2.6795, Top1=N/A, LR=0.000147]2025-11-08 05:23:06,098 - INFO - Step 223778: {'train_loss_batch': 2.9334099292755127, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6570130390648838, 'data_time': 0.004633738277826821}
2025-11-08 05:23:06 Train Epoch 089:  48%|████▊     | 1200/2502 [33:09<35:52,  1.65s/it, Loss=2.6814, Top1=N/A, LR=0.000147]2025-11-08 05:25:51,164 - INFO - Step 223878: {'train_loss_batch': 3.4003491401672363, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6564841060019055, 'data_time': 0.004329574991523177}
2025-11-08 05:25:51 Train Epoch 089:  52%|█████▏    | 1300/2502 [35:54<33:09,  1.66s/it, Loss=2.6792, Top1=N/A, LR=0.000147]2025-11-08 05:28:36,392 - INFO - Step 223978: {'train_loss_batch': 3.491039514541626, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6561606393970956, 'data_time': 0.004070502075207041}
2025-11-08 05:28:36 Train Epoch 089:  56%|█████▌    | 1400/2502 [38:39<30:27,  1.66s/it, Loss=2.6735, Top1=75.29%, LR=0.000147]2025-11-08 05:31:21,499 - INFO - Step 224078: {'train_loss_batch': 1.992523431777954, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6557971690911042, 'data_time': 0.003847950616110911}
2025-11-08 05:31:21 Train Epoch 089:  60%|█████▉    | 1500/2502 [41:25<27:31,  1.65s/it, Loss=2.6725, Top1=75.31%, LR=0.000147]2025-11-08 05:34:07,528 - INFO - Step 224178: {'train_loss_batch': 2.083167552947998, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6560963802858641, 'data_time': 0.003663456654405689}
2025-11-08 05:34:07 Train Epoch 089:  64%|██████▍   | 1600/2502 [44:10<25:00,  1.66s/it, Loss=2.6734, Top1=N/A, LR=0.000147]   2025-11-08 05:36:52,646 - INFO - Step 224278: {'train_loss_batch': 2.0272951126098633, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6557890106036766, 'data_time': 0.003497088424568248}
2025-11-08 05:36:52 Train Epoch 089:  68%|██████▊   | 1700/2502 [46:56<22:09,  1.66s/it, Loss=2.6682, Top1=N/A, LR=0.000147]2025-11-08 05:39:38,642 - INFO - Step 224378: {'train_loss_batch': 3.2051472663879395, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6560340855557523, 'data_time': 0.0033537768252382273}
2025-11-08 05:39:38 Train Epoch 089:  72%|███████▏  | 1800/2502 [49:42<19:17,  1.65s/it, Loss=2.6726, Top1=N/A, LR=0.000147]2025-11-08 05:42:24,516 - INFO - Step 224478: {'train_loss_batch': 3.527207374572754, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6561843415619861, 'data_time': 0.003222394426421018}
2025-11-08 05:42:24 Train Epoch 089:  76%|███████▌  | 1900/2502 [52:28<16:34,  1.65s/it, Loss=2.6712, Top1=N/A, LR=0.000147]2025-11-08 05:45:10,044 - INFO - Step 224578: {'train_loss_batch': 2.065415382385254, 'train_lr': 0.0001465276765775279, 'batch_time': 1.656136618232426, 'data_time': 0.003108609042501274}
2025-11-08 05:45:10 Train Epoch 089:  80%|███████▉  | 2000/2502 [55:13<13:48,  1.65s/it, Loss=2.6738, Top1=75.29%, LR=0.000147]2025-11-08 05:47:55,550 - INFO - Step 224678: {'train_loss_batch': 1.971286654472351, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6560825161311938, 'data_time': 0.0030068973491693485}
2025-11-08 05:47:55 Train Epoch 089:  84%|████████▍ | 2100/2502 [57:59<11:05,  1.66s/it, Loss=2.6762, Top1=N/A, LR=0.000147]   2025-11-08 05:50:41,380 - INFO - Step 224778: {'train_loss_batch': 1.935232400894165, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6561878611506988, 'data_time': 0.0029137446618204966}
2025-11-08 05:50:41 Train Epoch 089:  88%|████████▊ | 2200/2502 [1:00:44<08:19,  1.65s/it, Loss=2.6700, Top1=N/A, LR=0.000147]2025-11-08 05:53:26,490 - INFO - Step 224878: {'train_loss_batch': 1.966485857963562, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6559568338641142, 'data_time': 0.00282714996268564}
2025-11-08 05:53:26 Train Epoch 089:  92%|█████████▏| 2300/2502 [1:03:30<05:34,  1.65s/it, Loss=2.6682, Top1=N/A, LR=0.000147]2025-11-08 05:56:12,048 - INFO - Step 224978: {'train_loss_batch': 2.2578155994415283, 'train_lr': 0.0001465276765775279, 'batch_time': 1.6559401820504835, 'data_time': 0.002748352711431776}
2025-11-08 05:56:12 Train Epoch 089:  96%|█████████▌| 2400/2502 [1:06:15<02:47,  1.64s/it, Loss=2.6625, Top1=75.31%, LR=0.000147]2025-11-08 05:58:57,374 - INFO - Step 225078: {'train_loss_batch': 1.8625390529632568, 'train_lr': 0.0001465276765775279, 'batch_time': 1.655828515473429, 'data_time': 0.002677358522458853}
2025-11-08 05:58:57 Train Epoch 089: 100%|█████████▉| 2500/2502 [1:09:01<00:03,  1.67s/it, Loss=2.6602, Top1=75.34%, LR=0.000147]2025-11-08 06:01:42,809 - INFO - Step 225178: {'train_loss_batch': 2.0595428943634033, 'train_lr': 0.0001465276765775279, 'batch_time': 1.655769497144227, 'data_time': 0.00265615856776186}
2025-11-08 06:01:42 Train Epoch 089: 100%|██████████| 2502/2502 [1:09:02<00:00,  1.66s/it, Loss=2.6602, Top1=75.34%, LR=0.000147]
2025-11-08 06:01:45 Val Epoch 089:   0%|          | 0/98 [00:00<?, ?it/s]/home/ubuntu/imagenet-1k/src/training/trainer.py:136: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
2025-11-08 06:01:49   with torch.cuda.amp.autocast():
2025-11-08 06:01:50 Val Epoch 089: 100%|██████████| 98/98 [01:49<00:00,  1.12s/it, Loss=1.8724, Top1=76.65%, Top5=93.38%]
2025-11-08 06:03:35 /home/ubuntu/venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
2025-11-08 06:03:35   warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
2025-11-08 06:03:35 2025-11-08 06:03:35,094 - INFO - Step 89: {'epoch': 89, 'learning_rate': 4.414357814956981e-05, 'train_loss': 2.6608563757342973, 'train_top1': 75.33681057321773, 'train_top5': 91.08035284200385, 'train_precision': 75.26026980851172, 'train_recall': 75.22223370705157, 'train_f1': 75.11757316315685, 'val_loss': 1.8724307618713378, 'val_top1': 76.65400001464843, 'val_top5': 93.37999998046875, 'val_precision': 77.00211784588778, 'val_recall': 76.654, 'val_f1': 76.29680127704785}
2025-11-08 06:03:35 2025-11-08 06:03:35,095 - INFO - Epoch 089 Summary - LR: 0.000044, Train Loss: 2.6609, Val Loss: 1.8724, Val F1: 76.30%, Val Precision: 77.00%, Val Recall: 76.65%
2025-11-08 06:03:36 2025-11-08 06:03:36,509 - INFO - Checkpoint saved: /home/ubuntu/imagenet-1k/checkpoints/resnet50_20251103_194251/checkpoint_epoch_090.pth
2025-11-08 06:03:36 2025-11-08 06:03:36,510 - INFO - Training completed. Best validation accuracy: 76.770%